@article{Davydov++2022,
   abstract = {In this article, we study necessary and sufficient conditions for contraction and incremental stability of dynamical systems with respect to non-Euclidean norms. First, we introduce weak pairings as a framework to study contractivity with respect to arbitrary norms, and characterize their properties. We introduce and study the sign and max pairings for the norms, respectively. Using weak pairings, we establish five equivalent characterizations for contraction, including the one-sided Lipschitz condition for the vector field as well as logarithmic norm and Demidovich conditions for the corresponding Jacobian. Third, we extend our contraction framework in two directions: we prove equivalences for contraction of continuous vector fields, and we formalize the weaker notion of equilibrium contraction, which ensures exponential convergence to an equilibrium. Finally, as an application, we provide incremental input-to-state stability and finite input-state gain properties for contracting systems, and a general theorem about the Lipschitz interconnection of contracting systems, whereby the Hurwitzness of a gain matrix implies the contractivity of the interconnected system.},
   author = {Alexander Davydov and Saber Jafarpour and Francesco Bullo},
   doi = {10.1109/TAC.2022.3183966},
   issn = {15582523},
   issue = {12},
   journal = {IEEE Transactions on Automatic Control},
   keywords = {Contraction theory,non-Euclidean norms,stability analysis},
   month = {12},
   pages = {6667-6681},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Non-Euclidean Contraction Theory for Robust Nonlinear Stability},
   volume = {67},
   year = {2022},
}
@article{Pauli++2022c,
   abstract = {In this work, we propose a dissipativity-based method for Lipschitz constant estimation of 1D convolutional neural networks (CNNs). In particular, we analyze the dissipativity properties of convolutional, pooling, and fully connected layers making use of incremental quadratic constraints for nonlinear activation functions and pooling operations. The Lipschitz constant of the concatenation of these mappings is then estimated by solving a semidefinite program which we derive from dissipativity theory. To make our method as efficient as possible, we exploit the structure of convolutional layers by realizing these finite impulse response filters as causal dynamical systems in state space and carrying out the dissipativity analysis for the state space realizations. The examples we provide show that our Lipschitz bounds are advantageous in terms of accuracy and scalability.},
   author = {Patricia Pauli and Dennis Gramlich and Frank Allgöwer},
   journal = {Proceedings of Machine Learning Research},
   keywords = {Convolutional neural networks,dissipativity,incremental quadratic con-straints,robustness},
   month = {11},
   pages = {1-14},
   title = {Lipschitz constant estimation for 1D convolutional neural networks},
   volume = {211},
   url = {https://arxiv.org/abs/2211.15253v2},
   year = {2022},
}
@article{Junnarkar++2023,
   abstract = {We propose a parameterization of a nonlinear dynamic controller based on the recurrent equilibrium network, a generalization of the recurrent neural network. We derive constraints on the parameterization under which the controller guarantees exponential stability of a partially observed dynamical system with sector bounded nonlinearities. Finally, we present a method to synthesize this controller using projected policy gradient methods to maximize a reward function with arbitrary structure. The projection step involves the solution of convex optimization problems. We demonstrate the proposed method with simulated examples of controlling a nonlinear inverted pendulum.},
   author = {Neelay Junnarkar and He Yin and Fangda Gu and Murat Arcak and Peter Seiler},
   doi = {10.1109/CDC51059.2022.9992684},
   isbn = {9781665467612},
   issn = {25762370},
   journal = {Proceedings of the IEEE Conference on Decision and Control},
   pages = {7449-7454},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Synthesis of Stabilizing Recurrent Equilibrium Network Controllers},
   volume = {2022-December},
   year = {2022},
}
@article{Siekmann++2021a,
   abstract = {We study the problem of realizing the full spectrum of bipedal locomotion on a real robot with sim-to-real reinforcement learning (RL). A key challenge of learning legged locomotion is describing different gaits, via reward functions, in a way that is intuitive for the designer and specific enough to reliably learn the gait across different initial random seeds or hyperparameters. A common approach is to use reference motions (e.g. trajectories of joint positions) to guide learning. However, finding high-quality reference motions can be difficult and the trajectories themselves narrowly constrain the space of learned motion. At the other extreme, reference-free reward functions are often underspecified (e.g. move forward) leading to massive variance in policy behavior, or are the product of significant reward-shaping via trial-and-error, making them exclusive to specific gaits. In this work, we propose a reward-specification framework based on composing simple probabilistic periodic costs on basic forces and velocities. We instantiate this framework to define a parametric reward function with intuitive settings for all common bipedal gaits - standing, walking, hopping, running, and skipping. Using this function we demonstrate successful sim-to-real transfer of the learned gaits to the bipedal robot Cassie, as well as a generic policy that can transition between all of the two-beat gaits.},
   author = {Jonah Siekmann and Yesh Godse and Alan Fern and Jonathan Hurst},
   doi = {10.1109/ICRA48506.2021.9561814},
   isbn = {9781728190778},
   issn = {10504729},
   journal = {Proceedings - IEEE International Conference on Robotics and Automation},
   pages = {9943-9949},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Sim-to-Real Learning of All Common Bipedal Gaits via Periodic Reward Composition},
   volume = {2021-May},
   year = {2021},
}
@article{Luenberger1971,
   abstract = {Observers which approximately reconstruct missing state-variable information necessary for control are presented in an introductory manner. The special topics of the identity observer, a reduced-order observer, linear functional observers, stability properties, and dual observers are discussed. Copyright © 1971 by The Institute of Electrical and Electronics Engineers, Inc.},
   author = {David G. Luenberger},
   doi = {10.1109/TAC.1971.1099826},
   issn = {15582523},
   issue = {6},
   journal = {IEEE Transactions on Automatic Control},
   pages = {596-602},
   title = {An Introduction to Observers},
   volume = {16},
   year = {1971},
}
@article{He++2016,
   abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
   author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
   doi = {10.1109/CVPR.2016.90},
   isbn = {9781467388504},
   issn = {10636919},
   journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   month = {12},
   pages = {770-778},
   publisher = {IEEE Computer Society},
   title = {Deep residual learning for image recognition},
   volume = {2016-December},
   year = {2016},
}
@article{Tian++2020,
   author = {Jun Tian and other contributors},
   title = {ReinforcementLearning.jl: A Reinforcement Learning Package for the Julia Programming Language},
   url = {https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl},
   note = {[Online]. Available: \url{https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl}},
   year = {2020},
}
@article{LeCun++2010,
   author = {Yann LeCun and Corinna Cortes and C J Burges},
   journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
   title = {MNIST handwritten digit database},
   volume = {2},
   year = {2010},
}
@article{Innes2018b,
   author = {Michael Innes},
   journal = {CoRR},
   title = {Don't Unroll Adjoint: Differentiating SSA-Form Programs},
   volume = {abs/1810.07951},
   url = {http://arxiv.org/abs/1810.07951},
   year = {2018},
}
@article{Bezanson++2017,
   author = {Jeff Bezanson and Alan Edelman and Stefan Karpinski and Viral B Shah},
   doi = {10.1137/141000671},
   issue = {1},
   journal = {SIAM Review},
   pages = {65-98},
   publisher = {SIAM},
   title = {Julia: A fresh approach to numerical computing},
   volume = {59},
   url = {https://epubs.siam.org/doi/10.1137/141000671},
   year = {2017},
}
@article{Innes2018,
   author = {Mike Innes},
   doi = {10.21105/joss.00602},
   journal = {Journal of Open Source Software},
   title = {Flux: Elegant Machine Learning with Julia},
   year = {2018},
}
@article{Barbara++2023,
   abstract = {This paper presents a policy parameterization for learning-based control on nonlinear, partially-observed dynamical systems. The parameterization is based on a nonlinear version of the Youla parameterization and the recently proposed Recurrent Equilibrium Network (REN) class of models. We prove that the resulting Youla-REN parameterization automatically satisfies stability (contraction) and user-tunable robustness (Lipschitz) conditions on the closed-loop system. This means it can be used for safe learning-based control with no additional constraints or projections required to enforce stability or robustness. We test the new policy class in simulation on two reinforcement learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum. We find that the Youla-REN performs similarly to existing learning-based and optimal control methods while also ensuring stability and exhibiting improved robustness to adversarial disturbances.},
   author = {Nicholas H. Barbara and Ruigang Wang and Ian R. Manchester},
   isbn = {2304.06193v1},
   journal = {arXiv preprint arXiv:2304.06193},
   month = {4},
   title = {Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems},
   url = {https://arxiv.org/abs/2304.06193v1},
   year = {2023},
}
@article{Wang+Manchester2023,
   abstract = {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a “direct” parameterization, i.e. a smooth mapping from R^N onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the sandwich layer), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. We illustrate the method with some applications in image classification (MNIST and CIFAR-10).},
   author = {Ruigang Wang and Ian R. Manchester},
   journal = {arXiv preprint arXiv:2301.11526},
   month = {1},
   title = {Direct Parameterization of Lipschitz-Bounded Deep Networks},
   url = {https://arxiv.org/abs/2301.11526v1},
   year = {2023},
}
@article{Martinelli++2023,
   abstract = {In this work, we introduce and study a class of Deep Neural Networks (DNNs) in continuous-time. The proposed architecture stems from the combination of Neural Ordinary Differential Equations (Neural ODEs) with the model structure of recently introduced Recurrent Equilibrium Networks (RENs). We show how to endow our proposed NodeRENs with contractivity and dissipativity -- crucial properties for robust learning and control. Most importantly, as for RENs, we derive parametrizations of contractive and dissipative NodeRENs which are unconstrained, hence enabling their learning for a large number of parameters. We validate the properties of NodeRENs, including the possibility of handling irregularly sampled data, in a case study in nonlinear system identification.},
   author = {Daniele Martinelli and Clara Lucía Galimberti and Ian R. Manchester and Luca Furieri and Giancarlo Ferrari-Trecate},
   journal = {arXiv preprint 	arXiv:2304.02976},
   month = {4},
   title = {Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations},
   url = {https://arxiv.org/abs/2304.02976v1},
   year = {2023},
}
@article{Youla++1976,
   abstract = {In many modern-day control problems encountered in the fluid, petroleum, power, gas and paper industries, cross coupling (interaction) between controlled and manipulated variables can be so severe that any attempt to employ single-loop controllers results in unacceptable performance. In all these situations, any workable control strategy must take into account the true multivariable nature of the plant and address itself directly to the design of a compatible multivariable controller. Any practical design technique must be able to cope with load disturbance, plant saturation, measurement noise, process lag, sensitivity and also incorporate suitable criteria delimiting transient behavior and steady-state performance. These difficulties, when compounded by the fact that many plants (such as chemical reactors) are inherently open-loop unstable have hindered the development of an inclusive frequency-domain analytic design methodology. However, a solution based on a least-square Wiener-Hopf minimization of an appropriately chosen cost functional is now available. The optimal controller obtained by this method guarantees an asymptotically stable and dynamical closed-loop configuration irrespective of whether or not the plant is proper, stable, or minimum-phase and also permits the stability margin of the optimal design to be ascertained in advance. The main purpose of this paper is to lay bare the physical assumptions underlying the choice of model and to present an explicit formula for the optimal controller. Copyright © 1976 by The Institute of Electrical and Electronics Engineers. Inc.},
   author = {Dante C. Youla and Joseph J. Bongiorno and Hamid A. Jabr},
   doi = {10.1109/TAC.1976.1101223},
   issn = {15582523},
   issue = {3},
   journal = {IEEE Transactions on Automatic Control},
   pages = {319-338},
   title = {Modern Wiener-Hopf Design of Optimal Controllers — Part II: The Multivariable Case},
   volume = {21},
   year = {1976},
}
@article{Pauli++2022,
   abstract = {Due to their susceptibility to adversarial perturbations, neural networks (NNs) are hardly used in safety-critical applications. One measure of robustness to such perturbations in the input is the Lipschitz constant of the input-output map defined by an NN. In this letter, we propose a framework to train multi-layer NNs while at the same time encouraging robustness by keeping their Lipschitz constant small, thus addressing the robustness issue. More specifically, we design an optimization scheme based on the Alternating Direction Method of Multipliers that minimizes not only the training loss of an NN but also its Lipschitz constant resulting in a semidefinite programming based training procedure that promotes robustness. We design two versions of this training procedure. The first one includes a regularizer that penalizes an accurate upper bound on the Lipschitz constant. The second one allows to enforce a desired Lipschitz bound on the NN at all times during training. Finally, we provide two examples to show that the proposed framework successfully increases the robustness of NNs.},
   author = {Patricia Pauli and Anne Koch and Julian Berberich and Paul Kohler and Frank Allgower},
   doi = {10.1109/LCSYS.2021.3050444},
   issn = {24751456},
   journal = {IEEE Control Systems Letters},
   keywords = {Linear matrix inequalities,neural networks,robustness},
   pages = {121-126},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Training Robust Neural Networks Using Lipschitz Bounds},
   volume = {6},
   year = {2022},
}
@book{Bullo2022,
   author = {Francesco Bullo},
   edition = {1.0},
   isbn = {979-8836646806},
   publisher = {Kindle Direct Publishing},
   title = {Contraction Theory for Dynamical Systems},
   url = {http://motion.me.ucsb.edu/book-ctds},
   year = {2022},
}
@article{Wang++2022,
   abstract = {This paper proposes a nonlinear policy architecture for control of partially-observed linear dynamical systems providing built-in closed-loop stability guarantees. The policy is based on a nonlinear version of the Youla parameterization, and augments a known stabilizing linear controller with a nonlinear operator from a recently developed class of dynamic neural network models called the recurrent equilibrium network (REN). We prove that RENs are universal approximators of contracting and Lipschitz nonlinear systems, and subsequently show that the the proposed Youla-REN architecture is a universal approximator of stabilizing nonlinear controllers. The REN architecture simplifies learning since unconstrained optimization can be applied, and we consider both a model-based case where exact gradients are available and reinforcement learning using random search with zeroth-order oracles. In simulation examples our method converges faster to better controllers and is more scalable than existing methods, while guaranteeing stability during learning transients.},
   author = {Ruigang Wang and Nicholas H. Barbara and Max Revay and Ian R. Manchester},
   doi = {10.1109/LCSYS.2022.3184847},
   issn = {2475-1456},
   journal = {IEEE Control Systems Letters},
   pages = {1-1},
   title = {Learning over All Stabilizing Nonlinear Controllers for a Partially-Observed Linear System},
   url = {https://ieeexplore.ieee.org/document/9802667/},
   year = {2022},
}
@article{Revay++2021b,
   abstract = {This paper introduces recurrent equilibrium networks (RENs), a new class of nonlinear dynamical models for applications in machine learning, system identification and control. The new model class has ``built in'' guarantees of stability and robustness: all models in the class are contracting - a strong
form of nonlinear stability - and models can satisfy prescribed incremental integral quadratic constraints (IQC), including Lipschitz bounds and incremental passivity. RENs are otherwise very flexible: they can represent all stable linear systems, all previously-known sets of contracting recurrent neural networks and echo state networks, all deep feedforward neural networks,
and all stable Wiener/Hammerstein models. RENs are parameterized directly by a vector in R^N, i.e. stability and robustness are ensured without parameter constraints, which simplifies learning since generic methods for unconstrained
optimization can be used. The performance and robustness of the new model set is evaluated on benchmark nonlinear system identification problems, and the paper also presents applications in data-driven nonlinear observer design and control with stability guarantees.},
   author = {Max Revay and Ruigang Wang and Ian R. Manchester},
   journal = {arXiv preprint arXiv:2104.05942v2},
   month = {4},
   title = {Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness},
   url = {https://arxiv.org/abs/2104.05942v2},
   year = {2021},
}
@inproceedings{Huang++2017,
   abstract = {Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.},
   author = {Sandy Huang and Nicolas Papernot and Ian Goodfellow and Yan Duan and Pieter Abbeel},
   journal = {5th International Conference on Learning Representations, ICLR 2017 - Workshop Track Proceedings},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Adversarial attacks on neural network policies},
   year = {2017},
}
@inproceedings{Kingma+Ba2015,
   abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
   author = {Diederik P. Kingma and Jimmy Lei Ba},
   journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   month = {12},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Adam: A method for stochastic optimization},
   url = {https://arxiv.org/abs/1412.6980v9},
   year = {2015},
}
@book{Sutton+Barto2018,
   author = {Richard S. Sutton and Andrew G. Barto},
   city = {Cambridge},
   edition = {Second},
   isbn = {9780262039246},
   publisher = {The MIT Press},
   title = {Reinforcement Learning: An Introduction},
   url = {http://incompleteideas.net/book/the-book-2nd.html},
   year = {2018},
}
@article{Russo+Proutiere2019,
   abstract = {Control policies, trained using the Deep Reinforcement Learning, have been recently shown to be vulnerable to adversarial attacks introducing even very small perturbations to the policy input. The attacks proposed so far have been designed using heuristics, and build on existing adversarial example crafting techniques used to dupe classifiers in supervised learning. In contrast, this paper investigates the problem of devising optimal attacks, depending on a well-defined attacker's objective, e.g., to minimize the main agent average reward. When the policy and the system dynamics, as well as rewards, are known to the attacker, a scenario referred to as a white-box attack, designing optimal attacks amounts to solving a Markov Decision Process. For what we call black-box attacks, where neither the policy nor the system is known, optimal attacks can be trained using Reinforcement Learning techniques. Through numerical experiments, we demonstrate the efficiency of our attacks compared to existing attacks (usually based on Gradient methods). We further quantify the potential impact of attacks and establish its connection to the smoothness of the policy under attack. Smooth policies are naturally less prone to attacks (this explains why Lipschitz policies, with respect to the state, are more resilient). Finally, we show that from the main agent perspective, the system uncertainties and the attacker can be modeled as a Partially Observable Markov Decision Process. We actually demonstrate that using Reinforcement Learning techniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to more resilient policies.},
   author = {Alessio Russo and Alexandre Proutiere},
   journal = {arXiv},
   month = {7},
   publisher = {arXiv},
   title = {Optimal Attacks on Reinforcement Learning Policies},
   url = {http://arxiv.org/abs/1907.13548},
   year = {2019},
}
@article{Wang+Manchester2022,
   abstract = {This paper presents a parameterization of nonlinear controllers for uncertain systems building on a recently developed neural network architecture, called the recurrent equilibrium network (REN), and a nonlinear version of the Youla parameterization. The proposed framework has "built-in"guarantees of stability, i.e., all policies in the search space result in a contracting (globally exponentially stable) closed-loop system. Thus, it requires very mild assumptions on the choice of cost function and the stability property can be generalized to unseen data. Another useful feature of this approach is that policies are parameterized directly without any constraints, which simplifies learning by a broad range of policy-learning methods based on unconstrained optimization (e.g. stochastic gradient descent). We illustrate the proposed approach with a variety of simulation examples.},
   author = {Ruigang Wang and Ian R. Manchester},
   doi = {10.23919/ACC53348.2022.9867842},
   isbn = {9781665451963},
   issn = {07431619},
   journal = {Proceedings of the American Control Conference},
   pages = {2116-2123},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Youla-REN: Learning Nonlinear Feedback Policies with Robust Stability Guarantees},
   volume = {2022-June},
   year = {2022},
}

@InProceedings{Song++2023,
  title = 	 {{L}ips{N}et: A Smooth and Robust Neural Network with Adaptive {L}ipschitz Constant for High Accuracy Optimal Control},
  author =       {Song, Xujie and Duan, Jingliang and Wang, Wenxuan and Li, Shengbo Eben and Chen, Chen and Cheng, Bo and Zhang, Bo and Wei, Junqing and Wang, Xiaoming Simon},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32253--32272},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23b/song23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23b.html},
  abstract = 	 {Deep reinforcement learning (RL) is a powerful approach for solving optimal control problems. However, RL-trained policies often suffer from the action fluctuation problem, where the consecutive actions significantly differ despite only slight state variations. This problem results in mechanical components’ wear and tear and poses safety hazards. The action fluctuation is caused by the high Lipschitz constant of actor networks. To address this problem, we propose a neural network named LipsNet. We propose the Multi-dimensional Gradient Normalization (MGN) method, to constrain the Lipschitz constant of networks with multi-dimensional input and output. Benefiting from MGN, LipsNet achieves Lipschitz continuity, allowing smooth actions while preserving control performance by adjusting Lipschitz constant. LipsNet addresses the action fluctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more flexible and user-friendly than previous works. Experiments demonstrate that LipsNet has good landscape smoothness and noise robustness, resulting in significantly smoother action compared to the Multilayer Perceptron.}
}
