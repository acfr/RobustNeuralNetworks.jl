var documenterSearchIndex = {"docs":
[{"location":"index.html#RobustNeuralNetworks.jl-Documentation","page":"Home","title":"RobustNeuralNetworks.jl Documentation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Bringing robust machine learning to Julia.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"","category":"page"},{"location":"index.html#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Here's an example. You should be able to see the import statement of Random.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"using Random\na = 1\nb = 2*rand()\n2a + b","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Can we continue using variables from this example?","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"println(a+b)","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"We can even make things look like the REPL.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"a = 1\nb = 2\na + b","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"We can even delay execution of an example over a few different example blocks. Start a for loop here...","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"for i in 1:3\n    j = i^2","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Then write something insightful and finish it below...","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"    println(j)\nend","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"It's worth having a look at the @setup macro as well when you can. It will make it much easier to write examples that include a number of lines of setup which should be hidden. Having said that, it might be useful to show the reader how you set up the example!","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Most of your examples should be written with the @jldoctest macro. I'll give it a go below, but have a look at how ControlSystems.jl does things too.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Example:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"using Random\nusing RobustNeuralNetworks\n\nbatches = 50\nnu, nx, nv, ny = 4, 10, 20, 2\n\ncontracting_ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny)\ncontracting_ren = REN(contracting_ren_ps)\n\nx0 = init_states(contracting_ren, batches)\nu0 = randn(contracting_ren.nu, batches)\n\nx1, y1 = contracting_ren(x0, u0)  # Evaluates the REN over one timestep\n\nprintln(size(y1))\n\n# output\n\n(2, 50)","category":"page"},{"location":"index.html#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"","category":"page"},{"location":"index.html#Docstrings","page":"Home","title":"Docstrings","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Work on the presentation of this a bit....","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Modules = [RobustNeuralNetworks]\nPrivate = false","category":"page"},{"location":"index.html#RobustNeuralNetworks.AbstractLBDN","page":"Home","title":"RobustNeuralNetworks.AbstractLBDN","text":"abstract type AbstractLBDN end\n\nParameterisation for Lipschitz-bounded deep networks.\n\n\n\n\n\n","category":"type"},{"location":"index.html#RobustNeuralNetworks.AbstractREN","page":"Home","title":"RobustNeuralNetworks.AbstractREN","text":"abstract type AbstractREN end\n\nExplicit parameterisation for recurrent equilibrium networks.\n\n\n\n\n\n","category":"type"},{"location":"index.html#RobustNeuralNetworks.AbstractREN-Tuple{VecOrMat{T} where T, VecOrMat{T} where T}","page":"Home","title":"RobustNeuralNetworks.AbstractREN","text":"(m::AbstractREN)(xt::VecOrMat, ut::VecOrMat)\n\nCall a REN model given internal states xt and inputs ut. \n\nIf arguments are matrices, each column must be a vector of states or inputs (allows batch simulations).\n\nExamples\n\nThis example creates a contracting REN using ContractingRENParams and calls the model with some randomly generated inputs. \n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 2, 20, 1\n\n# Construct a REN\ncontracting_ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng=rng)\nren = REN(contracting_ren_ps)\n\n# Some dummy inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\nprintln(round.(y1;digits=2))\n\nSee also REN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.AbstractRENParams","page":"Home","title":"RobustNeuralNetworks.AbstractRENParams","text":"abstract type AbstractRENParams{T} end\n\nDirect parameterisation for recurrent equilibrium networks.\n\n\n\n\n\n","category":"type"},{"location":"index.html#RobustNeuralNetworks.ContractingRENParams-Union{NTuple{4, Int64}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.ContractingRENParams","text":"ContractingRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a contracting REN.\n\nThe parameters can be used to construct an explicit REN model that has guaranteed, built-in contraction properties.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.ContractingRENParams-Union{Tuple{T}, Tuple{Int64, AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}, AbstractMatrix{T}}} where T","page":"Home","title":"RobustNeuralNetworks.ContractingRENParams","text":"ContractingRENParams(nv, A, B, C, D; ...)\n\nAlternative constructor for ContractingRENParams that initialises the REN from a stable discrete-time linear system with state-space model\n\nbeginalign*\nx_t+1 = Ax_t + Bu_t \ny_t = Cx_t + Du_t\nendalign*\n\nTODO: This method may be removed in a later edition of the package. TODO: Make compatible with αbar ≠ 1.0.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.DiffREN","page":"Home","title":"RobustNeuralNetworks.DiffREN","text":"mutable struct DiffREN <: AbstractREN\n\nWrapper for REN type which automatically re-computes  explicit parameters every time the model is called.\n\nThis is slow, but is compatible with Flux.jl\n\n\n\n\n\n","category":"type"},{"location":"index.html#RobustNeuralNetworks.DiffREN-Union{Tuple{AbstractRENParams{T}}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.DiffREN","text":"DiffREN(ps::AbstractRENParams{T}) where T\n\nConstruct a differentiable REN from its direct parameterisation.\n\nDiffREN is an alternative to REN that computes the explicit parameterisation every time the model is called. This is slow and computationally inefficient. However, it can be used with Flux.jl just like any other Flux model to do machine learning.\n\nThe difference to REN and WrapREN is that the ExplicitParams struct is never stored, so an instance of DiffREN never has to be mutated or re-defined after it is created, even when learnable parameters are updated.\n\nSee also REN and WrapREN.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.DirectParams-Union{NTuple{4, Int64}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.DirectParams","text":"DirectParams{T}(nu, nx, nv; <keyword arguments>) where T\n\nConstruct direct parameterisation for an (acyclic) recurrent equilibrium network.\n\nThis is typically used by higher-level constructors when defining a REN, which take the direct parameterisation and define rules for converting it to an explicit parameterisation. See for example GeneralRENParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\n\nKeyword arguments\n\ninit=:random: Initialisation method. Options are:\n:random: Random sampling for all parameters.\n:cholesky: Compute X with cholesky factorisation of H, sets E,F,P = I.\npolar_param::Bool=true: Use polar parameterisation to construct H matrix from X in REN parameterisation (recommended).\nD22_free::Bool=false: Specify whether to train D22 as a free parameter (true), or construct it separately from X3, Y3, Z3 (false). Typically use D22_free = true only for a contracting REN.\nD22_zero::Bool=false: Fix D22 = 0 to remove any feedthrough.\nbx_scale::T=0: Set scale of initial state bias vector bx.\nbv_scale::T=1: Set scalse of initial neuron input bias vector bv.\nϵ::T=1e-12: Regularising parameter for positive-definite matrices.\nrng::AbstractRNG=Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Revay et al. (2021) for parameterisation details.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.ExplicitParams","page":"Home","title":"RobustNeuralNetworks.ExplicitParams","text":"mutable struct ExplicitParams{T}\n\nExplicit REN parameter struct.\n\nThese parameters define a recurrent equilibrium network with model inputs and outputs u_t y_t, neuron inputs and outputs v_tw_t, and states x_t.\n\nbeginequation*\nbeginbmatrix\nx_t+1  v_t  y_t\nendbmatrix\n= \nbeginbmatrix\nA  B_1  B_2 \nC_1  D_11  D_12 \nC_2  D_21  D_22 \nendbmatrix\nbeginbmatrix\nx_t  w_t  u_t\nendbmatrix\n+ \nbeginbmatrix\nb_x  b_v  b_y\nendbmatrix\nendequation*\n\nSee Revay et al. (2021) for more details on explicit parameterisations of REN.\n\n\n\n\n\n","category":"type"},{"location":"index.html#RobustNeuralNetworks.GeneralRENParams-Union{Tuple{T}, Tuple{Int64, Int64, Int64, Int64, Matrix{T}, Matrix{T}, Matrix{T}}} where T","page":"Home","title":"RobustNeuralNetworks.GeneralRENParams","text":"GeneralRENParams{T}(nu, nx, nv, ny, Q, S, R; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN satisfying general behavioural constraints.\n\nBehavioural constraints are encoded by the matrices Q,S,R in an incremental Integral Quadratic Constraint (IQC). See Equation 4 of Revay et al. (2021).\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nQ::Matrix{T}: IQC weight matrix on model outputs\nS::Matrix{T}: IQC coupling matrix on model outputs/inputs\nR::Matrix{T}: IQC weight matrix on model outputs\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.LBFN-Union{Tuple{T}, Tuple{Int64, Vector{Int64}, Int64}, Tuple{Int64, Vector{Int64}, Int64, T}, Tuple{Int64, Vector{Int64}, Int64, T, Any}, Tuple{Int64, Vector{Int64}, Int64, T, Any, Any}, Tuple{Int64, Vector{Int64}, Int64, T, Any, Any, Any}} where T","page":"Home","title":"RobustNeuralNetworks.LBFN","text":"LBFN{T}(nu, nh, ny, γ; ...)\n\nConstructor for an LBFN with nu inputs, nv outputs, and nh = [nh1, nh2,...] specifying the size of hidden layers. User-imposed Lipschitz bound γ has a default of 1.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.LipschitzRENParams-Union{Tuple{T}, Tuple{Int64, Int64, Int64, Int64, Number}} where T","page":"Home","title":"RobustNeuralNetworks.LipschitzRENParams","text":"LipschitzRENParams(nu, nx, nv, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN with a Lipschitz bound of γ.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nγ::Number: Lipschitz upper bound.\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, ContractingRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.PassiveRENParams-Union{NTuple{4, Int64}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.PassiveRENParams","text":"PassiveRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a passive REN.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nν::T=0: Passivity parameter. Use ν>0 for incrementally strictly input passive model, and ν == 0 for incrementally passive model. \nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.REN-Union{Tuple{AbstractRENParams{T}}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.REN","text":"REN(ps::AbstractRENParams{T}) where T\n\nConstruct a REN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of REN (eg: a GeneralRENParams instance) and converts it to a callable explicit parameterisation of the REN.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.WrapREN-Union{Tuple{AbstractRENParams{T}}, Tuple{T}} where T","page":"Home","title":"RobustNeuralNetworks.WrapREN","text":"WrapREN(ps::AbstractRENParams{T}) where T\n\nConstruct REN wrapper from its direct parameterisation.\n\nWrapREN is an alternative to REN that stores the AbstractRENParams and ExplicitParams within the same object. This means that a new REN object does not have to be created each time the parameters are updated. Explicit REN parameters must be updated by the user if the direct parameters have changed.\n\nNote that WrapREN cannot be used with Flux.jl, since it relies on mutating the WrapREN instance.\n\nExamples\n\nIn this example, we create a REN satisfying some generic behavioural constraints and demonstrate how to update the REN wrapper if model parameters are changed.\n\nusing LinearAlgebra\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\n\nQ = Matrix{Float64}(-I(ny))\nR = 0.1^2 * Matrix{Float64}(I(nu))\nS = zeros(Float64, nu, ny)\n\n# Construct a REN\nren_ps = GeneralRENParams{Float64}(nu, nx, nv, ny, Q, S, R; rng=rng)\nren = WrapREN(ren_ps)\n\n# Some dummy inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0) \n\n# Update the model after changing a parameter\nren.params.direct.B2 .*= rand(rng, size(ren.params.direct.B2)...)\nupdate_explicit!(ren)\n\nprintln(round(ren.explicit.B2[1];digits=4))\n\nSee also REN and DiffREN.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.direct_to_explicit","page":"Home","title":"RobustNeuralNetworks.direct_to_explicit","text":"direct_to_explicit(ps::AbstractRENParams, return_h=false) where T\n\nConvert direct parameterisation of RENs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitParams object that naturally satisfies a set of user-defined behavioural constraints.\n\nArguments\n\nps::AbstractRENParams: Direct parameterisation with behavioural constraints to convert to an explicit parameterisation of REN (eg: GeneralRENParams).\nreturn_h::Bool=false: Whether to return the H-matrix directly (see Revay et al. (2021)). Useful for debugging or model analysis. If false, function returns an object of type ExplicitParams{T}. \n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"function"},{"location":"index.html#RobustNeuralNetworks.init_states-Tuple{AbstractREN, Any}","page":"Home","title":"RobustNeuralNetworks.init_states","text":"init_states(m::AbstractREN, nbatches; rng=nothing)\n\nReturn matrix of (nbatches) state vectors of a REN initialised as zeros.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.set_output_zero!-Tuple{AbstractREN}","page":"Home","title":"RobustNeuralNetworks.set_output_zero!","text":"set_output_zero!(m::AbstractREN)\n\nSet output map of a REN to zero.\n\nIf the resulting model is called with x1,y = ren(x,u) then y = 0 for any x and u.\n\n\n\n\n\n","category":"method"},{"location":"index.html#RobustNeuralNetworks.update_explicit!-Tuple{WrapREN}","page":"Home","title":"RobustNeuralNetworks.update_explicit!","text":"update_explicit!(m::WrapREN)\n\nUpdate explicit model in WrapREN using the current direct parameters.\n\n\n\n\n\n","category":"method"},{"location":"index.html#TODO:","page":"Home","title":"TODO:","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Add a logo\nFill out this main documentation page\nSee ControlSystems.jl for a good example of how to structure this page.","category":"page"}]
}
