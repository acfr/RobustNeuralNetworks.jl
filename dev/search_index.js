var documenterSearchIndex = {"docs":
[{"location":"introduction/developing/#Contributing-to-the-Package","page":"Contributing to the Package","title":"Contributing to the Package","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All contributors welcome! Please contact nicholas.barbara@sydney.edu.au with any questions.","category":"page"},{"location":"introduction/developing/#Installation-for-Development","page":"Contributing to the Package","title":"Installation for Development","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the package, clone the repository into your ~/.julia/dev/ directory with","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"git clone git@github.com:acfr/RobustNeuralNetworks.jl.git RobustNeuralNetworks","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Note that the repo is RobustNeuralNetworks.jl but the folder name is RobustNeuralNetworks. This is convention for Julia packages. Navigate to the repository directory, start a Julia session, and type the following in the REPL to activate the package.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"using Pkg\nPkg.instantiate()\nPkg.activate(\".\")","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Check that the example in Getting Started runs without errors and matches the given output before continuing.","category":"page"},{"location":"introduction/developing/#Package-Structure","page":"Contributing to the Package","title":"Package Structure","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The main file is src/RobustNeuralNetworks.jl. This imports all relevant packages, defines abstract types, includes code from other files, and exports the necessary components of our package. ","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All using PackageName statements should be included in this file.\nOnly import the packages you really need\nIf you only need one function from a package, import it explicitly (not the whole package)","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"When including files in our src/ folder, the order often matters. Code should only ever be included with a single include statement in the main file. Please follow the convention outlined in the comments.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The source files for our package are all in the src/ directory, and are split into the following sub-directories.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"src/Base/: Contains code relevant to the core package functionality.\nsrc/ParameterTypes/: Contains the various REN parameterisations, all of type AbstractRENParams.\nsrc/LBDN/: Contains code exclusively used for AbstractLBDN models","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Once you have written any code for this package, be sure to test it thoroughly. Write testing scripts in the test/ directory.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"See Test.jl documentation for help with writing good package tests.\nRun all tests for the package with ] test in the REPL.\nAll tests will be run by the CI client when submitting pull requests to the main git branch.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the docs, this page provides a great outline of the required workflow.","category":"page"},{"location":"introduction/developing/#Git-Workflow","page":"Contributing to the Package","title":"Git Workflow","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The package can be treated as an independent git repository for devlopment.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Please feel free to submit git issues, pull requests, etc. as usual.\nAlways develop new features in a new branch labelled feature/<some_descriptive_words>. For example, the branch feature/documentation is where this documentation was first written and tested.\nSubmit pull requests once you have completed a new feature and tested it thorougly. Pull requests without thorough testing will be rejected.","category":"page"},{"location":"examples/pde_obsv/#PDE-Observer-Design-with-REN","page":"PDE Observer","title":"PDE Observer Design with REN","text":"","category":"section"},{"location":"introduction/layout/#Package-Overview","page":"Package Overview","title":"Package Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The RobustNeuralNetwork.jl package is divided into Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) models.","category":"page"},{"location":"introduction/layout/#REN-Overview","page":"Package Overview","title":"REN Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The REN models are defined by two fundamental types:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Any subtype of AbstractRENParams holds all the information required to directly parameterise a REN satisfying some user-defined behavioural constraints.\nAny subtype of AbstractREN represents the REN in its explicit form so that it can be called and evaluated.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"info: Separate Objects for Parameters and Model\nWhen working with most models (eg: RNN and LSTM) the typical workflow is to create a single instance of a model. Its parameters are updated during training, but the model object is only created once. For example:using Flux\n\n# Define a model\nmodel = Flux.RNNCell(2,5)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parametersWhen working with RENs, it is much more efficient to split up the model parameterisation and the model implementation into subtypes of AbstractRENParams and AbstractREN. Converting our direct parameterisation to an explicit model for evaluation can be slow, so we only do it when the model parameters are updated:using Flux\nusing RobustNeuralNetworks\n\n# Define a model parameterisation\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\n\n# Train the model\nfor k in 1:num_training_epochs\n    model = REN(params)     # Create explicit model for evaluation\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parametersSee the section on REN Wrappers for more details.","category":"page"},{"location":"introduction/layout/#(Direct)-Parameter-Types","page":"Package Overview","title":"(Direct) Parameter Types","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Subtypes of AbstractRENParams define direct parameterisations of a REN. They are not callable models. There are four REN parameter types currently in this package:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"ContractingRENParams parameterises a REN with a user-defined upper bound on the contraction rate.\nLipschitzRENParams parameterises a REN with a user-defined Lipschitz constant of gamma in (0infty).\nPassiveRENParams parameterises an input/output passive REN with user-tunable passivity parameter nu ge 0.\nGeneralRENParams parameterises a REN satisfying some generalbehavioural constraints defined by an Integral Quadratic Constraint (IQC).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"For more information on these four parameterisations, please see Revay et al. (2021).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Each of these parameter types has the following collection of attributes:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"A static nonlinearity nl. Common choices are Flux.relu or Flux.tanh (see Flux.jl for more information).\nModel sizes nu, nx, nv, ny defining the number of inputs, states, neurons, and outputs (respectively).\nAn instance of DirectParams containing the direct parameters of the REN, including all trainable parameters.\nOther attributes used to define how the direct parameterisation should be converted to the implicit model. These parameters encode the user-tunable behavioural constraints. Eg: gamma for a Lipschitz-bounded REN.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The typical workflow is to create an instance of a REN parameterisation only once. This defines all dimensions and desired properties of a REN. It is then converted to an explicit model for the REN to be evaluated.","category":"page"},{"location":"introduction/layout/#Explicit-REN-Models","page":"Package Overview","title":"Explicit REN Models","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"An explicit REN model must be created to call and use the network for computation. The explicit parameterisation contains all information required to evaluate a REN. We encode RENs in explicit form as subtypes of the AbstractREN type. Each subtype of AbstractREN is callable and includes the following attributes:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"A static nonlinearity nl and model sizes nu, nx, nv, ny (same as AbstractRENParams.\nAn instance of ExplicitParams containing all REN parameters in explicit form for model evaluation (see the ExplicitParams docs for more detail).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Each subtype of AbstractRENParams has a method direct_to_explicit associated with it that converts the DirectParams struct to an instance of ExplicitParams satisfying the specified behavioural constraints.","category":"page"},{"location":"introduction/layout/#REN-Wrappers","page":"Package Overview","title":"REN Wrappers","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"There are three explicit REN wrappers currently implemented in this package. Each of them constructs a REN from a direct parameterisation params::AbstractRENParams and can be used to evaluate REN models.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"REN is the basic and most commonly-used wrapper. A new instance of REN must be created whenever the parameters params are changed.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"tip: REN is recommended\nWe strongly recommend using REN to train your models with Flux.jl. It is the most efficient subtype of AbstractREN that is compatible with automatic differentiation.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"WrapREN includes both the DirectParams and ExplicitParams as part of the REN wrapper. When any of the direct parameters are changed, the explicit model can be updated by calling update_explicit!. This can be useful when not using automatic differentiation to train the model. For example:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using RobustNeuralNetworks\n\n# Define a model parameterisation AND a model\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\nmodel  = WrapREN(params)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    ...                     # Update model parameters\n    update_explicit!(model) # Update explicit model parameters","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"warning: WrapREN incompatible with Flux.jl\nSince the explicit parameters are stored in an instance of WrapREN, changing them with update_explicit! directly mutates the model. This will cause errors if the model is to be trained with Flux.jl. Use REN or DiffREN to avoid this issue.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"DiffREN also includes DirectParams, but never stores the ExplicitParams. Instead, the explicit parameters are computed every time the model is evaluated. This is slow, but does not require creating a new object when the parameters are updated, and is still compatible with Flux.jl. For example:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using Flux\n\n# Define a model parameterisation AND a model\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\nmodel  = DiffREN(params)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parameters","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"See the docstring of each wrapper and the examples (eg: PDE Observer Design with REN) for more details.","category":"page"},{"location":"introduction/layout/#LBDN-Overview","page":"Package Overview","title":"LBDN Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"[To be written once LBDN has been properly added to the package.]","category":"page"},{"location":"introduction/layout/#Walkthrough","page":"Package Overview","title":"Walkthrough","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's step through the example from Getting Started, which constructs and evaluates a Lipschitz-bounded REN. Start by importing packages and setting a random seed.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using Random\nusing RobustNeuralNetworks","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's set a random seed and define our batch size and some hyperparameters. For this example, we'll build a Lipschitz-bounded REN with 4 inputs, 2 outputs, 10 states, 20 neurons, and a Lipschitz bound of γ = 1.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"rng = MersenneTwister(42)\nbatches = 10\n\nnu, nx, nv, ny = 4, 10, 20, 2\nγ = 1","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's construct the REN parameters. The variable lipschitz_ren_ps contains all the parameters required to build a Lipschitz-bounded REN.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"lipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng=rng)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Once the parameters are defined, we can create a REN object in its explicit form.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"ren = REN(lipschitz_ren_ps)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Now we can evaluate the REN. Note that we can use the init_states function to create a batch of initial states, all zeros, of the correct dimensions.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Having evaluated the REN, we can check that the outputs are the same as in the original example.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"# Print results for testing\nyout = round.(y1; digits=2)\nprintln(yout[1,:])\nprintln(yout[2,:])","category":"page"},{"location":"lib/lbdn/","page":"Lipschitz-Bounded Deep Networks","title":"Lipschitz-Bounded Deep Networks","text":"Pages = [\"lbdn.md\"]","category":"page"},{"location":"lib/lbdn/#Lipschitz-Bounded-Deep-Networks-(LBDN)","page":"Lipschitz-Bounded Deep Networks","title":"Lipschitz-Bounded Deep Networks (LBDN)","text":"","category":"section"},{"location":"lib/lbdn/","page":"Lipschitz-Bounded Deep Networks","title":"Lipschitz-Bounded Deep Networks","text":"AbstractLBDN\nLBFN","category":"page"},{"location":"lib/lbdn/#RobustNeuralNetworks.AbstractLBDN","page":"Lipschitz-Bounded Deep Networks","title":"RobustNeuralNetworks.AbstractLBDN","text":"abstract type AbstractLBDN end\n\nParameterisation for Lipschitz-bounded deep networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/lbdn/#RobustNeuralNetworks.LBFN","page":"Lipschitz-Bounded Deep Networks","title":"RobustNeuralNetworks.LBFN","text":"LBFN{T}(nu, nh, ny, γ; ...)\n\nConstructor for an LBFN with nu inputs, nv outputs, and nh = [nh1, nh2,...] specifying the size of hidden layers. User-imposed Lipschitz bound γ has a default of 1.\n\n\n\n\n\n","category":"type"},{"location":"examples/rl/#Reinforcement-Learning-with-LBDN","page":"Reinforcement Learning","title":"Reinforcement Learning with LBDN","text":"","category":"section"},{"location":"lib/ren_params/","page":"REN Parameterisations","title":"REN Parameterisations","text":"Pages = [\"ren_params.md\"]","category":"page"},{"location":"lib/ren_params/#REN-Parameterisations","page":"REN Parameterisations","title":"REN Parameterisations","text":"","category":"section"},{"location":"lib/ren_params/","page":"REN Parameterisations","title":"REN Parameterisations","text":"AbstractRENParams\nContractingRENParams\nDirectParams\nExplicitParams\nGeneralRENParams\nLipschitzRENParams\nPassiveRENParams","category":"page"},{"location":"lib/ren_params/#RobustNeuralNetworks.AbstractRENParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.AbstractRENParams","text":"abstract type AbstractRENParams{T} end\n\nDirect parameterisation for recurrent equilibrium networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.ContractingRENParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.ContractingRENParams","text":"ContractingRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a contracting REN.\n\nThe parameters can be used to construct an explicit REN model that has guaranteed, built-in contraction properties.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\nContractingRENParams(nv, A, B, C, D; ...)\n\nAlternative constructor for ContractingRENParams that initialises the REN from a stable discrete-time linear system with state-space model\n\nbeginalign*\nx_t+1 = Ax_t + Bu_t \ny_t = Cx_t + Du_t\nendalign*\n\n[TODO:] This method may be removed in a later edition of the package.\n\n[TODO:] Make compatible with αbar ≠ 1.0.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.DirectParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.DirectParams","text":"DirectParams{T}(nu, nx, nv; <keyword arguments>) where T\n\nConstruct direct parameterisation for an (acyclic) recurrent equilibrium network.\n\nThis is typically used by higher-level constructors when defining a REN, which take the direct parameterisation and define rules for converting it to an explicit parameterisation. See for example GeneralRENParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\n\nKeyword arguments\n\ninit=:random: Initialisation method. Options are:\n:random: Random sampling for all parameters.\n:cholesky: Compute X with cholesky factorisation of H, sets E,F,P = I.\npolar_param::Bool=true: Use polar parameterisation to construct H matrix from X in REN parameterisation (recommended).\nD22_free::Bool=false: Specify whether to train D22 as a free parameter (true), or construct it separately from X3, Y3, Z3 (false). Typically use D22_free = true only for a contracting REN.\nD22_zero::Bool=false: Fix D22 = 0 to remove any feedthrough.\nbx_scale::T=0: Set scale of initial state bias vector bx.\nbv_scale::T=1: Set scalse of initial neuron input bias vector bv.\nϵ::T=1e-12: Regularising parameter for positive-definite matrices.\nrng::AbstractRNG=Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Revay et al. (2021) for parameterisation details.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.ExplicitParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.ExplicitParams","text":"mutable struct ExplicitParams{T}\n\nExplicit REN parameter struct.\n\nThese parameters define a recurrent equilibrium network with model inputs and outputs u_t y_t, neuron inputs and outputs v_tw_t, and states x_t.\n\nbeginequation*\nbeginbmatrix\nx_t+1  v_t  y_t\nendbmatrix\n= \nbeginbmatrix\nA  B_1  B_2 \nC_1  D_11  D_12 \nC_2  D_21  D_22 \nendbmatrix\nbeginbmatrix\nx_t  w_t  u_t\nendbmatrix\n+ \nbeginbmatrix\nb_x  b_v  b_y\nendbmatrix\nendequation*\n\nSee Revay et al. (2021) for more details on explicit parameterisations of REN.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.GeneralRENParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.GeneralRENParams","text":"GeneralRENParams{T}(nu, nx, nv, ny, Q, S, R; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN satisfying general behavioural constraints.\n\nBehavioural constraints are encoded by the matrices Q,S,R in an incremental Integral Quadratic Constraint (IQC). See Equation 4 of Revay et al. (2021).\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nQ::Matrix{T}: IQC weight matrix on model outputs\nS::Matrix{T}: IQC coupling matrix on model outputs/inputs\nR::Matrix{T}: IQC weight matrix on model outputs\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.LipschitzRENParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.LipschitzRENParams","text":"LipschitzRENParams(nu, nx, nv, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN with a Lipschitz bound of γ.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nγ::Number: Lipschitz upper bound.\n\nKeyword arguments\n\nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, ContractingRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren_params/#RobustNeuralNetworks.PassiveRENParams","page":"REN Parameterisations","title":"RobustNeuralNetworks.PassiveRENParams","text":"PassiveRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a passive REN.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nν::T=0: Passivity parameter. Use ν>0 for incrementally strictly input passive model, and ν == 0 for incrementally passive model. \nnl=Flux.relu: Static nonlinearity (eg: Flux.relu or Flux.tanh).\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectParams documentation for arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams.\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"examples/lbdn/#Image-Classification-with-LBDN","page":"Image Classification","title":"Image Classification with LBDN","text":"","category":"section"},{"location":"introduction/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"introduction/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"RobustNeuralNetworks.jl is written in Julia and can be installed with the in-built package manager. It is not currently a registered package and must be installed directly from our GitHub repository.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"To add the package, type the following into the REPL.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"] add git@github.com:acfr/RobustNeuralNetworks.jl.git","category":"page"},{"location":"introduction/getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"You should now be able to construct robust neural network models. The following example constructs a Lipschitz-bounded REN and evalutates it given a batch of random initial states and inputs.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\nγ = 1\n\n# Construct a REN\nlipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng=rng)\nren = REN(lipschitz_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\n# Print results for testing\nyout = round.(y1; digits=2)\nprintln(yout[1,:])\nprintln(yout[2,:])\n\n# output\n\n[0.73, 0.72, -0.53, 0.25, 0.84, 0.97, 0.96, 1.13, 0.87, 1.07]\n[1.13, 1.07, 1.44, 0.83, 0.94, 1.26, 0.86, 0.8, 0.96, 0.86]","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"See Package Overview for a detailed walkthrough of this example.","category":"page"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"lib/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"direct_to_explicit\ninit_states\nset_output_zero!\nupdate_explicit!","category":"page"},{"location":"lib/functions/#RobustNeuralNetworks.direct_to_explicit","page":"Functions","title":"RobustNeuralNetworks.direct_to_explicit","text":"direct_to_explicit(ps::AbstractRENParams, return_h=false) where T\n\nConvert direct parameterisation of RENs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitParams object that naturally satisfies a set of user-defined behavioural constraints.\n\nArguments\n\nps::AbstractRENParams: Direct parameterisation with behavioural constraints to convert to an explicit parameterisation of REN (eg: GeneralRENParams).\nreturn_h::Bool=false: Whether to return the H-matrix directly (see Revay et al. (2021)). Useful for debugging or model analysis. If false, function returns an object of type ExplicitParams{T}. \n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.init_states","page":"Functions","title":"RobustNeuralNetworks.init_states","text":"init_states(m::AbstractREN, nbatches; rng=nothing)\n\nReturn matrix of (nbatches) state vectors of a REN initialised as zeros.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.set_output_zero!","page":"Functions","title":"RobustNeuralNetworks.set_output_zero!","text":"set_output_zero!(m::AbstractREN)\n\nSet output map of a REN to zero.\n\nIf the resulting model is called with x1,y = ren(x,u) then y = 0 for any x and u.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.update_explicit!","page":"Functions","title":"RobustNeuralNetworks.update_explicit!","text":"update_explicit!(m::WrapREN)\n\nUpdate explicit model in WrapREN using the current direct parameters.\n\n\n\n\n\n","category":"function"},{"location":"examples/nonlinear_ctrl/#Nonlinear-Control-Design-with-REN","page":"Nonlinear Control","title":"Nonlinear Control Design with REN","text":"","category":"section"},{"location":"#RobustNeuralNetworks.jl-Documentation","page":"Home","title":"RobustNeuralNetworks.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Bringing robust machine learning to Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for RobustNeuralNetworks.jl! This package contains neural network models that are constructed to naturally satisfy robustness constraints, all in native Julia.","category":"page"},{"location":"#Why-Robust-Models?","page":"Home","title":"Why Robust Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modern machine learning relies heavily on rapidly training and evaluating neural networks in problems ranging from image classification to robotic control. Most existing neural network architectures have no robustness certificates, making them sensitive to poor data quality, adversarial attacks, and other input perturbations. The few neural network architectures proposed in recent years that offer solutions to this brittle behaviour rely on explicitly enforcing constraints during training to “smooth” the network response. These methods are computationally expensive, making them slow and difficult to scale up to complex real-world problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[TODO: Add comments on LBDN here.] Recently, we proposed the Recurrent Equilibrium Network (REN) architecture as computationally efficient solutions to these problems. The REN architecture is flexible in that it includes all commonly used neural network models, such as fully-connected networks, convolutional neural networks, and recurrent neural networks. The weight matrices and bias vectors in a REN are directly parameterised to naturally satisfy behavioural constraints chosen by the user. For example, the user can build a REN with a given Lipschitz constant to ensure the output of the network is quantifiably less sensitive to unexpected input perturbations. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The direct parameterisation of RENs means that we can train RENs with standard, unconstrained optimization methods (such as gradient descent) while also guaranteeing their robustness. Achieving the “best of both worlds” in this way is the main advantage of our REN/LBDN model classes, and allows us to freely train them for common machine learning problems as well as more difficult applications where safety and robustness are critical.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"introduction/getting_started.md\", \"introduction/layout.md\", \"introduction/developing.md\"]\nDepth = 1","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples/lbdn.md\", \"examples/rl.md\", \"examples/nonlinear_ctrl.md\", \"examples/pde_obsv.md\"]\nDepth = 1","category":"page"},{"location":"#Library","page":"Home","title":"Library","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"lib/lbdn.md\", \"lib/ren.md\", \"lib/ren_params.md\", \"lib/functions.md\"]\nDepth = 1","category":"page"},{"location":"#Research-Papers","page":"Home","title":"Research Papers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RobustNeurlaNetworks.jl is built on the REN and LBDN model parameterisations described in the following two papers (respectively):","category":"page"},{"location":"","page":"Home","title":"Home","text":"M. Revay, R. Wang, and I. R. Manchester, \"Recurrent equilibrium networks: Flexible dynamic models with guaranteed stability and robustness,\" April 2021. doi: https://doi.org/10.48550/arXiv.2104.05942.","category":"page"},{"location":"","page":"Home","title":"Home","text":"R. Wang and I. R. Manchester, \"Direct parameterization of Lipschitz-bounded deep networks,\" January 2023. doi: https://doi.org/10.48550/arXiv.2301.11526.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The REN parameterisation was extended to continuous-time systems in:","category":"page"},{"location":"","page":"Home","title":"Home","text":"D. Martinelli, C. L. Galimberti, I. R. Manchester, L. Furieri, and G. Ferrari-Trecate, \"Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations,\" April 2023. doi: https://doi.org/10.48550/arXiv.2304.02976.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See below for a collection of projects and papers using RobustNeuralNetworks.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"N. H. Barbara, R. Wang, and I. R. Manchester, \"Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems,\" April 2023. doi: https://doi.org/10.48550/arXiv.2304.06193.","category":"page"},{"location":"lib/ren/","page":"Recurrent Equilibrium Networks","title":"Recurrent Equilibrium Networks","text":"Pages = [\"ren.md\"]","category":"page"},{"location":"lib/ren/#Recurrent-Equilibrium-Networks-(REN)","page":"Recurrent Equilibrium Networks","title":"Recurrent Equilibrium Networks (REN)","text":"","category":"section"},{"location":"lib/ren/","page":"Recurrent Equilibrium Networks","title":"Recurrent Equilibrium Networks","text":"AbstractREN\nDiffREN\nREN\nWrapREN","category":"page"},{"location":"lib/ren/#RobustNeuralNetworks.AbstractREN","page":"Recurrent Equilibrium Networks","title":"RobustNeuralNetworks.AbstractREN","text":"abstract type AbstractREN end\n\nExplicit parameterisation for recurrent equilibrium networks.\n\n(m::AbstractREN)(xt::VecOrMat, ut::VecOrMat)\n\nCall an  AbstractREN model given internal states xt and inputs ut. \n\nIf arguments are matrices, each column must be a vector of states or inputs (allows batch simulations).\n\nExamples\n\nThis example creates a contracting REN using ContractingRENParams and calls the model with some randomly generated inputs. \n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 2, 20, 1\n\n# Construct a REN\ncontracting_ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng=rng)\nren = REN(contracting_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\nprintln(round.(y1;digits=2))\n\n# output\n\n[-31.41 0.57 -0.55 -3.56 -35.0 -18.28 -25.48 -7.49 -4.14 15.31]\n\nSee also REN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren/#RobustNeuralNetworks.DiffREN","page":"Recurrent Equilibrium Networks","title":"RobustNeuralNetworks.DiffREN","text":"DiffREN(ps::AbstractRENParams{T}) where T\n\nConstruct a differentiable REN from its direct parameterisation.\n\nDiffREN is an alternative to REN and WrapREN that computes the explicit parameterisation every time the model is called. This is slow and computationally inefficient. However, it can be trained with Flux.jl  (unlike WrapREN) and does not need to re-created if the parameters are updated (unlike REN).\n\nThe key feature is that the ExplicitParams struct is never stored, so an instance of DiffREN never has to be mutated or re-defined after it is created, even when learnable parameters are updated.\n\nSee also AbstractREN, REN, and WrapREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren/#RobustNeuralNetworks.REN","page":"Recurrent Equilibrium Networks","title":"RobustNeuralNetworks.REN","text":"REN(ps::AbstractRENParams{T}) where T\n\nConstruct a REN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of REN (eg: a GeneralRENParams instance) and converts it to a callable explicit parameterisation of the REN.\n\nSee also AbstractREN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/ren/#RobustNeuralNetworks.WrapREN","page":"Recurrent Equilibrium Networks","title":"RobustNeuralNetworks.WrapREN","text":"WrapREN(ps::AbstractRENParams{T}) where T\n\nConstruct REN wrapper from its direct parameterisation.\n\nWrapREN is an alternative to REN that stores the AbstractRENParams and ExplicitParams within the same object. This means that a new REN object does not have to be created each time the parameters are updated. Explicit REN parameters must be updated by the user if the direct parameters have changed.\n\nNote that WrapREN cannot be used with Flux.jl, since it relies on mutating the WrapREN instance.\n\nExamples\n\nIn this example, we create a REN satisfying some generic behavioural constraints and demonstrate how to update the REN wrapper if model parameters are changed.\n\nusing LinearAlgebra\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\n\nQ = Matrix{Float64}(-I(ny))\nR = 0.1^2 * Matrix{Float64}(I(nu))\nS = zeros(Float64, nu, ny)\n\n# Construct a REN\nren_ps = GeneralRENParams{Float64}(nu, nx, nv, ny, Q, S, R; rng=rng)\nren = WrapREN(ren_ps)\n\n# Some dummy inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0) \n\n# Update the model after changing a parameter\nren.params.direct.B2 .*= rand(rng, size(ren.params.direct.B2)...)\nupdate_explicit!(ren)\n\nprintln(round(ren.explicit.B2[1];digits=4))\n\nSee also AbstractREN, REN, and DiffREN.\n\n\n\n\n\n","category":"type"}]
}
