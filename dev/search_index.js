var documenterSearchIndex = {"docs":
[{"location":"introduction/developing/#Contributing-to-the-Package","page":"Contributing to the Package","title":"Contributing to the Package","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All contributors welcome! Please contact nicholas.barbara@sydney.edu.au with any questions.","category":"page"},{"location":"introduction/developing/#Installation-for-Development","page":"Contributing to the Package","title":"Installation for Development","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the package, please clone the repository into your ~/.julia/dev/ directory with","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"git clone git@github.com:acfr/RobustNeuralNetworks.jl.git RobustNeuralNetworks","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"info: The name matters\nNote that the repository is named RobustNeuralNetworks.jl but your folder should be named RobustNeuralNetworks (no .jl). This is convention for Julia packages, and will ensure the package manager knows where to look.Also make sure you have cloned the repo inside your ~/.julia/dev/ folder for the following instructions to work. If the dev/ directory does not exist, create it. ","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Navigate to ~/.julia/dev/RobustNeuralNetworks/, start a Julia session, and type the following in the REPL to activate the package.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"using Pkg\nPkg.activate(\".\")\nPkg.instantiate()","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Check that the example in Getting Started runs without errors and matches the given output before continuing.","category":"page"},{"location":"introduction/developing/#Git-Workflow","page":"Contributing to the Package","title":"Git Workflow","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The package is just another git repository, so the development workflow is similar to most projects.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"When developing features, please create a new branch labelled feature/<some_descriptive_name>. For example, the branch feature/documentation is where this documentation was first written and tested.\nIf you notice a bug, please create a git issue and a new branch associated with that issue to let others know what you are working on. The branch should be deleted once the issue is closed.\nSubmit pull requests once you have completed a new feature and tested it thorougly. All pull requests must be approved by at least one other developer of the package, and must pass the continuous integration pipeline.","category":"page"},{"location":"introduction/developing/#Package-Structure","page":"Contributing to the Package","title":"Package Structure","text":"","category":"section"},{"location":"introduction/developing/#Layout","page":"Contributing to the Package","title":"Layout","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The package is divided into four main directories:","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The src/ folder contains all source code required for users of the package.\nThe examples/ folder contains complete examples that are referenced in the documentation, and any assets or results required or produced by the examples.\nThe docs/ folder contains all files related to the documentation.\nThe test/ folder contains the main runtests.jl file and all other test files loaded within it.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"info: Separate projects\nNote that the examples/ and docs/ folders each have their own Project.toml (and Manifest.toml). You can activate them by typing] activate docs(or similar for examples) into the REPL from the home directory of the repository. Please do not add packages to the main Project.toml if they are not required by RobustNeuralNetworks.jl itself.","category":"page"},{"location":"introduction/developing/#Main-file","page":"Contributing to the Package","title":"Main file","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The main file is src/RobustNeuralNetworks.jl. This imports all relevant packages, defines abstract types, includes code from other files, and exports the necessary components of our package. ","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All using PackageName statements should be included in this file.\nOnly import packages that are absolutely required.\nIf you only need one function from a package, import it explicitly (not the whole package). For example:","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"using Flux: relu","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"When including files in the src/ folder, the order matters. Code should only ever be included with a single include statement in the main file. Please follow the convention outlined in the comments.","category":"page"},{"location":"introduction/developing/#Source-files","page":"Contributing to the Package","title":"Source files","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The source files for our package are all in the src/ directory, and are split into the following sub-directories.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"src/Base/: Contains code relevant to the core package functionality.\nsrc/ParameterTypes/: Contains the various REN and LBDN direct parameterisations, which are all subtypes of AbstractRENParams or AbstractLBDNParams.\nsrc/Wrappers/: Contains wrappers used to define explicit (callable) REN and LBDN models (eg: subtypes of AbstractREN and AbstractLBDN).","category":"page"},{"location":"introduction/developing/#Writing-tests","page":"Contributing to the Package","title":"Writing tests","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Once you have written any code for this package, be sure to test it thoroughly. You should also consider adding test scripts to the test/ directory.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"See the documentation for Test.jl for help with writing good package tests.\nRun all tests for the package with ] test in the REPL.\nAll tests will be run by the CI client when submitting pull requests to the main git branch.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Currently, the tests in place check that the various model parameterisations satisfy the constraints they should, and that the models are differentiable with Flux.jl.","category":"page"},{"location":"introduction/developing/#Writing-documentation","page":"Contributing to the Package","title":"Writing documentation","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the docs, this page provides a great outline of the required workflow. In summary:","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All documentation should be written in markdown (.md files) within the docs/ folder.\nPlease add documentation in the docs/src/ folder.\nTo build the docs locally from the ~/.julia/dev/RobustNeuralNetworks/ directory, activate the Julia REPL and enter the following.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"using Pkg\nPkg.activate(\"./docs\")\n\nusing LiveServer\nservedocs()","category":"page"},{"location":"examples/lbdn_mnist/#Image-Classification-with-LBDN","page":"Image Classification","title":"Image Classification with LBDN","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Full example code can be found here.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Our next example features an LBDN trained to classify the MNIST dataset. We showed in Wang & Manchester (2023) that training image classifiers with LBDNs makes them robust to adversarial attacks thanks to the built-in Lipschitz bound. In this example, we will demonstrate how to train an LBDN model on the MNIST dataset with the following steps:","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Load the training and test data\nDefine a Lipschitz-bounded model\nDefine a loss function\nTrain the model to minimise the loss function\nEvaluate the trained model\nInvestigate robustness","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"For details on how Lipschitz bounds increase classification robustness and reliability, please see the paper.","category":"page"},{"location":"examples/lbdn_mnist/#.-Load-the-data","page":"Image Classification","title":"1. Load the data","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Let's start by loading the training and test data. MLDatasets.jl contains a number of common machine-learning datasets, including the MNIST dataset. The following code loads the full dataset of 60,000 training images and 10,000 test images.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"info: Working on the GPU\nSince we're dealing with images, we will load our data and models onto the GPU to speed up training. We'll be using CUDA.jl. If you don't have a GPU on your machine, just switch to dev = cpu. If you have a GPU but not an NVIDIA GPU, switch out CUDA.jl with whichever GPU backend supports your device. For more information on training models on a GPU, see here.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using CUDA\nusing MLDatasets: MNIST\n\n# Choose device\ndev = gpu\n# dev = cpu\n\n# Get MNIST training and test data\nT = Float32\nx_train, y_train = MNIST(T, split=:train)[:] |> dev\nx_test,  y_test  = MNIST(T, split=:test)[:] |> dev","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"The feature matrices x_train and x_test are three-dimensional arrays where each 28x28 layer contains pixel data for a single handwritten number from 0 to 9 (see below for an example). The labels y_train and y_test are vectors containing the classification of each image as a number from 0 to 9. We can convert each of these to an input/output format better suited to training with Flux.jl.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using Flux\nusing Flux: OneHotMatrix\n\n# Reshape features for model input\nx_train = Flux.flatten(x_train)\nx_test  = Flux.flatten(x_test)\n\n# Encode categorical outputs and store training data\ny_train = Flux.onehotbatch(y_train, 0:9)\ny_test  = Flux.onehotbatch(y_test,  0:9)\ntrain_data = [(x_train, y_train)]","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Features are now stored in a 28xN Matrix where each column contains pixel data from a single image, and the labels have been converted to a 10xN OneHotMatrix where each column contains a 1 in the row corresponding to the image's classification (eg: row 3 for an image showing the number 2) and a 0 otherwise.","category":"page"},{"location":"examples/lbdn_mnist/#.-Define-a-model","page":"Image Classification","title":"2. Define a model","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"We can now construct an LBDN model to train on the MNIST dataset. The larger the model, the better the classification accuracy will be, at the cost of longer training times. The smaller the Lipschitz bound gamma, the more robust the model will be to input perturbations (such as noise in the image). If gamma is too small, however, it can restrict the model flexibility and limit the achievable performance. For this example, we use a small network of two 64-neuron hidden layers and set a Lipschitz bound of gamma=50 just to demonstrate the method.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using Random\nusing RobustNeuralNetworks\n\n# Random seed for consistency\nrng = MersenneTwister(42)\n\n# Model specification\nnu = 28*28              # Number of inputs (size of image)\nny = 10                 # Number of outputs (possible classifications)\nnh = fill(64,2)         # 2 hidden layers, each with 64 neurons\nγ  = 5.0f0              # Lipschitz bound of 5.0\n\n# Set up model: define parameters, then create model\nmodel_ps = DenseLBDNParams{T}(nu, nh, ny, γ; rng)\nmodel = Chain(DiffLBDN(model_ps), Flux.softmax) |> dev","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"The model consisnts of two parts. The first is a callable DiffLBDN model constructed from its direct parameterisation, which is defined by an instance of DenseLBDNParams (see the Package Overview for more detail). The output is then converted to a probability distribution using a softmax layer. Note that all AbstractLBDN models can be combined with traditional neural network layers using Flux.Chain. We could also have used SandwichFC layers to build the network, as outlined in Fitting a Curve with LBDN. The final model is loaded onto whichever device dev you chose in 1. Load the data.","category":"page"},{"location":"examples/lbdn_mnist/#.-Define-a-loss-function","page":"Image Classification","title":"3. Define a loss function","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"A typical loss function for training on datasets with discrete labels is the cross entropy loss. We can use the crossentropy loss function shipped with Flux.jl.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"# Loss function\nloss(model,x,y) = Flux.crossentropy(model(x), y)","category":"page"},{"location":"examples/lbdn_mnist/#.-Train-the-model","page":"Image Classification","title":"4. Train the model","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Before training the model to minimise the cross entropy loss, we can set up a callback function to evaluate the model performance during training.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using Statistics\n\n# Check test accuracy during training\ncompare(y::OneHotMatrix, ŷ) = maximum(ŷ, dims=1) .== maximum(y.*ŷ, dims=1)\naccuracy(model, x, y::OneHotMatrix) = mean(compare(y, model(x)))\n\n# Callback function to show results while training\nfunction progress(model, iter)\n    train_loss = round(loss(model, x_train, y_train), digits=4)\n    test_acc = round(accuracy(model, x_test, y_test), digits=4)\n    @show iter train_loss test_acc\n    println()\nend","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Let's train the model over 600 epochs using two learning rates: 1e-3 for the first 300, and 1e-4 for the last 300. We'll use the Adam optimiser and the default Flux.train! method. Once the model has been trained, we can save it for later with the BSON package. Note that Flux.train! updates the learnable parameters each time the model is evaluated on a batch of data, hence our choice of DiffLBDN over LBDN as a model wrapper.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using BSON\n\n# Train with the Adam optimiser, and display progress every 50 steps\nfunction train_mnist!(model, data; num_epochs=300, lrs=[1e-3,1e-4])\n    opt_state = Flux.setup(Adam(lrs[1]), model)\n    for k in eachindex(lrs)    \n        for i in 1:num_epochs\n            Flux.train!(loss, model, data, opt_state)\n            (i % 50 == 0) && progress(model, i)\n        end\n        (k < length(lrs)) && Flux.adjust!(opt_state, lrs[k+1])\n    end\nend\n\n# Train and save the model for later\ntrain_mnist!(model, train_data)\nbson(\"lbdn_mnist.bson\", Dict(\"model\" => model |> cpu))","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Note that we move the model back to the cpu before saving it!","category":"page"},{"location":"examples/lbdn_mnist/#.-Evaluate-the-trained-model","page":"Image Classification","title":"5. Evaluate the trained model","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Our final model has a test accuracy of about 97% the full 10,000-image test set. We could improve this further by (for example) using a larger model, training the model for longer, fine-tuning the learning rate, or switching to the convolutional LBDN from Wang & Manchester (2023) (yet to be implemented in this package).","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"# Print final results\ntrain_acc = accuracy(model, x_train, y_train)*100\ntest_acc  = accuracy(model, x_test,  y_test)*100\nprintln(\"Training accuracy: $(round(train_acc,digits=2))%\")\nprintln(\"Test accuracy:     $(round(test_acc,digits=2))%\")","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"println(\"Training accuracy:\",\" 98.15%\") #hide\nprintln(\"Test accuracy:\",\"     97.24%\") #hide","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Let's have a look at some examples too.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"using CairoMakie\n\n# Make a couple of example plots\nindx = rand(rng, 1:100, 3)\nfig = Figure(resolution = (800, 300))\nfor i in eachindex(indx)\n\n    # Get data and do prediction\n    x = x_test[:,indx[i]]\n    y = y_test[:,indx[i]]\n    ŷ = model(x)\n\n    # Make sure data is on CPU for plotting\n    x = x |> cpu\n    y = y |> cpu\n    ŷ = ŷ |> cpu\n\n    # Reshape data for plotting\n    xmat = reshape(x, 28, 28)\n    yval = (0:9)[y][1]\n    ŷval = (0:9)[ŷ .== maximum(ŷ)][1]\n\n    # Plot results\n    ax, _ = image(fig[1,i], xmat, axis=(\n            yreversed = true, \n            aspect = DataAspect(), \n            title = \"True class: $(yval), Prediction: $(ŷval)\"))\n\n    # Format the plot\n    ax.xticksvisible = false\n    ax.yticksvisible = false\n    ax.xticklabelsvisible = false\n    ax.yticklabelsvisible = false\nend\nsave(\"lbdn_mnist.svg\", fig)","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"(Image: )","category":"page"},{"location":"examples/lbdn_mnist/#.-Investigate-robustness","page":"Image Classification","title":"6. Investigate robustness","text":"","category":"section"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"The main advantage of using an LBDN for image classification is its built-in robustness to noise (or attacks) added to the image data. This robustness is a direct benefit of the Lipschitz bound. As explained in the Package Overview, the Lipschitz bound effectively defines how \"smooth\" the network is: the smaller the Lipschitz bound, the less the network outputs will change as the inputs vary. For example, small amounts of noise added to the image will be less likely to change its classification. A detailed investigation into this effect is presented in Wang & Manchester (2023).","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"We can see this effect first-hand by comparing the LBDN to a standard MLP built from Flux.Dense layers. Let's first create a dense network with the same layer structure as the LBDN, and train it with the same train_mnist!() function from earlier.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"# Create a Dense network\ninit = Flux.glorot_normal(rng)\ninitb(n) = Flux.glorot_normal(rng, n)\ndense = Chain(\n    Dense(nu, nh[1], Flux.relu; init, bias=initb(nh[1])),\n    Dense(nh[1], nh[2], Flux.relu; init, bias=initb(nh[2])),\n    Dense(nh[2], ny; init, bias=initb(ny)),\n    Flux.softmax\n) |> dev\n\n# Train it and save for later\ntrain_mnist!(dense, train_data)\nbson(\"dense_mnist.bson\", Dict(\"model\" => dense |> cpu))","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"The trained model performs similarly to the LBDN on the original test dataset.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"# Print final results\ntrain_acc = accuracy(dense, x_train, y_train)*100\ntest_acc  = accuracy(dense, x_test,  y_test)*100\nprintln(\"Training accuracy: $(round(train_acc,digits=2))%\")\nprintln(\"Test accuracy:     $(round(test_acc,digits=2))%\")","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"println(\"Training accuracy:\",\" 97.65%\") #hide\nprintln(\"Test accuracy:\",\"     96.61%\") #hide","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"As a simple test of robustness, we'll add uniformly-sampled random noise in the range -epsilon epsilon to the pixel data in the test dataset for a range of noise magnitudes epsilon in 0 200255 We can record the test accuracy for each perturbation size and store it for plotting.","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"# Get test accuracy as we add noise\nuniform(x) = 2*rand(rng, T, size(x)...) .- 1 |> dev\nfunction noisy_test_error(model, ϵ=0)\n    noisy_xtest = x_test .+ ϵ*uniform(x_test)\n    accuracy(model, noisy_xtest,  y_test)*100\nend\n\nϵs = T.(LinRange(0, 200, 10)) ./ 255\nlbdn_error = noisy_test_error.((model,), ϵs)\ndense_error = noisy_test_error.((dense,), ϵs)\n\n# Plot results\nfig = Figure(resolution=(500,300))\nax1 = Axis(fig[1,1], xlabel=\"Perturbation size\", ylabel=\"Test accuracy (%)\")\nlines!(ax1, ϵs, lbdn_error, label=\"LBDN (γ=5)\")\nlines!(ax1, ϵs, dense_error, label=\"Dense\")\n\nxlims!(ax1, 0, 0.8)\naxislegend(ax1, position=:lb)\nsave(\"lbdn_mnist_robust.svg\", fig)","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"(Image: )","category":"page"},{"location":"examples/lbdn_mnist/","page":"Image Classification","title":"Image Classification","text":"Plotting the results very clearly shows that the dense network, which has no guarantees on its Lipschitz bound, quickly loses its accuracy as small amounts of noise are added to the image. In contrast, the LBDN model maintains its accuracy even when the (maximum) perturbation size is as much as 80% of the maximum pixel values. This is an illustration of why image classification is one of the most promising use-cases for LBDN models. For a more detailed comparison of LBDN with state-of-the-art image classification methods, see Wang & Manchester (2023).","category":"page"},{"location":"examples/echo_ren/#(Convex)-Nonlinear-Control-with-REN","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control with REN","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"This example was first presented in Section IX of Revay, Wang & Manchester (2021). Full example code can be found here.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"RENs and LBDNs can be used for a lot more than just learning-based problems. In this example, we'll see how RENs can be used to design nonlinear feedback controllers with stability guarantees for linear dynamical systems with constraints. Introducing constraints (eg: minimum/maximum control inputs) often means that nonlinear controllers perform better than linear policies. A common approach is to use Model Predictive Control (MPC). In our case, we'll use convex optimisation to design a nonlinear controller. The controller will be an echo state network based on a contracting REN. We'll use this alongside the Youla-Kucera parameterisation to guarantee stability of the final controller.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"For a detailed explanation of the theory behind this example, please read Section IX of the original paper. For more on using RENs with the Youla parameterisation, see Wang et al. (2022) and Barbara, Wang & Manchester (2023).","category":"page"},{"location":"examples/echo_ren/#.-Background-theory","page":"(Convex) Nonlinear Control","title":"1. Background theory","text":"","category":"section"},{"location":"examples/echo_ren/#Stabilising-a-linear-system","page":"(Convex) Nonlinear Control","title":"Stabilising a linear system","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"We'll start with some background on the structure of linear systems and output-feedback controllers. Consider a discrete-time linear system with state vector x_t, control signal u_t, external inputs d_t, measured output y_t and some performance variable z_t to be kept small.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"beginaligned\nx_t+1 = mathbbAx_t + mathbbB_1 d_t + mathbbB_2 u_t \nz_t = mathbbC_1 x_t + mathbbD_11 d_t + mathbbD_12 u_t \ny_t = mathbbC_2 x_t + mathbbD_21 d_t\nendaligned","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"A typical choice of stabilising controller is an output-feedback structure with state estimate hatx_t and observer/controller gain matrices L and K, respectively.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"beginaligned\nhatx_t+1 = mathbbAhatx_t + mathbbB_2 u_t + L tildey_t \ntildey_t = y_t - mathbbC_2 hatx_t \nu_t = -Khatx_t + tildeu_t\nendaligned","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"We have also included an additional signal tildeu_t to augment the control inputs u_t With a little bit of algebra, the closed-loop dynamics of the system can be written in the following form, where mathcalT_0 mathcalT_1 mathcalT_2 are linear systems.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"beginbmatrix\nz  tildey\nendbmatrix\n= \nbeginbmatrix\nmathcalT_0  mathcalT_1  mathcalT_2  0\nendbmatrix\nbeginbmatrix\nd  tildeu\nendbmatrix","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Notice that there is no coupling between tildey and tildeu. ","category":"page"},{"location":"examples/echo_ren/#Controller-augmentation","page":"(Convex) Nonlinear Control","title":"Controller augmentation","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"The linear controller will stabilise our linear dynamical system in the absence of any constraints. But what if we want to shape the closed-loop response to meet some user-defined design criteria without losing stability? For example, what if we want to keep the control signal in some safe range u_mathrmmin  u_t  u_mathrmmax at all times?","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"It turns out that if we augment the original controller with tildeu = mathcalQ(tildey) where mathcalQ is a contracting system then the closed-loop system is guaranteed to remain stable. This is incredibly useful for optimal control design. For example, we could use a contracting REN as our parameter mathcalQ and optimise it to meet some performance specifications (like control constrains), knowing that final closed-loop system is guaranteed to be stable. The closed-loop response can be written as follows.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"z = mathcalT_0 d + mathcalT_1 mathcalQ(mathcalT_2 d)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"This is an old idea in linear control theory called the Youla-Kucera parameterisation. We extended it to nonlinear models (like RENs) and nonlinear dynamical systems in Wang et al. (2022) and Barbara, Wang & Manchester (2023), respectively.","category":"page"},{"location":"examples/echo_ren/#Echo-state-networks-with-REN","page":"(Convex) Nonlinear Control","title":"Echo state networks with REN","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Now that we've decided on a structure for our control framework, we need a way to create and optimise a contracting mathcalQ to meet our design criteria. We could directly use a contracting REN for mathcalQ and train it with reinforcement learning, thereby learning over the space of all stabilising controllers  for this linear system. While it's very useful to have this option, sometimes we'll want a more efficient solution. Enter convex optimisation with echo state networks.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Let's say mathcalQ has learnable parameters theta. Suppose our problem is to minimise some convex objective function J(z) subject to a set of convex constraints. I.e:","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"min_theta J(z) quad textst quad c(z) le 0","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"An echo state network is a dynamic model with randomly sampled but fixed dynamics and a learnable output map. We can create contracting echo state networks mathcalQ with contracting RENs. When a REN model is called, it can be viewed as a system with the following form (see ExplicitRENParams).","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"beginequation*\nbeginbmatrix\nbarx_t+1  v_t  bary_t\nendbmatrix\n= \nbeginbmatrix\nA  B_1  B_2 \nC_1  D_11  D_12 \nC_2  D_21  D_22 \nendbmatrix\nbeginbmatrix\nbarx_t  w_t  baru_t\nendbmatrix\n+ \nbeginbmatrix\nb_x  b_v  b_y\nendbmatrix\nendequation*\nquad textwhere quad w_t = sigma(v_t)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Note that sigma is the nonlinear activation function (eg: a ReLU). The inputs and outputs of the REN are baru_t and bary_t, respectively. We can therefore create a contracting echo state network by randomly initialising a contracting REN whose outputs are barx_t w_t baru_t and separately optimising the output layer ","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"bary_t = C_2 barx_t + D_21 w_t + D_22 baru_t + b_y","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"where the learnable parameters are theta = C_2  D_21  D_22  b_y","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"The advantage of using an echo state network for mathcalQ in the Youla parameterisation is that the problem is entirely convex in theta This means we can solve for the best choice of theta using standard convex optimisation tools. To see why, think of the echo state network as mathcalQ(tildey) = sum_i theta_i mathcalQ_i(tildey) where the mathcalQ_i are defined by the contracting REN. The closed-loop dynamics are therefore affine in theta since mathcalT_1 is linear.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"z = mathcalT_0 d + sum_i theta_i mathcalT_1 mathcalQ_i(mathcalT_2 d)","category":"page"},{"location":"examples/echo_ren/#.-Problem-setup","page":"(Convex) Nonlinear Control","title":"2. Problem setup","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Let's consider a simple discrete-time linear system whose closed-loop transfer functions are","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"mathcalT_0 = mathcalT_1 = -mathcalT_2 = frac03q^2 - 2rho cos(phi)q + rho^2","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"where q is the shift operator and rho = 08 phi = 02pi ControlSystems.jl offers a nice interface for working with discrete-time transfer functions.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using ControlSystems\n\n# System parameters and poles: λ = ρ*exp(± im ϕ)\nρ = 0.8\nϕ = 0.2π\nλ = ρ .* [cos(ϕ) + sin(ϕ)*im, cos(ϕ) - sin(ϕ)*im]\n\n# Construct discrete-time system with gain 0.3, sampling time 1.0s\nk = 0.3\nTs = 1.0\nsys = zpk([], λ, k, Ts)\n\n# Closed-loop system components\nsim_sys(u::AbstractMatrix) = lsim(sys, u, 1:size(u,2))[1]\nT0(u) = sim_sys(u)\nT1(u) = sim_sys(u)\nT2(u) = -sim_sys(u)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"tip: Our goal\nOur aim is to minimise the ell^1 norm of the performance objective z while constraining the control input to -5 le u_t le 5 at all times in response to step inputs with an amplitude of 10.0.","category":"page"},{"location":"examples/echo_ren/#.-Generate-training-data","page":"(Convex) Nonlinear Control","title":"3. Generate training data","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"We'll generate a long trajectory of sample inputs (\"disturbances\") d consisting of a random step every 50 time samples. The step amplitude is at most 10.0.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using Random\nusing LinearAlgebra\n\nrng = MersenneTwister(1)\n\n# Sample disturbances\nfunction sample_disturbance(amplitude=10, samples=500, hold=50)\n    d = 2 * amplitude * (rand(rng, 1, samples) .- 0.5)\n    return kron(d, ones(1, hold))\nend\nd = sample_disturbance()","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Here's a plot of the inputs over the first 1000 time samples.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using CairoMakie\n\n# Check out the disturbance\nf = Figure(resolution = (600, 400))\nax = Axis(f[1,1], xlabel=\"Time steps\", ylabel=\"Output\")\nlines!(ax, vec(d)[1:1000],  label=\"Disturbance\")\naxislegend(ax, position=:rt)\ndisplay(f)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"(Image: )","category":"page"},{"location":"examples/echo_ren/#.-Define-a-stable-echo-state-network","page":"(Convex) Nonlinear Control","title":"4. Define a stable echo state network","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Now that we have training data, let's define a stable echo state network straight from a contracting REN, as described above. We'll start by creating a contracting REN whose inputs are baru = tildey and outputs are barx w baru.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using RobustNeuralNetworks\n\n# Initialise a contracting REN\nnu = 1\nnx, nv = 50, 500\nny = nx + nv + nu\nren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng)\nmodel  = REN(ren_ps)\n\n# Make sure the outputs are yt = [xt; wt; ut]\nmodel.explicit.C2  .= [I(nx); zeros(nv, nx); zeros(nu, nx)]\nmodel.explicit.D21 .= [zeros(nx, nv); I(nv); zeros(nu, nv)]\nmodel.explicit.D22 .= [zeros(nx, nu); zeros(nv, nu); I(nu)]\nmodel.explicit.by  .= zeros(ny)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"When we simulate the REN components, we add a row of ones to the output to multiply the output bias vector b_y in theta","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"# Echo-state components (add ones for bias vector)\nfunction Qᵢ(u)\n    x0 = init_states(model, size(u,2))\n    _, y = model(x0, u)\n    return [y; ones(1,size(y,2))]\nend","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"The last part of the echo state network is the optimisable output map, which we can set up with Convex.jl.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using Convex\n\n# Echo-state network params θ = [C2 D21 D22 by]\nθ = Convex.Variable(1, nx+nv+nu+1)","category":"page"},{"location":"examples/echo_ren/#.-Optimise-the-model","page":"(Convex) Nonlinear Control","title":"5. Optimise the model","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Now that we've defined the model, we can simulate the closed-loop system and optimise it to meet our design requirements. We need a function that computes the performance signal z and control inputs u with the echo state network as our augmenting system mathcalQ.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"# Complete the closed-loop response and control inputs \n# z = T₀ + ∑ θᵢ*T₁(Qᵢ(T₂(d)))\n# u = ∑ θᵢ*Qᵢ(T₂(d))\nfunction sim_echo_state_network(d, θ)\n    z0 = T0(d)\n    ỹ  = T2(d)\n    ũ  = Qᵢ(ỹ)\n    z1 = reduce(vcat, T1(ũ') for ũ in eachrow(ũ))\n    z  = z0 + θ * z1\n    u  = θ * ũ\n    return z, u, z0\nend\nz, u, _= sim_echo_state_network(d, θ)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"The variables z and u have been constructed through Convex.jl, so we can use them to define our objective function and constraints. That is, keep the ell^1 norm of z small and the control inputs between -5 le u le 5 We've added a small regularisation term to the objective function to help with numerical conditioning.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"# Cost function and constraints\nJ = norm(z, 1) + 1e-4*(sumsquares(u) + norm(θ, 2))\nconstraints = [u < 5, u > -5]","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"With the problem all nicely defined, all we have to do is solve it and investigate the resulting control system. We used the Mosek solver with Mosek.jl. Note that Mosek requires a license. A free academic license can be obtained from this link. You could try using any of the other solvers compatible with Convex.jl, but second-order interior point methods will be the most reliable.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"using BSON\nusing Mosek, MosekTools\n\n# Optimise the closed-loop response\nproblem = minimize(J, constraints)\nConvex.solve!(problem, Mosek.Optimizer)\n\n# Save the parameters\nθ_solved = evaluate(θ)\nbson(\"../results/echo_ren_params.bson\", Dict(\"params\" => θ_solved))","category":"page"},{"location":"examples/echo_ren/#.-Evaluate-the-model","page":"(Convex) Nonlinear Control","title":"6. Evaluate the model","text":"","category":"section"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"We can now assess the closed-loop performance of our system under the optimised nonlinear controller. We'll first generate some test data: repeating square waves of increasing amplitude.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"# Test on different inputs\na_test = range(0, length=7, stop=8)\nd_test = reduce(hcat, a .* [ones(1, 50) zeros(1, 50)] for a in a_test)\nz_test, u_test, z0_test = sim_echo_state_network(d_test, θ_solved)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"Now plot the closed-loop response and required control signal.","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"# Plot the results\nf = Figure(resolution = (1000, 400))\nga = f[1,1] = GridLayout()\n\n# Response\nax1 = Axis(ga[1,1], xlabel=\"Time steps\", ylabel=\"Output\")\nlines!(ax1, vec(d_test),  label=\"Disturbance\")\nlines!(ax1, vec(z0_test), label=\"Open Loop\")\nlines!(ax1, vec(z_test),  label=\"Echo-REN\")\naxislegend(ax1, position=:lt)\n\n# Control inputs\nax2 = Axis(ga[1,2], xlabel=\"Time steps\", ylabel=\"Control signal\")\nlines!(ax2, vec(u_test), label=\"Echo-REN\")\nlines!(\n    ax2, [1, length(u_test)], [-5, -5], \n    color=:black, linestyle=:dash, label=\"Constraints\"\n)\nlines!(ax2, [1, length(u_test)], [5, 5], color=:black, linestyle=:dash)\naxislegend(ax2, position=:rt)\n\ndisplay(f)","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"(Image: )","category":"page"},{"location":"examples/echo_ren/","page":"(Convex) Nonlinear Control","title":"(Convex) Nonlinear Control","text":"In open loop (i.e: just the system mathcalT_0 without our echo state REN), the performance output increases linearly with the disturbance amplitude. When we add our optimised \"Echo-REN\", it returns the performance output to zero as quickly as possible without exceeding the pm 5 limits on the control signal. The steady-state amplitude only starts to deviate from zero when the control signal reaches its limits.","category":"page"},{"location":"examples/pde_obsv/#PDE-Observer-Design-with-REN","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"This example was first presented in Section VIII of Revay, Wang & Manchester (2021). Full example code can be found here.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"This example considers learning a state observer for a reaction-diffusion PDE. See Observer Design with REN for a brief explanation of the theory, and Revay, Wang & Manchester (2021) for a detailed overview of the original problem.","category":"page"},{"location":"examples/pde_obsv/#.-Problem-statement","page":"PDE Observer Design with REN","title":"1. Problem statement","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"We consider designing an observer for the following semi-linear reaction-diffusion partial differential equation.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"beginaligned\nfracpartial xi(zt)partial t = fracpartial^2 xi(zt)partial z^2+R(xizt)\nxi (z0)=1\nxi(1t)=xi(0t)=b(t)\ny=g(xizt)\nR(xi z t)=frac12xi(1-xi)(xi-frac12)\nendaligned","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"where the state xi(zt) is a function of both spatial coordinate z in 01 and time. The boundary condition is considered to be a known input and we assume there is a single measurement taken from the center of the spatial domain: y(t)=xi(05t) We discretize z into N intervals with z^i=iDelta z Then the state of spatial coordinate is described by xi^i_t=xi(z^it) The dynamics over a time period Delta t can be approximated using the finite difference fracpartial xi(zt)partial t approx fracxi^i_t+Delta t-xi ^i_tDelta t Substituting these into the reaction-diffusion PDEs, we can develop the state-space form:","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"barxi_t+Delta t=a_RD(barxi_tb_t) quad y_t=c_RD(barxi_t)","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using LinearAlgebra\nusing Statistics\n\ndtype = Float64\n# Problem setup\nnx = 51             # Number of states\nn_in = 1            # Number of inputs\nL = 10.0            # Size of spatial domain\nsigma = 0.1         # Used to construct time step\n\n# Discretise space and time\ndx = L / (nx - 1)\ndt = sigma * dx^2\n\n# State dynamics and output functions f, g\nfunction f(u0, d)\n    u, un = copy(u0), copy(u0)\n    for _ in 1:5\n        u = copy(un) \n\n        # FD approximation of heat equation\n        f_local(v) = v[2:end - 1, :] .* (1 .- v[2:end - 1, :]) .* ( v[2:end - 1, :] .- 0.5)\n        laplacian(v) = (v[1:end - 2, :] + v[3:end, :] - 2v[2:end - 1, :]) / dx^2\n        \n        # Euler step for time\n        un[2:end - 1, :] = u[2:end - 1, :] + dt * (laplacian(u) + f_local(u) / 2 )\n\n        # Boundary condition\n        un[1:1, :] = d;\n        un[end:end, :] = d;\n    end\n    return u\nend\n\ng(u, d) = [d; u[end ÷ 2:end ÷ 2, :]]\n","category":"page"},{"location":"examples/pde_obsv/#.-Generate-training-data","page":"PDE Observer Design with REN","title":"2. Generate training data","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"We will generate training data for t=0cdots10^5Delta t by simulating the system with N=50 intervals and 10^5 time steps with the stochastic input b_t+1=b_t+005w_t where w_tsim mathcalN01","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using Random\n\n# Generate simulated data\nfunction get_data(npoints=1000; init=zeros)\n\n    X = init(dtype, nx, npoints)\n    U = init(dtype, n_in, npoints)\n\n    for t in 1:npoints-1\n\n        # Next state\n        X[:, t+1] = f(X[:, t], U[:, t])\n        \n        # Next input bₜ\n        u_next = U[t] + 0.05f0*randn(dtype)\n        (u_next > 1) && (u_next = 1)\n        (u_next < 0) && (u_next = 0)\n        U[t + 1] = u_next\n    end\n    return X, U\nend\n\nX, U = get_data(100000; init=zeros)\nxt = X[:, 1:end - 1]\nxn = X[:, 2:end]\ny = g(X, U)\n\n# Store for the observer (inputs are inputs to observer)\ninput_data = [U; y][:, 1:end - 1]\nbatches = 200\ndata = Flux.Data.DataLoader((xn, xt, input_data), batchsize=batches, shuffle=true)","category":"page"},{"location":"examples/pde_obsv/#.-Define-a-contracting-REN","page":"PDE Observer Design with REN","title":"3. Define a contracting REN","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"Now we can define a contracting REN to parameterise the observer mentioned above. We'll use a contracting REN with q=500 neurons, and output mapping as C_2D_21D_22=I00 DiffREN constructs a differentialble REN from its direct parametrization, i.e. ContractingRENParams (see the Package Overview for more detail) and updates the parameter every time the model is called.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using RobustNeuralNetworks\n\n# Constuct a REN\nnv = 500\nnu = size(input_data, 1)\nny = nx\nmodel_params = ContractingRENParams{dtype}(\n    nu, nx, nv, ny; \n    nl = tanh, ϵ=0.01,\n    polar_param = false, \n    output_map = false # Define the output mapping\n)\nmodel = DiffREN(model_params)","category":"page"},{"location":"examples/pde_obsv/#.-Train-the-model","page":"PDE Observer Design with REN","title":"4. Train the model","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"Now we can train the observer to give the prediction of system states. First, we need to define the loss function mathcalL(tilde z) = frac1T sum^T-1_t=0a_RD(tildexi_ttildeb_t)-f_o(tildexi_ttildeb_ttildey_t)^2 where tildez = (tildexi_ttildey_ttildeb_t) is defined as the the training data generated from previous section. This cost funtion calculates the one step ahead prediction error.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using BSON\nusing Flux\nusing Formatting\n\n# Define a loss function\nfunction loss(model, xn, x, u)\n    xpred = model(x, u)[1]\n    return mean(norm(xpred[:, i] - xn[:, i]).^2 for i in 1:size(x, 2))\nend","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"We use SGD with the Adam optimiser to train te REN. We use Flux.withgradient to calucate the gradient and the value of the loss function, then use Flux.update! to update the trainable parameters of the REN. We start from a learning rate of 10^-3 and decrease it by powers of 10 once when the loss function does not decrease. The training loop will stop when it reaches the minimal learning rate 10^-7 or when we have reached 50 training epochs. Once the model has been trained, we can save it for later with the BSON package.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"# Train the model\nfunction train_observer!(model, data; Epochs=50, lr=1e-3, min_lr=1e-7)\n\n    # Set up the optimiser\n    opt_state = Flux.setup(Adam(lr), model)\n\n    mean_loss, loss_std = [1e5], []\n    for epoch in 1:Epochs\n        batch_loss = []\n        for (xni, xi, ui) in data\n\n            # Get gradient and store loss\n            train_loss, ∇J = Flux.withgradient(loss, model, xni, xi, ui)\n            Flux.update!(opt_state, model, ∇J[1])\n        \n            # Store losses for later\n            push!(batch_loss, train_loss)\n            printfmt(\"Epoch: {1:2d}\\tTraining loss: {2:1.4E} \\t lr={3:1.1E}\\n\", epoch, train_loss, lr)\n        end\n\n        # Print stats through epoch\n        println(\"------------------------------------------------------------------------\")\n        printfmt(\"Epoch: {1:2d} \\t mean loss: {2:1.4E}\\t std: {3:1.4E}\\n\", epoch, mean(batch_loss), std(batch_loss))\n        println(\"------------------------------------------------------------------------\")\n        push!(mean_loss, mean(batch_loss))\n        push!(loss_std, std(batch_loss))\n\n        # Check for decrease in loss\n        if mean_loss[end] >= mean_loss[end - 1]\n            println(\"Reducing Learning rate\")\n            lr *= 0.1\n            Flux.adjust!(opt_state, lr)\n            (lr <= min_lr) && (return mean_loss, loss_std)\n        end\n    end\n    return mean_loss, loss_std\nend\n\n# Train and save the model\ntloss, loss_std = train_observer!(model, data; Epochs=50, lr=1e-3, min_lr=1e-7)\nbson(\"../results/pde_obsv.bson\", \n    Dict(\n        \"model\" => model, \n        \"training_loss\" => tloss, \n        \"loss_std\" => loss_std\n    )\n)","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"Running the training loop can take an hour or two, so we've saved one in /examples/results/pde_obsv.bson. You can load it with the following code (you may need to change the file path depending on where you run this from).","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using BSON\nmodel = BSON.load(\"./examples/results/pde_obsv.bson\")[\"model\"]","category":"page"},{"location":"examples/pde_obsv/#.-Evaluate-the-model","page":"PDE Observer Design with REN","title":"5. Evaluate the model","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"Now we can evaluate the performance of the learned observer using REN. We'll first generate some test data by simulating the system for 2000 time steps, and calculate the prediction using the observer.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"# Test observer\nT = 2000\ninit = (args...) -> 0.5*ones(args...)\nx, u = get_data(T, init=init)\ny = [g(x[:, t:t], u[t]) for t in 1:T]\n\nbatches = 1\nobserver_inputs = [repeat([ui; yi], outer=(1, batches)) for (ui, yi) in zip(u, y)]\n\n# Simulate the model through time\nfunction simulate(model::AbstractREN, x0, u)\n    recurrent = Flux.Recur(model, x0)\n    output = recurrent.(u)\n    return output\nend\nx0 = init_states(model, batches)\nxhat = simulate(model, x0, observer_inputs)\nXhat = reduce(hcat, xhat)","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"Now we can plot the result of the ground truth and the prediction, as well as the error between the generated data and observer.","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"using CairoMakie\n\n# Make a plot to show PDE and errors\nfunction plot_heatmap(f1, xdata, i)\n\n    # Make and label the plot\n    xlabel = i < 3 ? \"\" : \"Time steps\"\n    ylabel = i == 1 ? \"True\" : (i == 2 ? \"Observer\" : \"Error\")\n    ax, _ = heatmap(f1[i,1], xdata', colormap=:thermal, axis=(xlabel=xlabel, ylabel=ylabel))\n\n    # Format the axes\n    ax.yticksvisible = false\n    ax.yticklabelsvisible = false\n    if i < 3\n        ax.xticksvisible = false\n        ax.xticklabelsvisible = false\n    end\n    xlims!(ax, 0, T)\nend\n\nf1 = Figure(resolution=(500,400))\nplot_heatmap(f1, x, 1)\nplot_heatmap(f1, Xhat[:, 1:batches:end], 2)\nplot_heatmap(f1, abs.(x - Xhat[:, 1:batches:end]), 3)\nColorbar(f1[:,2], colorrange=(0,1),colormap=:thermal)\n\ndisplay(f1)","category":"page"},{"location":"examples/pde_obsv/","page":"PDE Observer Design with REN","title":"PDE Observer Design with REN","text":"In the plot, the x-axis is the time dimension and the y-axis is the spatial dimension. (Image: )","category":"page"},{"location":"examples/rl/#Reinforcement-Learning-with-LBDN","page":"Reinforcement Learning","title":"Reinforcement Learning with LBDN","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Full example code can be found here.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"One of the original motivations for developing RobustNeuralNetworks.jl was to guarantee stability and robustness in learning-based control. Some of our recent research (eg: Wang et al. (2022) and Barbara, Wang & Manchester (2023)) has shown that, with the right controller architecture, we can learn over a space of stabilising controllers for linear/nonlinear systems using standard reinforcement learning techniques, so long as our control policy is parameterised by a REN (see also (Convex) Nonlinear Control with REN).","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"In this example, we'll demonstrate how to train an LBDN controller with Reinforcement Learning (RL) for a simple nonlinear dynamical system. This controller will not have any stability guarantees. The purpose of this example is simply to showcase the steps required to set up RL experiments for more complex systems with RENs and LBDNs.","category":"page"},{"location":"examples/rl/#.-Overview","page":"Reinforcement Learning","title":"1. Overview","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Let's consider the simple mechanical system shown below: a box of mass m sits in a tub of fluid, held between the walls of the tub by two springs, each with spring constant k2 We can push the box with force u Its dynamics are","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"mddotq = u - kq - mu dotq dotq","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"where mu is the viscous friction coefficient due to the box moving through the fluid.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"@html_str \"\"\"<p align=\"center\"> <object type=\"image/png\" data=$(joinpath(Main.buildpath, \"../assets/lbdn-rl/mass_rl.png\")) width=\"35%\"></object> </p>\"\"\" #hide","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"We can write this as a (nonlinear) state-space model with state x = (qdotq)^top control input u and dynamics","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"dotx = f(xu) = beginbmatrix\ndotq  (u - kq - mu dotq dotq)m\nendbmatrix","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"This is a continous-time model of the dynamics. For our purposes, we'll need a discrete-time model. We can discretise the dynamics using a forward Euler approximation such that","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"x_t+1 = x_t + Delta t cdot f(x_t u_t)","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"where Delta t is the time-step. This approximation typically requires a very small time-step for numerical stability, but will be fine for our simple example. A more robust method is to use a fourth (or higher) order Runge-Kutta scheme.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Our aim is to learn a controller u = mathcalK_theta(x q_mathrmref) defined by some learnable parameters theta that can push the box to any goal position q_mathrmref that we choose. Specifically, we want the box to:","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Reach a (stationary) goal position q_mathrmref\nWithin a time period T","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Note that the force required to keep the box an equilibrium position q_mathrmref is simply u_mathrmref = k q_mathrmref (set the derivative terms in the dynamics to zero and re-arrange for u). We can encode these objectives into a cost function J_theta and write our RL problem as","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"min_theta mathbbE left J_theta right\nquad textwhere quad\nJ_theta = sum_t=0^T-1 c_1 (q_t - q_mathrmref)^2 + c_2 dotq_t^2 + c_3 (u_t - u_mathrmref)^2","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"where c_1 c_2 c_3 are cost weightings, and the expectation is over different initial and goal positions of the box.","category":"page"},{"location":"examples/rl/#.-Problem-setup","page":"Reinforcement Learning","title":"2. Problem setup","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Let's define some parameters for our system and translate the dynamics into Julia code. We'll consider a box of mass m=1, a spring constant of k=5 and a viscous damping coefficient mu = 05. We'll simulate the system over T = 4s time horizons with a time-step of Delta t = 002s.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"m = 1                   # Mass (kg)\nk = 5                   # Spring constant (N/m)\nμ = 0.5                 # Viscous damping coefficient (kg/m)\n\nTmax = 4                # Simulation horizon (s)\ndt = 0.02               # Time step (s)\nts = 1:Int(Tmax/dt)     # Array of time indices","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Now we can generate some training data. Suppose the box always starts at rest from the zero position, and the goal position could be anywhere in the range q_mathrmref in -11. Our training data consists of a batch of 80 initial conditions (all zeros) and 80 randomly-sampled goal positions.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using Random\nrng = MersenneTwister(42)\n\nnx, nref, batches = 2, 1, 80\nx0 = zeros(nx, batches)\nqref = 2*rand(rng, nref, batches) .- 1\nuref = k*qref","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"It's good practice (and faster) to simulate all of these simulation batches at once, so we define our dynamics functions to operate on matrices of states and controls. Each row is a different state or control, and each column corresponds to a simulation for a particular goal position.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"_visc(v::Matrix) = μ * v .* abs.(v)\nf(x::Matrix,u::Matrix) = [x[2:2,:]; (u[1:1,:] - k*x[1:1,:] - _visc(x[2:2,:]))/m]\nfd(x::Matrix,u::Matrix) = x + dt*f(x,u)","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Reinforcement learning problems generally involve simulating the system over some time horizon and collecting a series of rewards or costs at each time step. Control policies are then trained using approximations of the cost gradient nabla J_theta because it is often difficult (or impossible) to compute the exact gradient. See ReinforcementLearning.jl for more RL in Julia.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"For this simple example, we can just write a differentiable simulator of the dynamics. The simulator takes a batch of initial states, goal positions, and a controller model whose inputs are x q_mathrmref. It computes a batch of trajectories of states and controls z = x_0u_0 ldots x_T-1u_T-1 for later use. To get around the well-known issue of array mutation with auto-differentiation, we use a Zygote.Buffer to iteratively store the outputs.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using Zygote: Buffer\n\nfunction rollout(model, x0, qref)\n    z = Buffer([zero([x0;qref])], length(ts))\n    x = x0\n    for t in ts\n        u = model([x;qref])\n        z[t] = vcat(x,u)\n        x = fd(x,u)\n    end\n    return copy(z)\nend","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Once we have our trajectories, we just need a function to evaluate the cost given some weightings c_1c_2c_3.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using Statistics\n\nweights = [10,1,0.1]\nfunction _cost(z, qref, uref)\n    Δz = z .- [qref; zero(qref); uref]\n    return mean(sum(weights .* Δz.^2; dims=1))\nend\ncost(z::AbstractVector, qref, uref) = mean(_cost.(z, (qref,), (uref,)))","category":"page"},{"location":"examples/rl/#.-Define-a-model","page":"Reinforcement Learning","title":"3. Define a model","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"For this example, we'll learn an LBDN controller with a Lipschitz bound of gamma = 20. Its inputs are the state x_t and goal position q_mathrmref, while its outputs are the control force u_t. We have chosen a model with two hidden layers each of 32 neurons just as an example. For details on how Lipschitz bounds can be useful in learning robust controllers, please see Barbara, Wang & Manchester (2023).","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using Flux\nusing RobustNeuralNetworks\n\nnu = nx + nref          # Inputs (states and reference)\nny = 1                  # Outputs (control action u)\nnh = fill(32, 2)        # Hidden layers\nγ = 20                  # Lipschitz bound\nmodel_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; nl=relu, rng)","category":"page"},{"location":"examples/rl/#.-Define-a-loss-function","page":"Reinforcement Learning","title":"4. Define a loss function","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"This is where things get interesting. The model_ps above contain all the information required to define a dense LBDN model. However, model_ps is not a model that can be evaluated on data: it is a model parameterisation, and contains the learnable parameters theta. If we want to evaluate an LBDN on data, we first need to construct the model using the LBDN wrapper. This converts the model parameterisation to an \"explicit\" form for evaluation, as described in detail in the Package Overview. Our loss function therefore looks like this.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"function loss(model_ps, x0, qref, uref)\n    model = LBDN(model_ps)\n    z = rollout(model, x0, qref)\n    return cost(z, qref, uref)\nend","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"info: Include model construction in the loss function\nWe need to include this mapping in the loss function, because it describes how the learnable parameters theta in model_ps affect the data. If we don't include it, then loss() does not depend on the model parameters, and their gradients will be zero!See Can't I just use DiffLBDN? below for more details.","category":"page"},{"location":"examples/rl/#.-Train-the-model","page":"Reinforcement Learning","title":"5. Train the model","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Now that we have set up the RL problem, all that remains is to train the controller model. We've written a function that trains the controller and keeps track of the loss (cost J_theta) for each simulation in our batch of 80. Note that we pass in the model_ps, which are converted to a model inside the loss() function during training.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using Printf\n\nfunction train_box_ctrl!(model_ps, loss_func; lr=1e-3, epochs=250, verbose=false)\n    costs = Vector{Float64}()\n    opt_state = Flux.setup(Adam(lr), model_ps)\n    for k in 1:epochs\n\n        train_loss, ∇J = Flux.withgradient(loss_func, model_ps, x0, qref, uref)\n        Flux.update!(opt_state, model_ps, ∇J[1])\n\n        push!(costs, train_loss)\n        verbose && @printf \"Iter %d loss: %.2f\\n\" k train_loss\n    end\n    return costs\nend\n\ncosts = train_box_ctrl!(model_ps, loss; verbose=true)","category":"page"},{"location":"examples/rl/#.-Evaluate-the-trained-model","page":"Reinforcement Learning","title":"6. Evaluate the trained model","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Once we've trained the model to move the box, we should check that it actually works. In the code below, we generate 100 batches of test data. In each one, the box starts at the origin at rest, and is moved through the fluid to a different (random) goal position q_mathrmref in -11 We plot the states and controls alongside the loss curve from training.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"using CairoMakie\n\nlbdn = LBDN(model_ps)\nx0_test = zeros(2,100)\nqr_test = 2*rand(rng, 1, 100) .- 1\nz_lbdn = rollout(lbdn, x0_test, qr_test)\n\n# Plot position, velocity, and control input over time\nfunction plot_box_learning(costs, z, qr)\n\n    _get_vec(x, i) = reduce(vcat, [xt[i:i,:] for xt in x])\n    q = _get_vec(z, 1)\n    v = _get_vec(z, 2)\n    u = _get_vec(z, 3)\n    t = dt*ts\n    \n    Δq = q .- qr .* ones(length(z), length(qr_test))\n    Δu = u .- k*qr .* ones(length(z), length(qr_test))\n\n    f1 = Figure(resolution = (600, 400))\n    ga = f1[1,1] = GridLayout()\n\n    ax0 = Axis(ga[1,1], xlabel=\"Training epochs\", ylabel=\"Cost\")\n    ax1 = Axis(ga[1,2], xlabel=\"Time (s))\", ylabel=\"Position error (m)\", )\n    ax2 = Axis(ga[2,1], xlabel=\"Time (s))\", ylabel=\"Velocity (m/s)\")\n    ax3 = Axis(ga[2,2], xlabel=\"Time (s)\", ylabel=\"Control error (N)\")\n\n    lines!(ax0, costs, color=:black)\n    for k in axes(q,2)\n        lines!(ax1, t, Δq[:,k], linewidth=0.5,  color=:grey)\n        lines!(ax2, t,  v[:,k], linewidth=0.5,  color=:grey)\n        lines!(ax3, t, Δu[:,k], linewidth=0.5,  color=:grey)\n    end\n\n    lines!(ax1, t, zeros(size(t)), color=:red, linestyle=:dash)\n    lines!(ax2, t, zeros(size(t)), color=:red, linestyle=:dash)\n    lines!(ax3, t, zeros(size(t)), color=:red, linestyle=:dash)\n    \n    xlims!.((ax1,ax2,ax3), (t[1],), (t[end],))\n    display(f1)\n    return f1\nend\n\nfig = plot_box_learning(costs, z_lbdn, qr_test)","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"(Image: )","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"The box clearly moves to the required position within the time frame and stays there in all cases, showing that the control is up to the task.","category":"page"},{"location":"examples/rl/#Can't-I-just-use-DiffLBDN?","page":"Reinforcement Learning","title":"Can't I just use DiffLBDN?","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Readers who have worked through the Fitting a Curve with LBDN and Image Classification with LBDN examples will know that we have included the DiffLBDN and DiffREN wrappers to make training models more like Flux.jl. These wrappers convert a model parameterisation to an explicit model each time they are called. This means the user does not have to re-construct the model in the loss function, and can simply use the following.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"loss2(model, x0, qref, uref) = cost(rollout(model, x0, qref), qref, uref)","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"The catch is computation speed, particular in an RL context. Careful inspection of the rollout() function shows that the model is evaluated many times within the loss function before the learnable parameters are updated with Flux.update!(). As discussed in the Package Overview, the major computational bottleneck for RENs and LBDNs is the conversion from learnable (direct) parameters to an explicit model. Constructing the model only when the parameters are updated therefore saves considerably on computation time, particularly for large models.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"For example, let's train single-hidden-layer LBDNs with n neurons over 100 training epochs on this RL problem,  and log the time taken to train a model when using LBDN and DiffLBDN.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"function lbdn_compute_times(n; epochs=100)\n\n    lbdn_ps = DenseLBDNParams{Float64}(nu, [n], ny, γ; nl=relu, rng)\n    diff_lbdn = DiffLBDN(deepcopy(lbdn_ps))\n\n    t_lbdn = @elapsed train_box_ctrl!(lbdn_ps, loss; epochs)            # Build LBDN in loss\n    t_diff_lbdn = @elapsed train_box_ctrl!(diff_lbdn, loss2; epochs)    # Use DiffLBDN\n    return [t_lbdn, t_diff_lbdn]\n\nend","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Let's run this function over hidden layers of size n = 2 2^2 ldots 2^9 and plot the results.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"# Evaluate computation time with different hidden-layer sizes\n# Run it once first for just-in-time compiler\nsizes = 2 .^ (1:9)\nlbdn_compute_times(2; epochs=1)\ncomp_times = reduce(hcat, lbdn_compute_times.(sizes))\n\n# Plot the results\nf1 = Figure(resolution = (600, 400))\nax = Axis(\n    f1[1,1], \n    xlabel=\"Hidden layer size\", \n    ylabel=\"Training time (s) (100 epochs)\", \n    xscale=Makie.log2, yscale=Makie.log10\n)\nlines!(ax, sizes, comp_times[1,:], label=\"LBDN\")\nlines!(ax, sizes, comp_times[2,:], label=\"DiffLBDN\")\n\nxlims!(ax, [sizes[1], sizes[end]])\naxislegend(ax, position=:lt)\ndisplay(f1)","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"(Image: )","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"Even for a single-layer LBDN with 2^9 = 512 neurons, using DiffLBDN takes an order of magnitude longer to train than only constructing the LBDN model each time the loss() function is called. We offer the following advice.","category":"page"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"info: When to use LBDN vs. DiffLBDN\nIf many evaluations of the model are required before Flux.update!() (or equivalent) is called, use LBDN to define the model once, evaluate it many times, then construct it again once the learnable parameters change.If Flux.update!() is called after each model evaluation (eg: Image Classification with LBDN), it is often more convenient and equally fast to use DiffLBDN.The same applies for REN and DiffREN.","category":"page"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"Pages = [\"models.md\"]","category":"page"},{"location":"lib/models/#Model-Wrappers","page":"Model Wrappers","title":"Model Wrappers","text":"","category":"section"},{"location":"lib/models/#Lipschitz-Bounded-Deep-Networks","page":"Model Wrappers","title":"Lipschitz-Bounded Deep Networks","text":"","category":"section"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"AbstractLBDN\nDiffLBDN\nLBDN\nSandwichFC","category":"page"},{"location":"lib/models/#RobustNeuralNetworks.AbstractLBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.AbstractLBDN","text":"abstract type AbstractLBDN{T, L} end\n\nExplicit parameterisation for Lipschitz-bounded deep networks.\n\n(m::AbstractLBDN)(u::AbstractVecOrMat)\n\nCall and AbstractLBDN model given inputs u.\n\nIf arguments are matrices, each column must be a vector of inputs (allows batch simulations).\n\nExamples\n\nThis example creates a dense LBDN using DenseLBDNParams and calls the model with some randomly generated inputs.\n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = Xoshiro(42)\nbatches = 10\nγ = 20.0\n\n# Model with 4 inputs, 1 ouput, 4 hidden layers\nnu, ny = 4, 1\nnh = [5, 10, 5, 15]\n\nlbdn_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; rng)\nlbdn = LBDN(lbdn_ps)\n\n# Evaluate model with a batch of random inputs\nu = 10*randn(rng, nu, batches)\ny = lbdn(u)\n\nprintln(round.(y; digits=2))\n\n# output\n\n[-1.11 -1.01 -0.07 -2.25 -4.22 -1.76 -3.82 -1.13 -11.85 -3.01]\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.DiffLBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.DiffLBDN","text":"DiffLBDN(ps::AbstractLBDNParams)\n\nConstruct a differentiable LBDN from its direct parameterisation.\n\nDiffLBDN is an alternative to LBDN that computes the explicit parameterisation ExplicitLBDNParams each time the model is called, rather than storing it in the LBDN object.\n\nThis model wrapper is easiest to use if you plan to update the model parameters after every call of the model. It can be trained just like any other Flux.jl model and does not need to be re-created if the trainable parameters are updated (unlike LBDN).\n\nHowever, it is slow and computationally inefficient if the model is called many times before updating the parameters (eg: in reinforcement learning). \n\nExamples\n\nThe syntax to construct a DiffLBDN is identical to that of an LBDN. \n\nusing RobustNeuralNetworks\n\nnu, nh, ny, γ = 1, [10, 20], 1\nlbdn_params = DenseLBDNParams{Float64}(nu, nh, ny, γ)\nmodel = DiffLBDN(lbdn_params)\n\nSee also AbstractLBDN, LBDN, SandwichFC.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.LBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.LBDN","text":"LBDN(ps::AbstractLBDNParams)\n\nConstruct an LBDN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of LBDN (eg: a DenseLBDNParams instance) and converts it to a callable explicit parameterisation of the LBDN. An example can be found in the docs for AbstractLBDN.\n\nSee also AbstractLBDN, DiffLBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.SandwichFC","page":"Model Wrappers","title":"RobustNeuralNetworks.SandwichFC","text":"SandwichFC((in, out), σ::F; <keyword arguments>) where F\n\nConstruct a non-expansive \"sandwich layer\" for a dense (fully-connected) LBDN.\n\nA non-expensive layer is a layer with a Lipschitz bound of exactly 1. This layer is provided as a Lipschitz-bounded alternative to Flux.Dense. Its interface and usage was intentionally designed to be very similar to make it easy to use for anyone familiar with Flux.\n\nArguments\n\n(in, out)::Pair{<:Integer, <:Integer}: Input and output sizes of the layer.\nσ::F=identity: Activation function.\n\nKeyword arguments\n\ninit::Function=Flux.glorot_normal: Initialisation function for all weights and bias vectors.\nbias::Bool=true: Include bias vector or not.\noutput_layer::Bool=false: Just the output layer of a dense LBDN or regular sandwich layer.\nT::DataType=Float32: Data type for weight matrices and bias vectors.\nrng::AbstractRNG = Random.GLOBAL_RNG: rng for model initialisation.\n\nExamples\n\nWe can build a dense LBDN directly using SandwichFC layers. The model structure is described in Equation 8 of Wang & Manchester (2023).\n\nusing Flux\nusing Random\nusing RobustNeuralNetworks\n\n# Random seed for consistency\nrng = Xoshiro(42)\n\n# Model specification\nnu = 1                  # Number of inputs\nny = 1                  # Number of outputs\nnh = fill(16,2)         # 2 hidden layers, each with 16 neurons\nγ = 5                   # Lipschitz bound of 5.0\n\n# Set up dense LBDN model\nmodel = Flux.Chain(\n    (x) -> (√γ * x),\n    SandwichFC(nu => nh[1], relu; T=Float64, rng),\n    SandwichFC(nh[1] => nh[2], relu; T=Float64, rng),\n    (x) -> (√γ * x),\n    SandwichFC(nh[2] => ny; output_layer=true, T=Float64, rng),\n)\n\n# Evaluate on dummy inputs\nu = 10*randn(rng, nu, 10)\ny = model(u)\n\nprintln(round.(y;digits=2))\n\n# output\n\n[4.13 4.37 3.22 8.38 4.15 3.71 0.7 2.04 1.78 2.64]\n\nSee also DenseLBDNParams, DiffLBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#Recurrent-Equilibrium-Networks","page":"Model Wrappers","title":"Recurrent Equilibrium Networks","text":"","category":"section"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"AbstractREN\nDiffREN\nREN\nWrapREN","category":"page"},{"location":"lib/models/#RobustNeuralNetworks.AbstractREN","page":"Model Wrappers","title":"RobustNeuralNetworks.AbstractREN","text":"abstract type AbstractREN end\n\nExplicit parameterisation for recurrent equilibrium networks.\n\n(m::AbstractREN)(xt::AbstractVecOrMat, ut::AbstractVecOrMat)\n\nCall an  AbstractREN model given internal states xt and inputs ut. \n\nIf arguments are matrices, each column must be a vector of states or inputs (allows batch simulations).\n\nExamples\n\nThis example creates a contracting REN using ContractingRENParams and calls the model with some randomly generated inputs. \n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = Xoshiro(42)\nbatches = 10\nnu, nx, nv, ny = 4, 2, 20, 1\n\n# Construct a REN\ncontracting_ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng)\nren = REN(contracting_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\nprintln(round.(y1;digits=2))\n\n# output\n\n[-1.49 0.75 1.34 -0.23 -0.84 0.38 0.79 -0.1 0.72 0.54]\n\nSee also REN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.DiffREN","page":"Model Wrappers","title":"RobustNeuralNetworks.DiffREN","text":"DiffREN(ps::AbstractRENParams{T}) where T\n\nConstruct a differentiable REN from its direct parameterisation.\n\nDiffREN is an alternative to REN and WrapREN that computes the explicit parameterisation ExplicitRENParams every time the model is called, rather than storing it in the REN object.\n\nThis model wrapper is easiest to use if you plan to update the model parameters after every call of the model. It an be trained just like any other Flux.jl model (unlike WrapREN) and does not need to be re-created if the trainable parameters are updated (unlike REN).\n\nHowever, it is slow and computationally inefficient if the model is called many times before updating the parameters (eg: in reinforcement learning). \n\nExamples\n\nThe syntax to construct a DiffREN is identical to that of a REN. \n\nusing RobustNeuralNetworks\n\nnu, nx, nv, ny = 1, 10, 20, 1\nren_params = ContractingRENParams{Float64}(nu, nx, nv, ny)\nmodel = DiffREN(ren_params)\n\nSee also AbstractREN, REN, and WrapREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.REN","page":"Model Wrappers","title":"RobustNeuralNetworks.REN","text":"REN(ps::AbstractRENParams{T}) where T\n\nConstruct a REN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of REN (eg: a GeneralRENParams instance) and converts it to a callable explicit parameterisation of the REN. An example can be found in the docs for AbstractREN.\n\nSee also AbstractREN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.WrapREN","page":"Model Wrappers","title":"RobustNeuralNetworks.WrapREN","text":"WrapREN(ps::AbstractRENParams{T}) where T\n\n[NOTE:] THIS IS A LEGACY WRAPPER AND WILL BE REMOVED IN A FUTURE RELEASE.\n\nConstruct REN wrapper from its direct parameterisation.\n\nWrapREN is an alternative to REN that stores the AbstractRENParams and ExplicitRENParams within the same object. This means that a new REN object does not have to be created each time the parameters are updated. Explicit REN parameters must be updated by the user if the direct parameters have changed.\n\nNote that WrapREN cannot be used with Flux.jl, since it relies on mutating the WrapREN instance.\n\nExamples\n\nIn this example, we create a REN satisfying some generic behavioural constraints and demonstrate how to update the REN wrapper if model parameters are changed.\n\nusing LinearAlgebra\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = Xoshiro(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\n\nQ = Matrix{Float64}(-I(ny))\nR = 0.1^2 * Matrix{Float64}(I(nu))\nS = zeros(Float64, nu, ny)\n\n# Construct a REN\nren_ps = GeneralRENParams{Float64}(nu, nx, nv, ny, Q, S, R; rng)\nren = WrapREN(ren_ps)\n\n# Some dummy inputs\nx0 = init_states(ren, batches; rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0) \n\n# Update the model after changing a parameter\nren.params.direct.B2 .*= rand(rng, size(ren.params.direct.B2)...)\nupdate_explicit!(ren)\n\nprintln(round(ren.explicit.B2[10];digits=4))\n\n# output\n\n-0.0034\n\nSee also AbstractREN, REN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"examples/lbdn_curvefit/#Fitting-a-Curve-with-LBDN","page":"Fitting a Curve","title":"Fitting a Curve with LBDN","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Full example code can be found here.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"For our first example, let's fit a Lipschitz-bounded Deep Network (LBDN) to a curve in one dimension. Consider the step function function below.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"f(x) = \nbegincases\n1  textif  x  0  0   textif  x  0\nendcases","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Our aim is to demonstrate how to train a model in RobustNeuralNetworks.jl, and how to ensure the model naturally satisfies some user-defined robustness certificate (the Lipschitz bound). We'll follow the steps below to fit an LBDN model to our function f(x):","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Generate training data\nDefine a model with a Lipshitz bound (maximum slope) of 10.0\nDefine a loss function\nTrain the model to minimise the loss function\nExamine the trained model","category":"page"},{"location":"examples/lbdn_curvefit/#.-Generate-training-data","page":"Fitting a Curve","title":"1. Generate training data","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Let's generate training data for f(x) on the interval -03 03 as an example. We zip() the data up into a sequence of tuples (x,y) to make training with Flux.jl easier in Step 4.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"# Function to estimate\nf(x) = x < 0 ? 0 : 1\n\n# Training data\ndx = 0.01\nxs = -0.3:dx:0.3\nys = f.(xs)\ndata = zip(xs,ys)","category":"page"},{"location":"examples/lbdn_curvefit/#.-Define-a-model","page":"Fitting a Curve","title":"2. Define a model","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Since we are only dealing with a simple one-dimensional curve, we can afford to use a small model. Let's choose an LBDN with four hidden layers, each with 16 neurons, and a Lipschitz bound of γ = 10.0. This means that the maximum slope the model can achieve between two points should be exactly 10.0 by construction.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"using Random\nusing RobustNeuralNetworks\n\n# Random seed for consistency\nrng = Xoshiro(0)\n\n# Model specification\nnu = 1                  # Number of inputs\nny = 1                  # Number of outputs\nnh = fill(16,4)         # 4 hidden layers, each with 16 neurons\nγ = 10                  # Lipschitz bound of 10\n\n# Set up model: define parameters, then create model\nmodel_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; rng)\nmodel = DiffLBDN(model_ps)","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Note that we first constructed the model parameters model_ps, and then created a callable model. In RobustNeuralNetworks.jl, model parameterisations are separated from \"explicit\" definitions of a model used for evaluation on data. See the Direct & explicit parameterisations for more information.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"info: A layer-wise approach\nWe have also provided single LBDN layers with SandwichFC. Introduced in Wang & Manchester (2023), the SandwichFC layer is a fully-connected or dense layer with a guaranteed Lipschitz bound of 1.0. We have designed the user interface for SandwichFC to be as similar to that of Flux.Dense as possible. This may be more convenient for users used to working with Flux.jl.For example, we can construct an identical model to the LBDN model above with the following.using Flux\n\nchain_model = Flux.Chain(\n    (x) -> (√γ * x),\n    SandwichFC(nu => nh[1], Flux.relu; T=Float64, rng),\n    SandwichFC(nh[1] => nh[2], Flux.relu; T=Float64, rng),\n    SandwichFC(nh[2] => nh[3], Flux.relu; T=Float64, rng),\n    SandwichFC(nh[3] => nh[4], Flux.relu; T=Float64, rng),\n    (x) -> (√γ * x),\n    SandwichFC(nh[4] => ny; output_layer=true, T=Float64, rng),\n)See Section 3.1 of Wang & Manchester (2023) for further details.","category":"page"},{"location":"examples/lbdn_curvefit/#.-Define-a-loss-function","page":"Fitting a Curve","title":"3. Define a loss function","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Let's stick to a simple loss function based on the mean-squared error (MSE) for this example. All AbstractLBDN models take an AbstractArray as their input, which is why x and y are wrapped in vectors.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"# Loss function\nloss(model,x,y) = Flux.mse(model([x]),[y]) ","category":"page"},{"location":"examples/lbdn_curvefit/#.-Train-the-model","page":"Fitting a Curve","title":"4. Train the model","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Our objective is to minimise the loss function with a model that has a Lipschitz bound no greater than 10.0. Let's set up a callback function to check the fit error and slope of our model at each training epoch.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"using Flux\n\n# Check fit error/slope during training\nmse(model, xs, ys) = sum(loss.((model,), xs, ys)) / length(xs)\nlip(model, xs, dx) = maximum(abs.(diff(model(xs'), dims=2)))/dx\n\n# Callback function to show results while training\nfunction progress(model, iter, xs, ys, dx) \n    fit_error = round(mse(model, xs, ys), digits=4)\n    slope = round(lip(model, xs, dx), digits=4)\n    @show iter fit_error slope\n    println()\nend","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"We'll train the model for 300 training epochs a learning rate of lr = 2e-4. We'll also use the Adam optimiser from Flux.jl and the default Flux.train! method.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"# Define hyperparameters and optimiser\nnum_epochs = 300\nlr = 2e-4\nopt_state = Flux.setup(Adam(lr), model)\n\n# Train the model\nfor i in 1:num_epochs\n    Flux.train!(loss, model, data, opt_state)\n    (i % 100 == 0) && progress(model, i, xs, ys, dx)\nend","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Note that this training loop is for demonstration only. For a better fit, or on more complex problems, we strongly recommend:","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"Increasing the number of training epochs\nDefining your own training loop \nUsing ParameterSchedulers.jl to vary the learning rate.","category":"page"},{"location":"examples/lbdn_curvefit/#.-Examine-the-trained-model","page":"Fitting a Curve","title":"5. Examine the trained model","text":"","category":"section"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"The final estimated lower bound of our Lipschitz constantt is very close to the maximum allowable value of 10.0.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"using Printf\n\n# Estimate Lipschitz lower-bound\nEmpirical_Lipschitz = lip(model, xs, dx)\n@printf \"Imposed Lipschitz upper bound:   %.2f\\n\" get_lipschitz(model)\n@printf \"Empirical Lipschitz lower bound: %.2f\\n\" Empirical_Lipschitz","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"We can now plot the results to see what our model looks like.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"using CairoMakie\n\n# Create a figure\nf1 = Figure(resolution = (600, 400))\nax = Axis(f1[1,1], xlabel=\"x\", ylabel=\"y\")\n\n# Compute the best-possible fit with Lipschitz bound 10.0\nget_best(x) = x<-0.05 ? 0 : (x<0.05 ? 10x + 0.5 : 1)\nybest = get_best.(xs)\nŷ = map(x -> model([x])[1], xs)\n\n# Plot\nlines!(xs, ys, label = \"Data\")\nlines!(xs, ybest, label = \"Max. slope = 10.0\")\nlines!(xs, ŷ, label = \"LBDN slope = $(round(Empirical_Lipschitz; digits=2))\")\naxislegend(ax, position=:lt)\nsave(\"lbdn_curve_fit.svg\", f1)","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"(Image: )","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"The model roughly approximates the step function f(x), but maintains a maximum Lipschitz constant (slope on the graph) below 10.0. It is reasonably close to the best-possible value, and can easily be improved with a slightly larger model and more training time.","category":"page"},{"location":"examples/lbdn_curvefit/","page":"Fitting a Curve","title":"Fitting a Curve","text":"The benefit of using an LBDN is that we have full control over the Lipschitz bound, and can still use standard unconstrained gradient descent tools lile Flux.train! to train our models. For examples in which setting the Lipschitz bound improves model performance and robustness, see Image Classification with LBDN and Reinforcement Learning with LBDN.","category":"page"},{"location":"introduction/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"introduction/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"RobustNeuralNetworks.jl is written in Julia and can be installed with the package manager. To add the package, type the following into the REPL.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"] add RobustNeuralNetworks","category":"page"},{"location":"introduction/getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"You should now be able to construct robust neural network models. The following example constructs a Lipschitz-bounded REN and evalutates it given a batch of random initial states and inputs.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = Xoshiro(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 1\nγ = 1\n\n# Construct a REN\nlipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng)\nren = REN(lipschitz_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\n# Print results for testing\nprintln(round.(y1; digits=2))\n\n# output\n\n[1.06 1.13 0.95 0.93 1.03 0.78 0.75 1.42 0.89 1.44]","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"For detailed examples of training models from RobustNeuralNetworks.jl, we recommend starting with Fitting a Curve with LBDN and working through the subsequent examples.","category":"page"},{"location":"introduction/getting_started/#Walkthrough","page":"Getting Started","title":"Walkthrough","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Let's step through the example above. It constructs and evaluates a Lipschitz-bounded REN. We start by importing packages and setting a random seed.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nusing RobustNeuralNetworks","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Let's set a random seed and define our batch size and some hyperparameters. For this example, we'll build a Lipschitz-bounded REN with 4 inputs, 1 output, 10 states, 20 neurons, and a Lipschitz bound of γ = 1.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"rng = Xoshiro(42)\nbatches = 10\n\nγ = 1\nnu, nx, nv, ny = 4, 10, 20, 1","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we can construct the REN parameters. The variable lipschitz_ren_ps contains all the parameters required to build a Lipschitz-bounded REN. Note that we separate the model parameterisation and its \"explicit\" (callable) form in RobustNeuralNetworks.jl. See the Package Overview for more details.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"lipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng)","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Once the parameters are defined, we can create a REN object in its explicit form.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"ren = REN(lipschitz_ren_ps)","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we can evaluate the REN. We can use the init_states function to create a batch of initial states, all zeros, of the correct dimensions.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"# Some random inputs\nx0 = init_states(ren, batches)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"Having evaluated the REN, we can check that the outputs are the same as in the original example.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"# Print results for testing\nprintln(round.(y1; digits=2))","category":"page"},{"location":"introduction/package_overview/#Package-Overview","page":"Package Overview","title":"Package Overview","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"RobustNeuralNetwork.jl contains two classes of neural network models: Recurrent Equilibrium Networks (RENs) and Lipschitz-Bounded Deep Networks (LBDNs). This page gives a brief overview of the two model architectures and how they are parameterised to automatically satisfy robustness certificates. We also provide some background on the different types of robustness metrics used to construct the models.","category":"page"},{"location":"introduction/package_overview/#What-are-RENs-and-LBDNs?","page":"Package Overview","title":"What are RENs and LBDNs?","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"A Recurrent Equilibrium Network (REN) is a linear system in feedback with a nonlinear activation function. Denote x_t in mathbbR^n_x as the internal states of the system, u_t inmathbbR^n_u as its inputs, and y_t in mathbbR^n_u as its outputs. Mathematically, a REN can be represented as","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"beginaligned\nbeginbmatrix\nx_t+1  v_t  y_t\nendbmatrix=\noversetWoverbrace\n\t\tleft\n\t\tbeginarrayccc\n\t\tA  B_1  B_2  hline \n\t\tC_1  D_11  D_12 \n\t\tC_2  D_21  D_22\n\t\tendarray \n\t\tright\n\nbeginbmatrix\nx_t  w_t  u_t\nendbmatrix+\noversetboverbrace\n\t\tbeginbmatrix\n\t\tb_x  b_v  b_y\n\t\tendbmatrix\n \nw_t=sigma(v_t)=beginbmatrix\nsigma(v_t^1)  sigma(v_t^2)  cdots  sigma(v_t^q)\nendbmatrix^top \nendaligned","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"where v_t w_t in mathbbR^n_v are the inputs and outputs of neurons and sigma is the activation function. Graphically, this is equivalent to the following, where the linear (actually affine) system G represents the first equation above.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"@html_str \"\"\"<p align=\"center\"> <object type=\"image/png\" data=$(joinpath(Main.buildpath, \"../assets/ren.png\")) width=\"35%\"></object> </p>\"\"\" #hide","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"A Lipschitz-Bounded Deep Network (LBDN) is a (memoryless) deep neural network model with a built-in upper-bound on its Lipschitz constant. Although it is a specialisation of a REN with a state dimension of n_x = 0, we use this simplification to construct LBDN models completely differently to RENs. We construct LBDNs as L-layer feed-forward networks, much like MLPs or CNNs, described by the following recursive equations.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"beginaligned\nz_0 = x \nz_k+1 = sigma(W_k z_k + b_k) quad k = 0 ldots L-1 \ny = W_L z_L + b_L\nendaligned","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"See Revay, Wang & Manchester (2021) and Wang & Manchester (2023) for more details on RENs and LBDNs, respectively.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"info: Acyclic REN models\nRevay, Wang & Manchester (2021) make special mention of \"acyclic\" RENs, which have a lower-triangular D_11. These are significantly more efficient to evaluate and train than a REN with dense D_11 and they perform similarly. All RENs in RobustNeuralNetworks.jl are therefore acyclic RENs.","category":"page"},{"location":"introduction/package_overview/#Direct-and-explicit-parameterisations","page":"Package Overview","title":"Direct & explicit parameterisations","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"The key advantage of the models in RobustNeuralNetworks.jl is that they naturally satisfy a set of user-defined robustness constraints (outlined in Robustness metrics and IQCs). I.e., robustness is guaranteed by construction. There is no need to impose additional (possibly computationally-expensive) constraints while training a REN or an LBDN. One can simply use unconstrained optimisation methods like gradient descent and be sure that the final model will satisfy the robustness requirements.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"We achieve this by constructing the weight matrices and bias vectors in our models to automatically satisfy some specific linear matrix inequalities (see Revay, Wang & Manchester (2021) for details). The learnable parameters of a model are a set of free variables theta in mathbbR^N which are completely unconstrained. When the set of learnable parameters is exactly mathbbR^N like this, we call it a direct parameterisation. The equations above describe the explicit parameterisation of RENs and LBDNs: a callable model that we can evaluate on data. For a REN, the explicit parameters are bartheta = W b, and for an LBDN they are bartheta = W_0 b_0 ldots W_L b_L.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"RENs are defined by two abstract types in RobustNeuralNetworks.jl. Subtypes of AbstractRENParams hold all the information required to directly parameterise a REN satisfying some robustness properties. For example, to initialise the direct parameters of a contracting REN with 1 input, 10 states, 20 neurons, 1 output, and a relu activation function, we use the following. The direct parameters theta are stored in model_ps.direct. ","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"using RobustNeuralNetworks\n\nnu, nx, nv, ny = 1, 10, 20, 1\nmodel_params = ContractingRENParams{Float64}(nu, nx, nv, ny)\n\ntypeof(model_params) <: AbstractRENParams","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"Subtypes of AbstractREN represent RENs in their explicit form so that they can be called and evaluated. The conversion from the direct to explicit parameters theta mapsto bartheta is performed when the REN is constructed.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"model = REN(model_params)\n\nprintln(typeof(model) <: AbstractREN)\nprintln(typeof(model_params.direct)) \t\t# Access direct params","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"The same is true for AbstractLBDNParams and AbstractLBDN regarding LBDN models.","category":"page"},{"location":"introduction/package_overview/#Types-of-direct-parameterisations","page":"Package Overview","title":"Types of direct parameterisations","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"There are currently four REN parameterisations implemented in this package:","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"ContractingRENParams parameterises RENs with a user-defined upper bound on the contraction rate.\nLipschitzRENParams parameterises RENs with a user-defined (or learnable) Lipschitz bound of gamma in (0infty).\nPassiveRENParams parameterises input/output passive RENs with user-tunable passivity parameter nu ge 0.\nGeneralRENParams parameterises RENs satisfying some general behavioural constraints defined by an Integral Quadratic Constraint (IQC) with parameters (Q,S,R).","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"Similarly, subtypes of AbstractLBDNParams define the direct parameterisation of LBDNs. There is currently only one version implemented in RobustNeuralNetworks.jl:","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"DenseLBDNParams parameterise dense (fully-connected) LBDNs. A dense LBDN is effectively a Lipschitz-bounded Flux.Dense network.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"See Robustness metrics and IQCs for an explanation of these robustness metrics. We intend on adding ConvolutionalLBDNParams to parameterise convolutional LBDNs in future iterations of the package (see Wang & Manchester (2023)).","category":"page"},{"location":"introduction/package_overview/#Explicit-model-wrappers","page":"Package Overview","title":"Explicit model wrappers","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"When training a REN or LBDN, we learn and update the direct parameters theta and convert them to the explicit parameters bartheta only for model evaluation. The main constructors for explicit models are REN and LBDN.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"Users familiar with Flux.jl will be used to creating a model once and then training it on their data. The typical workflow is as follows.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"using Flux\nusing Random\n\n# Define a model and a loss function\nmodel = Chain(Flux.Dense(1 => 10, Flux.relu), Flux.Dense(10 => 1, Flux.relu))\nloss(model, x, y) = Flux.mse(model(x), y)\n\n# Set up some dummy training data\nbatches = 20\nxs, ys = rand(Float32,1,batches), rand(Float32,1,batches)\ndata = [(xs, ys)]\n\n# Train the model for 50 epochs\nopt_state = Flux.setup(Adam(0.01), model)\nfor _ in 1:50\n    Flux.train!(loss, model, data, opt_state)\nend","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"When training a model constructed from REN or LBDN, we need to back-propagate through the mapping from direct (learnable) parameters to the explicit model. We must therefore include the model construction as part of the loss function. If we do not, then the auto-differentiation engine has no knowledge of how the model parameters affect the loss, and will return zero gradients. Here is an example with an LBDN, where the model is defined by the direct parameterisation stored in model_params.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"using Flux\nusing Random\nusing RobustNeuralNetworks\n\n# Define a model parameterisation and a loss function\nmodel_params = DenseLBDNParams{Float64}(1, [10], 1)\nfunction loss(model_params, x, y) \n    model = LBDN(model_params)\n    Flux.mse(model(x), y)\nend\n\n# Set up some dummy training data\nbatches = 20\nxs, ys = rand(1,batches), rand(1,batches)\ndata = [(xs, ys)]\n\n# Train the model for 50 epochs\nopt_state = Flux.setup(Adam(0.01), model_params)\nfor _ in 1:50\n    Flux.train!(loss, model_params, data, opt_state)\nend","category":"page"},{"location":"introduction/package_overview/#Separating-parameters-and-models-is-efficient","page":"Package Overview","title":"Separating parameters and models is efficient","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"For the sake of convenience, we have included the model wrappers DiffREN, DiffLBDN, and SandwichFC as alternatives to REN, and LBDN, respectively. These wrappers compute the explicit parameters each time the model is called rather than just once when they are constructed. Any model created with these wrappers can therefore be used exactly the same way as a regular Flux.jl model, and there is no need for model construction in the loss function. One can simply replace the definition of the Flux.Chain model in the demo cell above with","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"model_params = DenseLBDNParams{Float64}(1, [10], 1; nl=relu)\nmodel = DiffLBDN(model_params)","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"and train the LBDN just like any other Flux.jl model. We use these wrappers in many of the examples (eg: Image Classification with LBDN).","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"The reason we nominally keep the model_params and model separate with REN and LBDN is to offer flexibility. The computational bottleneck in training a REN or LBDN is converting from the direct to explicit parameters (mapping theta mapsto bartheta). Direct parameters are stored in model_params, while explicit parameters are computed when the model is created and are stored within it. We can see this from our earlier example with the contracting REN:","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"println(typeof(model_params.direct))\nprintln(typeof(model.explicit))","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"In some applications (eg: reinforcement learning), a model is called many times with the same explicit parameters bartheta before its learnable parameters theta are updated. It's therefore significantly efficient to store the explicit parameters, use them many times, and then update them only when the learnable parameters change. We can't store the direct and explicit parameters in the same model object since auto-differentiation in Flux.jl does not permit array mutation. Instead, we separate the two.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"info: Which wrapper should I use?\nThe model wrappers DiffREN, DiffLBDN, and SandwichFC  re-compute the explicit parameters every time the model is called. In applications where the learnable parameters are updated after one model call (eg: image classification), it is often more convenient and equally fast to use these wrappers.In applications where the model is called many times before updating it (eg: reinforcement learning), use REN or LBDN. They compute the explicit model when constructed and store it for later use, making them more efficient.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"See Can't I just use DiffLBDN? in Reinforcement Learning with LBDN for a demonstration of this trade-off.","category":"page"},{"location":"introduction/package_overview/#Onto-the-GPU","page":"Package Overview","title":"Onto the GPU","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"If you have a GPU on your machine, then you're in luck. All models in RobustNeuralNetworks.jl can be loaded onto the GPU for training and evaluation in exactly the same way as any other Flux.jl model. To adapt our example from Explicit model wrappers to run on the GPU, we would do the following.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"using CUDA\n\nmodel_params = model_params |> gpu\ndata = data |> gpu\n\nopt_state = Flux.setup(Adam(0.01), model_params)\nfor _ in 1:50\n    Flux.train!(loss, model_params, data, opt_state)\nend","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"An example of training a DiffLBDN on the GPU is provided in Image Classification with LBDN. See Flux.jl's GPU support page for more information on training models with different GPU backends.","category":"page"},{"location":"introduction/package_overview/#Robustness-metrics-and-IQCs","page":"Package Overview","title":"Robustness metrics and IQCs","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"All neural network models in RobustNeuralNetworks.jl are designed to satisfy a set of user-defined robustness constraints. There are a number of different robustness criteria which our RENs can satisfy. Some relate to the internal dynamics of the model, others relate to the input-output map. LBDNs are less general, and are specifically constructed to satisfy Lipschitz bounds. See the section on Lipschitz bounds (smoothness) below.","category":"page"},{"location":"introduction/package_overview/#Contracting-systems","page":"Package Overview","title":"Contracting systems","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"First and foremost, all of our RENs are contracting systems. This means that they exponentially \"forget\" initial conditions. If the system starts at two different initial conditions but is given the same inputs, the internal states will converge over time. See below for an example of a contracting REN with a single internal state. The code used to generate this figure can be found here.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"@html_str \"\"\"<p align=\"center\"> <object type=\"image/svg+xml\" data=$(joinpath(Main.buildpath, \"../assets/contracting_ren.svg\")) width=\"50%\"></object> </p>\"\"\" #hide","category":"page"},{"location":"introduction/package_overview/#Integral-quadratic-constraints","page":"Package Overview","title":"Integral quadratic constraints","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"We define additional robustness criteria on the input/output map of our RENs with incremental integral quadratic constraints (IQCs). Suppose we have a model mathcalM starting at two different initial conditions ab with two different input signals u v, and consider their corresponding output trajectories y^a = mathcalM_a(u) and y^b = mathcalM_b(v) The model mathcalM satisfies the IQC defined by matrices (Q S R) if","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"sum_t=0^T\nbeginbmatrix\ny^a_t - y^b_t  u_t - v_t\nendbmatrix^top\nbeginbmatrix\nQ  S^top  S  R\nendbmatrix\nbeginbmatrix\ny^a_t - y^b_t  u_t - v_t\nendbmatrix \nge -d(ab)\nquad forall  T","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"for some function d(ab) ge 0 with d(aa) = 0, where 0 preceq Q in mathbbR^n_ytimes n_y, SinmathbbR^n_utimes n_y R=R^top in mathbbR^n_utimes n_u ","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"In general, the IQC matrices (QSR) can be chosen (or optimised) to meet a range of performance criteria. There are a few special cases that are worth noting.","category":"page"},{"location":"introduction/package_overview/#Lipschitz-bounds-(smoothness)","page":"Package Overview","title":"Lipschitz bounds (smoothness)","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"If Q = -frac1gammaI, R = gamma I, S = 0, the model mathcalM satisfies a Lipschitz bound (incremental ell^2-gain bound) of gamma.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"mathcalM_a(u) - mathcalM_b(v)_T le gamma u - v_T","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"Qualitatively, the Lipschitz bound is a measure of how smooth the network is. If the Lipschitz bound gamma is small, then small changes in the inputs uv will lead to small changes in the model output. If gamma is large, then the model output might change significantly for even small changes to the inputs. This can make the model more sensitive to noise, adversarial attacks, and other input disturbances.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"As the name suggests, the LBDN models are all constructed to have a user-tunable Lipschitz bound.","category":"page"},{"location":"introduction/package_overview/#Incremental-passivity","page":"Package Overview","title":"Incremental passivity","text":"","category":"section"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"There are two cases to consider here. In both cases, the network must have the same number of inputs and outs.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"If Q = 0 R = -2nu I S = I where nu ge 0, the model is incrementally passive (incrementally strictly input passive if nu  0). Mathematically, the following inequality holds.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"langle mathcalM_a(u) - mathcalM_b(v) u-v rangle_T ge nu  u-v^2_T","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"If Q = -2rho I R = 0 S = I where rho  0, the model is incrementally strictly output passive. Mathematically, the following inequality holds.","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"langle mathcalM_a(u) - mathcalM_b(v) u-v rangle_T ge rho  mathcalM_a(u) - mathcalM_b(v)^2_T","category":"page"},{"location":"introduction/package_overview/","page":"Package Overview","title":"Package Overview","text":"For more details on IQCs and their use in RENs, please see Revay, Wang & Manchester (2021).","category":"page"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"lib/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"direct_to_explicit\nget_lipschitz\ninit_states\nset_output_zero!\nupdate_explicit!","category":"page"},{"location":"lib/functions/#RobustNeuralNetworks.direct_to_explicit","page":"Functions","title":"RobustNeuralNetworks.direct_to_explicit","text":"direct_to_explicit(ps::AbstractRENParams{T}, return_h=false) where T\n\nConvert direct parameterisation of RENs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitRENParams object that naturally satisfies a set of user-defined behavioural constraints.\n\nArguments\n\nps::AbstractRENParams: Direct parameterisation with behavioural constraints to convert to an explicit parameterisation of REN (eg: GeneralRENParams).\nreturn_h::Bool=false: Whether to return the H-matrix directly (see Revay et al. (2021)). Useful for debugging or model analysis. If false, function returns an object of type ExplicitRENParams{T}. \n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\ndirect_to_explicit(ps::AbstractRENParams{T}) where T\n\nConvert direct parameterisation of LBDNs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitLBDNParams object that naturally respects a user-defined Lipschitz bound.\n\nArguments\n\nps::AbstractLBDNParams: Direct parameterisation of an LBDN to convert to an explicit parameterisation for model evaluation (eg: DenseLBDNParams).\n\nSee also DenseLBDNParams.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.get_lipschitz","page":"Functions","title":"RobustNeuralNetworks.get_lipschitz","text":"get_lipschitz(model)\n\nExtract Lipschitz bound from a Lipschitz-bounded model\n\nReturns Lipschitz bound as a float. Function only works on the following types:\n\nLBDN and DiffLBDN\nDenseLBDNParams and DirectLBDNParams\nLipschitzRENParams\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.init_states","page":"Functions","title":"RobustNeuralNetworks.init_states","text":"init_states(m::AbstractREN, nbatches; rng=nothing)\n\nReturn matrix of (nbatches) state vectors of a REN initialised as zeros.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.set_output_zero!","page":"Functions","title":"RobustNeuralNetworks.set_output_zero!","text":"set_output_zero!(m::AbstractRENParams)\n\nSet output map of a REN to zero.\n\nIf the resulting model is called with\n\nren = REN(m)\nx1, y = ren(x, u)\n\nthen y = 0 for any x and u.\n\n\n\n\n\nset_output_zero!(m::AbstractLBDNParams)\n\nSet output map of an LBDN to zero.\n\nIf the resulting model is called with \n\nlbdn = LBDN(m)\ny = lbdn(u)\n\nthen y = 0 for any u.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.update_explicit!","page":"Functions","title":"RobustNeuralNetworks.update_explicit!","text":"update_explicit!(m::WrapREN)\n\nUpdate explicit model in WrapREN using the current direct parameters.\n\n\n\n\n\n","category":"function"},{"location":"examples/box_obsv/#Observer-Design-with-REN","page":"Observer Design","title":"Observer Design with REN","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Full example code can be found here.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"In Reinforcement Learning with LBDN, we designed a controller for a simple nonlinear system consisting of a box sitting in a tub of fluid, suspended between two springs. We assumed the controller had full state knowledge: i.e, it had access to both the position and velocity of the box. In many practical situations, we might only be able to measure some of the system states. For example, our box may have a camera to estimate its position but not its velocity. In these cases, we need a state observer to estimate the full state of the system for feedback control.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"In this example, we will show how a contracting REN can be used to learn stable observers for dynamical systems. A common approach to designing state estimators for nonlinear systems is the Extended Kalman Filter (EKF). In our case, we'll consider observer design as a supervised learning problem. For a detailed explanation of the theory behind this example, please refer to Section VIII of Revay, Wang & Manchester (2021). ","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"See PDE Observer Design with REN for explanation of a more complex example from the paper.","category":"page"},{"location":"examples/box_obsv/#.-Background-theory","page":"Observer Design","title":"1. Background theory","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Suppose we have a discrete-time, nonlinear dynamical system of the form","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"beginaligned\nx_t+1 = f_d(x_t u_t) \ny_t = g_d(x_t u_t)\nendaligned","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"with state vector x_t controlled inputs u_t and measured outputs y_t Our aim is to estimate the sequence x_0 x_1 ldots x_T  over some time period 0T given only the measurements y_t and inputs u_t at each time step. We'll use a very general form for an observer","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"hatx_t+1 = f_o(hatx_t u_t y_t)","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"where hatx is the state estimate. For those interested, a more common structure is the Luenberger observer.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"We want the observer error to converge to zero as time progresses, or hatx_t rightarrow x_t as t rightarrow infty. It turns out that our observer only has to satisfy the following two conditions to guarantee this.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"The observer must be a contracting system (see Contracting systems).\nThe observer must satisfy a \"correctness\" condition which says that, given perfect knowledge of the state, measurements, and inputs, the observer can exactly predict the next state. Mathematically, we write this as","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"f_o(x_tu_ty_t) = f_d(x_tu_t)","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Note the use of x_t not hatx_t above. It turns out that if the correctness condition is only approximately satisfied so that f_o(x_tu_ty_t) - f_d(x_tu_t)  rho for some small number rho, then the observer error will still be bounded. See Appendix E of the paper for details.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Lucky for us, RobustNeuralNetworks.jl contains REN models that are guaranteed to be contracting. To learn a stable observer with RENs, all we have to do is minimise the one-step-ahead prediction error. I.e: if we have a batch of data z = x_i u_i y_i  i = 12ldotsN then we should train our model to minimise the loss function","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"mathcalL(z theta) = sum_i=1^N f_o(x_iu_iy_i) - f_d(x_iu_i)^2","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"where theta contains the learnable parameters of the REN.","category":"page"},{"location":"examples/box_obsv/#.-Generate-training-data","page":"Observer Design","title":"2. Generate training data","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Consider the same nonlinear box system we used for Reinforcement Learning with LBDN, this time with a measurement function gd to give y_t = g_d(x_tu_t). We'll assume that only the position of the box is known, so y_t = x_t.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"m = 1                   # Mass (kg)\nk = 5                   # Spring constant (N/m)\nμ = 0.5                 # Viscous damping coefficient (kg/m)\nnx = 2                  # Number of states\n\n# Continuous and discrete dynamics and measurements\n_visc(v::Matrix) = μ * v .* abs.(v)\nf(x::Matrix,u::Matrix) = [x[2:2,:]; (u[1:1,:] - k*x[1:1,:] - _visc(x[2:2,:]))/m]\nfd(x,u) = x + dt*f(x,u)\ngd(x::Matrix) = x[1:1,:]","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"We'll assume for this example that the box always starts at rest in a random initial position between pm05m, after which it is released and allowed to oscillate freely with no added forces (so u = 0). Learning an observer typically requires a large amount of training data to capture the behaviour of the system in different scenarios, so we'll consider 200 batches simulating 10s of motion.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"using Random\nrng = MersenneTwister(0)\n\ndt = 0.01               # Time-step (s)\nTmax = 10               # Simulation horizon\nts = 1:Int(Tmax/dt)     # Time array indices\n\nbatches = 200\nu  = fill(zeros(1, batches), length(ts)-1)\nX  = fill(zeros(1, batches), length(ts))\nX[1] = 0.5*(2*rand(rng, nx, batches) .-1)\n\nfor t in ts[1:end-1]\n    X[t+1] = fd(X[t],u[t])\nend","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"We've stored the states of the system across each batch in X. To compute the one-step-ahead loss mathcalL we'll need to separate this data into the states at the \"current\" time Xt and at the \"next\" time Xn, then compute the measurement outputs.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Xt = X[1:end-1]\nXn = X[2:end]\ny = gd.(Xt)","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"With that done, we store the data for training, shuffling it so there is no bias in the training towards earlier timesteps.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"observer_data = [[ut; yt] for (ut,yt) in zip(u, y)]\nindx = shuffle(rng, 1:length(observer_data))\ndata = zip(Xn[indx], Xt[indx], observer_data[indx])","category":"page"},{"location":"examples/box_obsv/#.-Define-a-model","page":"Observer Design","title":"3. Define a model","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Since we need our model to be a contracting dynamical system, the obvious choice is to use ContractingRENParams. The inputs to the model are u_ty_t, and its outputs should be the state estimate hatx_t+1. The flag output_map=false sets the output map of the REN to just return its own internal state. That way, the internal state of the REN is exactly the state estimate hatx.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"using RobustNeuralNetworks\n\nnv = 200\nnu = size(observer_data[1], 1)\nny = nx\nmodel_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; output_map=false, rng)\nmodel = DiffREN(model_ps)","category":"page"},{"location":"examples/box_obsv/#.-Train-the-model","page":"Observer Design","title":"4. Train the model","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"As mentioned above, our loss function should be the one-step-ahead prediction error of the REN observer. We can write this as follows.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"using Statistics\n\nfunction loss(model, xn, xt, inputs)\n    xpred = model(xt, inputs)[1]\n    return mean(sum((xn - xpred).^2, dims=1))\nend","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"We've written a function to train the observer that decreases the learning rate by a factor of 10 if the mean gets stuck or starts to increase. The core of this function is just a simple Flux.jl training loop. We also report the mean loss at each epoch to inform the user how training is progressing.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"using Flux\nusing Printf\n\nfunction train_observer!(model, data; epochs=50, lr=1e-3, min_lr=1e-6)\n\n    opt_state = Flux.setup(Adam(lr), model)\n    mean_loss = [1e5]\n    for epoch in 1:epochs\n\n        # Gradient descent update\n        batch_loss = []\n        for (xn, xt, inputs) in data\n            train_loss, ∇J = Flux.withgradient(loss, model, xn, xt, inputs)\n            Flux.update!(opt_state, model, ∇J[1])\n            push!(batch_loss, train_loss)\n        end\n        @printf \"Epoch: %d, Lr: %.1g, Loss: %.4g\\n\" epoch lr mean(batch_loss)\n\n        # Drop learning rate if mean loss is stuck or growing\n        push!(mean_loss, mean(batch_loss))\n        if (mean_loss[end] >= mean_loss[end-1]) && !(lr <= min_lr)\n            lr = 0.1lr\n            Flux.adjust!(opt_state, lr)\n        end\n    end\n    return mean_loss\nend\ntloss = train_observer!(model, data)","category":"page"},{"location":"examples/box_obsv/#.-Evaluate-the-trained-model","page":"Observer Design","title":"5. Evaluate the trained model","text":"","category":"section"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Now that we've trained the REN observer to minimise the one-step-ahead prediction error, let's see if the observer error actually does converge to zero. First, we'll need some test data. ","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"batches   = 50\nts_test   = 1:Int(20/dt)\nu_test    = fill(zeros(1, batches), length(ts_test))\nx_test    = fill(zeros(nx,batches), length(ts_test))\nx_test[1] = 0.2*(2*rand(rng, nx, batches) .-1)\n\nfor t in ts_test[1:end-1]\n    x_test[t+1] = fd(x_test[t], u_test[t])\nend\nobserver_inputs = [[u;y] for (u,y) in zip(u_test, gd.(x_test))]","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Next, we'll need a function to simulate the REN observer using its own state hatx rather than the true system state. We can use the very neat tool Flux.Recur for this. We'll assume the observer has no idea what the initial state is, so guess that hatx_0 = 0.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"function simulate(model::AbstractREN, x0, u)\n    recurrent = Flux.Recur(model, x0)\n    output = recurrent.(u)\n    return output\nend\nx0hat = init_states(model, batches)\nxhat = simulate(model, x0hat, observer_inputs)","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"Having simulated the state estimate on the test data, it's time to plot our results. This takes a little bit of setting up to make it look nice, but all the code below is just formatting and plotting.","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"using CairoMakie\n\nfunction plot_results(x, x̂, ts)\n\n    # Observer error\n    Δx = x .- x̂\n\n    ts = ts.*dt\n    _get_vec(x, i) = reduce(vcat, [xt[i:i,:] for xt in x])\n    q   = _get_vec(x,1)\n    q̂   = _get_vec(x̂,1)\n    qd  = _get_vec(x,2)\n    q̂d  = _get_vec(x̂,2)\n    Δq  = _get_vec(Δx,1)\n    Δqd = _get_vec(Δx,2)\n\n    fig = Figure(resolution = (800, 400))\n    ga = fig[1,1] = GridLayout()\n\n    ax1 = Axis(ga[1,1], xlabel=\"Time (s)\", ylabel=\"Position (m)\", title=\"States\")\n    ax2 = Axis(ga[1,2], xlabel=\"Time (s)\", ylabel=\"Position (m)\", title=\"Observer Error\")\n    ax3 = Axis(ga[2,1], xlabel=\"Time (s)\", ylabel=\"Velocity (m/s)\")\n    ax4 = Axis(ga[2,2], xlabel=\"Time (s)\", ylabel=\"Velocity (m/s)\")\n    axs = [ax1, ax2, ax3, ax4]\n\n    for k in axes(q,2)\n        lines!(ax1, ts,  q[:,k],  linewidth=0.5,  color=:grey)\n        lines!(ax1, ts,  q̂[:,k],  linewidth=0.25, color=:red)\n        lines!(ax2, ts, Δq[:,k],  linewidth=0.5,  color=:grey)\n        lines!(ax3, ts,  qd[:,k], linewidth=0.5,  color=:grey)\n        lines!(ax3, ts,  q̂d[:,k], linewidth=0.25, color=:red)\n        lines!(ax4, ts, Δqd[:,k], linewidth=0.5,  color=:grey)\n    end\n\n    qmin, qmax = minimum(minimum.((q,q̂))), maximum(maximum.((q,q̂)))\n    qdmin, qdmax = minimum(minimum.((qd,q̂d))), maximum(maximum.((qd,q̂d)))\n    ylims!(ax1, qmin, qmax)\n    ylims!(ax2, qmin, qmax)\n    ylims!(ax3, qdmin, qdmax)\n    ylims!(ax4, qdmin, qdmax)\n    xlims!.(axs, ts[1], ts[end])\n    display(fig)\n    return fig\nend\nfig = plot_results(x_test, xhat, ts_test)","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"(Image: )","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"In the left-hand panels, grey lines represent the true states of the system, while red lines are for the observer prediction. In the right-hand panels, we see the observer error nicely converging to zero as the observer identifies the correct velocity for all simulation runs. ","category":"page"},{"location":"examples/box_obsv/","page":"Observer Design","title":"Observer Design","text":"It's worth noting that at no point did we directly train the REN to minimise the observer error. This is a natural result of using a model that is guaranteed to be contracting, and training it to minimise the one-step-ahead prediction error. Note that there is still some residual observer error in the velocity, since our observer is only trained to approximately satisfy the correctness condition.","category":"page"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"Pages = [\"model_params.md\"]","category":"page"},{"location":"lib/model_params/#Model-Parameterisations","page":"Model Parameterisations","title":"Model Parameterisations","text":"","category":"section"},{"location":"lib/model_params/#Lipschitz-Bounded-Deep-Networks","page":"Model Parameterisations","title":"Lipschitz-Bounded Deep Networks","text":"","category":"section"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"AbstractLBDNParams\nDenseLBDNParams\nDirectLBDNParams\nExplicitLBDNParams","category":"page"},{"location":"lib/model_params/#RobustNeuralNetworks.AbstractLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.AbstractLBDNParams","text":"abstract type AbstractLBDNParams{T, L} end\n\nDirect parameterisation for Lipschitz-bounded deep networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DenseLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DenseLBDNParams","text":"DenseLBDNParams{T}(nu, nh, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a dense (fully-connected) LBDN.\n\nThis is the equivalent of a multi-layer perceptron (eg: Flux.Dense) with a guaranteed Lipschitz bound of γ. Note that the Lipschitz bound can made a learnable parameter.\n\nArguments\n\nnu::Int: Number of inputs.\nnh::Union{Vector{Int}, NTuple{N, Int}}: Number of hidden units for each layer. Eg: nh = [5,10] for 2 hidden layers with 5 and 10 nodes (respectively).\nny::Int: Number of outputs.\nγ::Real=T(1): Lipschitz upper bound, must be positive.\n\nKeyword arguments:\n\nnl::Function=relu: Sector-bounded static nonlinearity.\nlearn_γ::Bool=false: Whether to make the Lipschitz bound γ a learnable parameter.\n\nSee DirectLBDNParams for documentation of keyword arguments initW, initb, rng.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DirectLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DirectLBDNParams","text":"DirectLBDNParams{T}(nu, nh, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation for a Lipschitz-bounded deep network.\n\nThis is typically used by a higher-level constructor to define an LBDN model, which takes the direct parameterisation in DirectLBDNParams and defines rules for converting it to an explicit parameterisation. See for example DenseLBDNParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnh::Union{Vector{Int}, NTuple{N, Int}}: Number of hidden units for each layer. Eg: nh = [5,10] for 2 hidden layers with 5 and 10 nodes (respectively).\nny::Int: Number of outputs.\nγ::Real=T(1): Lipschitz upper bound, must be positive.\n\nKeyword arguments\n\ninitW::Function=Flux.glorot_normal: Initialisation function for implicit params X,Y,d.\ninitb::Function=Flux.glorot_normal: Initialisation function for bias vectors.\nlearn_γ::Bool=false: Whether to make the Lipschitz bound γ a learnable parameter.\nrng::AbstractRNG = Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Wang et al. (2023) for parameterisation details.\n\nSee also DenseLBDNParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ExplicitLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ExplicitLBDNParams","text":"mutable struct ExplicitLBDNParams{T, N, M}\n\nExplicit LBDN parameter struct.\n\nThese parameters define the explicit form of a Lipschitz-bounded deep network used for model evaluation. Parameters are stored in NTuples, where each element of an NTuple is the parameter for a single layer of the network. Tuples are faster to work with than vectors of arrays.\n\nSee Wang et al. (2023) for more details on explicit parameterisations of LBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#Recurrent-Equilibrium-Networks","page":"Model Parameterisations","title":"Recurrent Equilibrium Networks","text":"","category":"section"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"AbstractRENParams\nContractingRENParams\nDirectRENParams\nExplicitRENParams\nGeneralRENParams\nLipschitzRENParams\nPassiveRENParams","category":"page"},{"location":"lib/model_params/#RobustNeuralNetworks.AbstractRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.AbstractRENParams","text":"abstract type AbstractRENParams{T} end\n\nDirect parameterisation for recurrent equilibrium networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ContractingRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ContractingRENParams","text":"ContractingRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a contracting REN.\n\nThe parameters can be used to construct an explicit REN model that has guaranteed, built-in contraction properties.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nnl::Function=relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, output_map, rng.\n\nSee also GeneralRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\nContractingRENParams(nv, A, B, C, D; ...)\n\nAlternative constructor for ContractingRENParams that initialises the REN from a stable discrete-time linear system with state-space model\n\nbeginalign*\nx_t+1 = Ax_t + Bu_t \ny_t = Cx_t + Du_t\nendalign*\n\n[TODO:] This method has not been used or tested in a while. If you find it useful, please reach out to us and we will add full support and testing! :) [TODO:] Make compatible with αbar ≠ 1.0.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DirectRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DirectRENParams","text":"DirectRENParams{T}(nu, nx, nv; <keyword arguments>) where T\n\nConstruct direct parameterisation for an (acyclic) recurrent equilibrium network.\n\nThis is typically used by higher-level constructors when defining a REN, which take the direct parameterisation and define rules for converting it to an explicit parameterisation. See for example GeneralRENParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\n\nKeyword arguments\n\ninit=:randomQR: Initialisation method. Options are:\n:random: Random sampling with Glorot normal distribution. Typically samples \"faster\"/short memory dynamic models.\n:randomQR: Compute X with glorot_normal and take the QR decomposition X = qr(X).Q. Good for initialising X close to the identity when long memory is needed. Default as legacy.\n:cholesky: Compute X with cholesky factorisation of H, sets E,F,P = I. Good for slow/long memory dynamic models.\npolar_param::Bool=true: Use polar parameterisation to construct H matrix from X in REN parameterisation (recommended).\nD22_free::Bool=false: Specify whether to train D22 as a free parameter (true), or construct it separately from X3, Y3, Z3 (false). Typically use D22_free = true only for a contracting REN.\nD22_zero::Bool=false: Fix D22 = 0 to remove any feedthrough.\nbx_scale::T=0: Set scale of initial state bias vector bx.\nbv_scale::T=1: Set scalse of initial neuron input bias vector bv.\noutput_map::Bool=true: Include output layer y_t = C_2 x_t + D_21 w_t + D_22 u_t + b_y. Otherwise, output is just y_t = x_t.\nϵ::T=1e-12: Regularising parameter for positive-definite matrices.\nrng::AbstractRNG=Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Revay et al. (2021) for parameterisation details.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ExplicitRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ExplicitRENParams","text":"mutable struct ExplicitRENParams{T}\n\nExplicit REN parameter struct.\n\nThese parameters define a recurrent equilibrium network with model inputs and outputs u_t y_t, neuron inputs and outputs v_tw_t, and states x_t.\n\nbeginequation*\nbeginbmatrix\nx_t+1  v_t  y_t\nendbmatrix\n= \nbeginbmatrix\nA  B_1  B_2 \nC_1  D_11  D_12 \nC_2  D_21  D_22 \nendbmatrix\nbeginbmatrix\nx_t  w_t  u_t\nendbmatrix\n+ \nbeginbmatrix\nb_x  b_v  b_y\nendbmatrix\nendequation*\n\nSee Revay et al. (2021) for more details on explicit parameterisations of REN.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.GeneralRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.GeneralRENParams","text":"GeneralRENParams{T}(nu, nx, nv, ny, Q, S, R; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN satisfying general behavioural constraints.\n\nBehavioural constraints are encoded by the matrices Q,S,R in an incremental Integral Quadratic Constraint (IQC). See Equation 4 of Revay et al. (2021).\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nQ::AbstractMatrix: IQC weight matrix on model outputs\nS::AbstractMatrix: IQC coupling matrix on model outputs/inputs\nR::AbstractMatrix: IQC weight matrix on model outputs\n\nKeyword arguments\n\nnl::Function=relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.LipschitzRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.LipschitzRENParams","text":"LipschitzRENParams(nu, nx, nv, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN with a Lipschitz bound of γ.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nγ::Number: Lipschitz upper bound.\n\nKeyword arguments\n\nnl::Function=relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\nlearn_γ::Bool=false: Whether to make the Lipschitz bound γ a learnable parameter.\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, ContractingRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.PassiveRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.PassiveRENParams","text":"PassiveRENParams{T}(nu, nx, nv, ny, ν, ρ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a passive REN.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nν::Number=0: Passivity index. Use ν > 0 for an incrementally strictly input passive model. Set both ν = 0 and ρ = 0 for incrementally passive model.\nρ::Number=0: Passivity index. Use ρ > 0 for an incrementally strictly output passive model. \n\nNote that setting both ν,ρ > 0 or both ν,ρ < 0 is not currently supported and will throw an error.\n\nKeyword arguments\n\nnl::Function=relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams.\n\n\n\n\n\n","category":"type"},{"location":"#RobustNeuralNetworks.jl-Documentation","page":"Home","title":"RobustNeuralNetworks.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for robust neural networks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for RobustNeuralNetworks.jl! This package contains neural network models that are constructed to naturally satisfy robustness constraints, all in native Julia. Check out our GitHub repository here.","category":"page"},{"location":"#Why-Robust-Models?","page":"Home","title":"Why Robust Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modern machine learning relies heavily on rapidly training and evaluating neural networks in problems ranging from image classification to robotic control. Most neural network architectures have no robustness certificates, and can be sensitive to adversarial attacks, poor data quality, and other input perturbations. Many solutions that address this brittle behaviour rely on explicitly enforcing constraints during training to smooth or stabilise the network response. While effective on small-scale problems, these methods are computationally expensive, making them slow and difficult to scale up to complex real-world problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Recently, we proposed the Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) or sandwich layer model classes as computationally efficient solutions to these problems. The REN architecture is flexible in that it includes many common neural network models, such as multi-layer-perceptrons (MLPs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The weights and biases in RENs are directly parameterised to naturally satisfy behavioural constraints chosen by the user. For example, we can build a REN with a given Lipschitz constant to ensure its output is quantifiably less sensitive to input perturbations. LBDNs are specializations of RENs with the specific feed-forward structure of deep neural networks like MLPs or CNNs and built-in guarantees on the Lipschitz bound.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The direct parameterisation of RENs and LBDNs means that we can train models with standard, unconstrained optimisation methods (such as stochastic gradient descent) while also guaranteeing their robustness. Achieving the \"best of both worlds\" in this way is the main advantage of the REN and LBDN model classes, and allows the user to freely train robust models for many common machine learning problems, as well as for more challenging real-world applications where safety is critical.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"introduction/getting_started.md\", \"introduction/package_overview.md\", \"introduction/developing.md\"]\nDepth = 1","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples/lbdn_curvefit.md\", \"examples/lbdn_mnist.md\", \"examples/rl.md\", \"examples/box_obsv.md\", \"examples/echo_ren.md\"]\nDepth = 1","category":"page"},{"location":"#Library","page":"Home","title":"Library","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"lib/models.md\", \"lib/model_params.md\", \"lib/functions.md\"]\nDepth = 1","category":"page"},{"location":"#Citing-the-Package","page":"Home","title":"Citing the Package","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use RobustNeuralNetworks.jl for any research or publications, please cite our work as necessary.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{barbara2023robustneuralnetworksjl,\n   title   = {RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness},\n   author  = {Nicholas H. Barbara and Max Revay and Ruigang Wang and Jing Cheng and Ian R. Manchester},\n   journal = {arXiv preprint arXiv:2306.12612},\n   month   = {6},\n   year    = {2023},\n   url     = {https://arxiv.org/abs/2306.12612v1},\n}","category":"page"},{"location":"#Research-Papers","page":"Home","title":"Research Papers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RobustNeurlaNetworks.jl is built on the REN and LBDN model classes described in the following two papers (respectively):","category":"page"},{"location":"","page":"Home","title":"Home","text":"M. Revay, R. Wang, and I. R. Manchester, \"Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness\" IEEE Trans Automat Contr 1–16 (2023) doi:10.1109/TAC.2023.3294101.","category":"page"},{"location":"","page":"Home","title":"Home","text":"R. Wang and I. R. Manchester, \"Direct parameterization of Lipschitz-bounded deep networks\" in Proceedings of the 40th International Conference on Machine Learning (PMLR, 2023) 202:36093-36110.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The REN parameterisation was extended to continuous-time systems in [yet to be implemented]:","category":"page"},{"location":"","page":"Home","title":"Home","text":"D. Martinelli, C. L. Galimberti, I. R. Manchester, L. Furieri, and G. Ferrari-Trecate, \"Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations,\" April 2023. doi: https://doi.org/10.48550/arXiv.2304.02976.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See below for a collection of projects and papers using RobustNeuralNetworks.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"N. H. Barbara, R. Wang, and I. R. Manchester, \"Learning Over Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems,\" April 2023. doi: https://arxiv.org/abs/2304.06193v2.","category":"page"}]
}
