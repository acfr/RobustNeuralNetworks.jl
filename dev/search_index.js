var documenterSearchIndex = {"docs":
[{"location":"introduction/developing/#Contributing-to-the-Package","page":"Contributing to the Package","title":"Contributing to the Package","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All contributors welcome! Please contact nicholas.barbara@sydney.edu.au with any questions.","category":"page"},{"location":"introduction/developing/#Installation-for-Development","page":"Contributing to the Package","title":"Installation for Development","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the package, clone the repository into your ~/.julia/dev/ directory with","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"git clone git@github.com:acfr/RobustNeuralNetworks.jl.git RobustNeuralNetworks","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Note that the repo is RobustNeuralNetworks.jl but the folder name is RobustNeuralNetworks. This is convention for Julia packages. Navigate to the repository directory, start a Julia session, and type the following in the REPL to activate the package.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"using Pkg\nPkg.instantiate()\nPkg.activate(\".\")","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Check that the example in Getting Started runs without errors and matches the given output before continuing.","category":"page"},{"location":"introduction/developing/#Package-Structure","page":"Contributing to the Package","title":"Package Structure","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The main file is src/RobustNeuralNetworks.jl. This imports all relevant packages, defines abstract types, includes code from other files, and exports the necessary components of our package. ","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"All using PackageName statements should be included in this file.\nOnly import the packages you really need\nIf you only need one function from a package, import it explicitly (not the whole package)","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"When including files in our src/ folder, the order often matters. Code should only ever be included with a single include statement in the main file. Please follow the convention outlined in the comments.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The source files for our package are all in the src/ directory, and are split into the following sub-directories.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"src/Base/: Contains code relevant to the core package functionality.\nsrc/ParameterTypes/: Contains the various REN parameterisations, all of type AbstractRENParams.\nsrc/LBDN/: Contains code exclusively used for AbstractLBDN models","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Once you have written any code for this package, be sure to test it thoroughly. Write testing scripts in the test/ directory.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"See Test.jl documentation for help with writing good package tests.\nRun all tests for the package with ] test in the REPL.\nAll tests will be run by the CI client when submitting pull requests to the main git branch.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"If you would like to contribute the docs, this page provides a great outline of the required workflow.","category":"page"},{"location":"introduction/developing/#Git-Workflow","page":"Contributing to the Package","title":"Git Workflow","text":"","category":"section"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"The package can be treated as an independent git repository for devlopment.","category":"page"},{"location":"introduction/developing/","page":"Contributing to the Package","title":"Contributing to the Package","text":"Please feel free to submit git issues, pull requests, etc. as usual.\nAlways develop new features in a new branch labelled feature/<some_descriptive_words>. For example, the branch feature/documentation is where this documentation was first written and tested.\nSubmit pull requests once you have completed a new feature and tested it thorougly. Pull requests without thorough testing will be rejected.","category":"page"},{"location":"examples/pde_obsv/#PDE-Observer-Design-with-REN","page":"PDE Observer","title":"PDE Observer Design with REN","text":"","category":"section"},{"location":"examples/pde_obsv/","page":"PDE Observer","title":"PDE Observer","text":"[Example coming soon. See Section VIII of Revay, Wang & Manchester (2021).]","category":"page"},{"location":"introduction/layout/#Package-Overview","page":"Package Overview","title":"Package Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The RobustNeuralNetwork.jl package is divided into Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) models.","category":"page"},{"location":"introduction/layout/#REN-Overview","page":"Package Overview","title":"REN Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The REN models are defined by two fundamental types:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Any subtype of AbstractRENParams holds all the information required to directly parameterise a REN satisfying some user-defined behavioural constraints.\nAny subtype of AbstractREN represents the REN in its explicit form so that it can be called and evaluated.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"info: Separate Objects for Parameters and Model\nWhen working with most models (eg: RNN and LSTM) the typical workflow is to create a single instance of a model. Its parameters are updated during training, but the model object is only created once. For example:using Flux\n\n# Define a model\nmodel = Flux.RNNCell(2,5)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parametersWhen working with RENs, it is much more efficient to split up the model parameterisation and the model implementation into subtypes of AbstractRENParams and AbstractREN. Converting our direct parameterisation to an explicit model for evaluation can be slow, so we only do it when the model parameters are updated:using Flux\nusing RobustNeuralNetworks\n\n# Define a model parameterisation\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\n\n# Train the model\nfor k in 1:num_training_epochs\n    model = REN(params)     # Create explicit model for evaluation\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parametersSee the section on REN Wrappers for more details.","category":"page"},{"location":"introduction/layout/#(Direct)-Parameter-Types","page":"Package Overview","title":"(Direct) Parameter Types","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Subtypes of AbstractRENParams define direct parameterisations of a REN. They are not callable models. There are four REN parameter types currently in this package:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"ContractingRENParams parameterises a REN with a user-defined upper bound on the contraction rate.\nLipschitzRENParams parameterises a REN with a user-defined Lipschitz constant of gamma in (0infty).\nPassiveRENParams parameterises an input/output passive REN with user-tunable passivity parameter nu ge 0.\nGeneralRENParams parameterises a REN satisfying some generalbehavioural constraints defined by an Integral Quadratic Constraint (IQC).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"For more information on these four parameterisations, please see Revay et al. (2021).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Each of these parameter types has the following collection of attributes:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"A static nonlinearity nl. Common choices are Flux.relu or Flux.tanh (see Flux.jl for more information).\nModel sizes nu, nx, nv, ny defining the number of inputs, states, neurons, and outputs (respectively).\nAn instance of DirectRENParams containing the direct parameters of the REN, including all trainable parameters.\nOther attributes used to define how the direct parameterisation should be converted to the implicit model. These parameters encode the user-tunable behavioural constraints. Eg: gamma for a Lipschitz-bounded REN.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"The typical workflow is to create an instance of a REN parameterisation only once. This defines all dimensions and desired properties of a REN. It is then converted to an explicit model for the REN to be evaluated.","category":"page"},{"location":"introduction/layout/#Explicit-REN-Models","page":"Package Overview","title":"Explicit REN Models","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"An explicit REN model must be created to call and use the network for computation. The explicit parameterisation contains all information required to evaluate a REN. We encode RENs in explicit form as subtypes of the AbstractREN type. Each subtype of AbstractREN is callable and includes the following attributes:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"A static nonlinearity nl and model sizes nu, nx, nv, ny (same as AbstractRENParams.\nAn instance of ExplicitRENParams containing all REN parameters in explicit form for model evaluation (see the ExplicitRENParams docs for more detail).","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Each subtype of AbstractRENParams has a method direct_to_explicit associated with it that converts the DirectRENParams struct to an instance of ExplicitRENParams satisfying the specified behavioural constraints.","category":"page"},{"location":"introduction/layout/#REN-Wrappers","page":"Package Overview","title":"REN Wrappers","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"There are three explicit REN wrappers currently implemented in this package. Each of them constructs a REN from a direct parameterisation params::AbstractRENParams and can be used to evaluate REN models.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"REN is the basic and most commonly-used wrapper. A new instance of REN must be created whenever the parameters params are changed.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"tip: REN is recommended\nWe strongly recommend using REN to train your models with Flux.jl. It is the most efficient subtype of AbstractREN that is compatible with automatic differentiation.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"WrapREN includes both the DirectRENParams and ExplicitRENParams as part of the REN wrapper. When any of the direct parameters are changed, the explicit model can be updated by calling update_explicit!. This can be useful when not using automatic differentiation to train the model. For example:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using RobustNeuralNetworks\n\n# Define a model parameterisation AND a model\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\nmodel  = WrapREN(params)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    ...                     # Update model parameters\n    update_explicit!(model) # Update explicit model parameters","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"warning: WrapREN incompatible with Flux.jl\nSince the explicit parameters are stored in an instance of WrapREN, changing them with update_explicit! directly mutates the model. This will cause errors if the model is to be trained with Flux.jl. Use REN or DiffREN to avoid this issue.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"DiffREN also includes DirectRENParams, but never stores the ExplicitRENParams. Instead, the explicit parameters are computed every time the model is evaluated. This is slow, but does not require creating a new object when the parameters are updated, and is still compatible with Flux.jl. For example:","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using Flux\n\n# Define a model parameterisation AND a model\nparams = ContractingRENParams{Float64}(2, 5, 10, 1)\nmodel  = DiffREN(params)\n\n# Train the model\nfor k in 1:num_training_epochs\n    ...                     # Run some code and compute gradients\n    Flux.update!(...)       # Update model parameters","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"See the docstring of each wrapper and the examples (eg: PDE Observer Design with REN) for more details.","category":"page"},{"location":"introduction/layout/#LBDN-Overview","page":"Package Overview","title":"LBDN Overview","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"[To be written once LBDN has been properly added to the package.]","category":"page"},{"location":"introduction/layout/#Walkthrough","page":"Package Overview","title":"Walkthrough","text":"","category":"section"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's step through the example from Getting Started, which constructs and evaluates a Lipschitz-bounded REN. Start by importing packages and setting a random seed.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"using Random\nusing RobustNeuralNetworks","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's set a random seed and define our batch size and some hyperparameters. For this example, we'll build a Lipschitz-bounded REN with 4 inputs, 2 outputs, 10 states, 20 neurons, and a Lipschitz bound of γ = 1.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"rng = MersenneTwister(42)\nbatches = 10\n\nnu, nx, nv, ny = 4, 10, 20, 2\nγ = 1","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Let's construct the REN parameters. The variable lipschitz_ren_ps contains all the parameters required to build a Lipschitz-bounded REN.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"lipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng=rng)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Once the parameters are defined, we can create a REN object in its explicit form.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"ren = REN(lipschitz_ren_ps)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Now we can evaluate the REN. Note that we can use the init_states function to create a batch of initial states, all zeros, of the correct dimensions.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"Having evaluated the REN, we can check that the outputs are the same as in the original example.","category":"page"},{"location":"introduction/layout/","page":"Package Overview","title":"Package Overview","text":"# Print results for testing\nyout = round.(y1; digits=2)\nprintln(yout[1,:])\nprintln(yout[2,:])","category":"page"},{"location":"examples/rl/#Reinforcement-Learning-with-LBDN","page":"Reinforcement Learning","title":"Reinforcement Learning with LBDN","text":"","category":"section"},{"location":"examples/rl/","page":"Reinforcement Learning","title":"Reinforcement Learning","text":"[Example coming soon. Some examples of RL with REN can be found in Barbara, Wang & Manchester (2023).]","category":"page"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"Pages = [\"models.md\"]","category":"page"},{"location":"lib/models/#Model-Wrappers","page":"Model Wrappers","title":"Model Wrappers","text":"","category":"section"},{"location":"lib/models/#Lipschitz-Bounded-Deep-Networks","page":"Model Wrappers","title":"Lipschitz-Bounded Deep Networks","text":"","category":"section"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"AbstractLBDN\nDiffLBDN\nLBDN","category":"page"},{"location":"lib/models/#RobustNeuralNetworks.AbstractLBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.AbstractLBDN","text":"abstract type AbstractLBDN{T} end\n\nExplicit parameterisation for Lipschitz-bounded deep networks.\n\n(m::AbstractLBDN)(u::AbstractVecOrMat)\n\nCall and AbstractLBDN model given inputs u.\n\nIf arguments are matrices, each column must be a vector of inputs (allows batch simulations).\n\nExamples\n\nThis example creates a dense LBDN using DenseLBDNParams and calls the model with some randomly generated inputs.\n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nγ = 20.0\n\n# Model with 4 inputs, 1 ouput, 4 hidden layers\nnu, ny = 4, 1\nnh = [5, 10, 5, 15]\n\nlbdn_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; rng=rng)\nlbdn = LBDN(lbdn_ps)\n\n# Evaluate model with a batch of random inputs\nu = 10*randn(rng, nu, batches)\ny = lbdn(u)\n\nprintln(round.(y; digits=2))\n\n# output\n\n[-0.69 -1.89 -9.68 3.47 -11.65 -4.48 -4.53 3.61 1.37 -0.68]\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.DiffLBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.DiffLBDN","text":"DiffLBDN(ps::AbstractLBDNParams{T}) where T\n\nConstruct a differentiable LBDN from its direct parameterisation.\n\nDiffLBDN is an alternative to LBDN that computes the explicit parameterisation ExplicitLBDNParams each time the model is called, rather than storing it in the LBDN object.\n\nThis is slow and computationally inefficient if the model is called many times before updating the parameters (eg: in reinforcement learning). However, it can be trained just like any other Flux.jl model and does not need to be re-created if the trainable parameters are updated (unlike LBDN).\n\nSee also AbstractLBDN, LBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.LBDN","page":"Model Wrappers","title":"RobustNeuralNetworks.LBDN","text":"LBDN(ps::AbstractLBDNParams{T}) where T\n\nConstruct an LBDN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of LBDN (eg: a DenseLBDNParams instance) and converts it to a callable explicit parameterisation of the LBDN.\n\nSee also AbstractLBDN, DiffLBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#Recurrent-Equilibrium-Networks","page":"Model Wrappers","title":"Recurrent Equilibrium Networks","text":"","category":"section"},{"location":"lib/models/","page":"Model Wrappers","title":"Model Wrappers","text":"AbstractREN\nDiffREN\nREN\nWrapREN","category":"page"},{"location":"lib/models/#RobustNeuralNetworks.AbstractREN","page":"Model Wrappers","title":"RobustNeuralNetworks.AbstractREN","text":"abstract type AbstractREN end\n\nExplicit parameterisation for recurrent equilibrium networks.\n\n(m::AbstractREN)(xt::AbstractVecOrMat, ut::AbstractVecOrMat)\n\nCall an  AbstractREN model given internal states xt and inputs ut. \n\nIf arguments are matrices, each column must be a vector of states or inputs (allows batch simulations).\n\nExamples\n\nThis example creates a contracting REN using ContractingRENParams and calls the model with some randomly generated inputs. \n\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 2, 20, 1\n\n# Construct a REN\ncontracting_ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng=rng)\nren = REN(contracting_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\nprintln(round.(y1;digits=2))\n\n# output\n\n[-31.41 0.57 -0.55 -3.56 -35.0 -18.28 -25.48 -7.49 -4.14 15.31]\n\nSee also REN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.DiffREN","page":"Model Wrappers","title":"RobustNeuralNetworks.DiffREN","text":"DiffREN(ps::AbstractRENParams{T}) where T\n\nConstruct a differentiable REN from its direct parameterisation.\n\nDiffREN is an alternative to REN and WrapREN that computes the explicit parameterisation ExplicitRENParams](@ref) every time the model is called, rather than storing it in the REN object.\n\nThis is slow and computationally inefficient if the model is called many times before updating the parameters (eg: in reinforcement learning). However, it can be trained just like any other Flux.jl model (unlike WrapREN) and does not need to re-created if the trainable parameters are updated (unlike REN).\n\nSee also AbstractREN, REN, and WrapREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.REN","page":"Model Wrappers","title":"RobustNeuralNetworks.REN","text":"REN(ps::AbstractRENParams{T}) where T\n\nConstruct a REN from its direct parameterisation.\n\nThis constructor takes a direct parameterisation of REN (eg: a GeneralRENParams instance) and converts it to a callable explicit parameterisation of the REN.\n\nSee also AbstractREN, WrapREN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"lib/models/#RobustNeuralNetworks.WrapREN","page":"Model Wrappers","title":"RobustNeuralNetworks.WrapREN","text":"WrapREN(ps::AbstractRENParams{T}) where T\n\nConstruct REN wrapper from its direct parameterisation.\n\nWrapREN is an alternative to REN that stores the AbstractRENParams and ExplicitRENParams within the same object. This means that a new REN object does not have to be created each time the parameters are updated. Explicit REN parameters must be updated by the user if the direct parameters have changed.\n\nNote that WrapREN cannot be used with Flux.jl, since it relies on mutating the WrapREN instance.\n\nExamples\n\nIn this example, we create a REN satisfying some generic behavioural constraints and demonstrate how to update the REN wrapper if model parameters are changed.\n\nusing LinearAlgebra\nusing Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\n\nQ = Matrix{Float64}(-I(ny))\nR = 0.1^2 * Matrix{Float64}(I(nu))\nS = zeros(Float64, nu, ny)\n\n# Construct a REN\nren_ps = GeneralRENParams{Float64}(nu, nx, nv, ny, Q, S, R; rng=rng)\nren = WrapREN(ren_ps)\n\n# Some dummy inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0) \n\n# Update the model after changing a parameter\nren.params.direct.B2 .*= rand(rng, size(ren.params.direct.B2)...)\nupdate_explicit!(ren)\n\nprintln(round(ren.explicit.B2[1];digits=4))\n\nSee also AbstractREN, REN, and DiffREN.\n\n\n\n\n\n","category":"type"},{"location":"examples/lbdn/#Image-Classification-with-LBDN","page":"Image Classification","title":"Image Classification with LBDN","text":"","category":"section"},{"location":"examples/lbdn/","page":"Image Classification","title":"Image Classification","text":"[Example coming soon. It will feature an LBDN trained to classify the MNIST dataset. See Wang & Manchester (2023).]","category":"page"},{"location":"introduction/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"introduction/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"RobustNeuralNetworks.jl is written in Julia and can be installed with the in-built package manager. To add the package, type the following into the REPL.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"] add RobustNeuralNetworks","category":"page"},{"location":"introduction/getting_started/#Basic-Usage","page":"Getting Started","title":"Basic Usage","text":"","category":"section"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"You should now be able to construct robust neural network models. The following example constructs a Lipschitz-bounded REN and evalutates it given a batch of random initial states and inputs.","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Random\nusing RobustNeuralNetworks\n\n# Setup\nrng = MersenneTwister(42)\nbatches = 10\nnu, nx, nv, ny = 4, 10, 20, 2\nγ = 1\n\n# Construct a REN\nlipschitz_ren_ps = LipschitzRENParams{Float64}(nu, nx, nv, ny, γ; rng=rng)\nren = REN(lipschitz_ren_ps)\n\n# Some random inputs\nx0 = init_states(ren, batches; rng=rng)\nu0 = randn(rng, ren.nu, batches)\n\n# Evaluate the REN over one timestep\nx1, y1 = ren(x0, u0)\n\n# Print results for testing\nyout = round.(y1; digits=2)\nprintln(yout[1,:])\nprintln(yout[2,:])\n\n# output\n\n[0.73, 0.72, -0.53, 0.25, 0.84, 0.97, 0.96, 1.13, 0.87, 1.07]\n[1.13, 1.07, 1.44, 0.83, 0.94, 1.26, 0.86, 0.8, 0.96, 0.86]","category":"page"},{"location":"introduction/getting_started/","page":"Getting Started","title":"Getting Started","text":"See Package Overview for a detailed walkthrough of this example.","category":"page"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"Pages = [\"functions.md\"]","category":"page"},{"location":"lib/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"lib/functions/","page":"Functions","title":"Functions","text":"direct_to_explicit\ninit_states\nset_output_zero!\nupdate_explicit!","category":"page"},{"location":"lib/functions/#RobustNeuralNetworks.direct_to_explicit","page":"Functions","title":"RobustNeuralNetworks.direct_to_explicit","text":"direct_to_explicit(ps::AbstractRENParams{T}, return_h=false) where T\n\nConvert direct parameterisation of RENs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitRENParams object that naturally satisfies a set of user-defined behavioural constraints.\n\nArguments\n\nps::AbstractRENParams: Direct parameterisation with behavioural constraints to convert to an explicit parameterisation of REN (eg: GeneralRENParams).\nreturn_h::Bool=false: Whether to return the H-matrix directly (see Revay et al. (2021)). Useful for debugging or model analysis. If false, function returns an object of type ExplicitRENParams{T}. \n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\ndirect_to_explicit(ps::AbstractRENParams{T}) where T\n\nConvert direct parameterisation of LBDNs to explicit parameterisation.\n\nUses the parameterisation encoded in ps to construct an ExplicitLBDNParams object that naturally respects a user-defined Lipschitz bound.\n\nArguments\n\nps::AbstractLBDNParams: Direct parameterisation of an LBDN to convert to an explicit parameterisation for model evaluation (eg: DenseLBDNParams).\n\nSee also DenseLBDNParams.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.init_states","page":"Functions","title":"RobustNeuralNetworks.init_states","text":"init_states(m::AbstractREN, nbatches; rng=nothing)\n\nReturn matrix of (nbatches) state vectors of a REN initialised as zeros.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.set_output_zero!","page":"Functions","title":"RobustNeuralNetworks.set_output_zero!","text":"set_output_zero!(m::AbstractREN)\n\nSet output map of a REN to zero.\n\nIf the resulting model is called with x1,y = ren(x,u) then y = 0 for any x and u.\n\n\n\n\n\nset_output_zero!(m::AbstractLBDN)\n\nSet output map of an LBDN to zero.\n\nIf the resulting model is called with y = lbdn(u) then y = 0 for any u.\n\n\n\n\n\n","category":"function"},{"location":"lib/functions/#RobustNeuralNetworks.update_explicit!","page":"Functions","title":"RobustNeuralNetworks.update_explicit!","text":"update_explicit!(m::WrapREN)\n\nUpdate explicit model in WrapREN using the current direct parameters.\n\n\n\n\n\n","category":"function"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"Pages = [\"ren_params.md\"]","category":"page"},{"location":"lib/model_params/#Model-Parameterisations","page":"Model Parameterisations","title":"Model Parameterisations","text":"","category":"section"},{"location":"lib/model_params/#Lipschitz-Bounded-Deep-Networks","page":"Model Parameterisations","title":"Lipschitz-Bounded Deep Networks","text":"","category":"section"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"AbstractLBDNParams\nDenseLBDNParams\nDirectLBDNParams\nExplicitLBDNParams","category":"page"},{"location":"lib/model_params/#RobustNeuralNetworks.AbstractLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.AbstractLBDNParams","text":"abstract type AbstractLBDNParams{T} end\n\nDirect parameterisation for Lipschitz-bounded deep networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DenseLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DenseLBDNParams","text":"DenseLBDNParams{T}(nu, nh, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a dense (fully-connected) LBDN.\n\nThis is the equivalent of a multi-layer perceptron (eg: Flux.Dense) with a guaranteed Lipschitz bound of γ.\n\nArguments\n\nnu::Int: Number of inputs.\nnh::Vector{Int}: Number of hidden units for each layer. Eg: nh = [5,10] for 2 hidden layers with 5 and 10 nodes (respectively).\nny::Int: Number of outputs.\nγ::Number=T(1): Lipschitz upper bound.\n\nKeyword arguments:\n\nnl::Function=Flux.relu: Sector-bounded static nonlinearity.\n\nSee DirectLBDNParams for documentation of keyword arguments initW, initb, rng.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DirectLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DirectLBDNParams","text":"DirectLBDNParams{T}(nu, nh, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation for a Lipschitz-bounded deep network.\n\nThis is typically used by a higher-level constructor to define an LBDN model, which takes the direct parameterisation in DirectLBDNParams and defines rules for converting it to an explicit parameterisation. See for example DenseLBDNParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnh::Vector{Int}: Number of hidden units for each layer. Eg: nh = [5,10] for 2 hidden layers with 5 and 10 nodes (respectively).\nny::Int: Number of outputs.\n\nKeyword arguments\n\ninitW::Function=Flux.glorot_normal: Initialisation function for implicit params X,Y,d.\ninitb::Function=Flux.glorot_normal: Initialisation function for bias vectors.\nrng::AbstractRNG = Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Wang et al. (2023) for parameterisation details.\n\nSee also DenseLBDNParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ExplicitLBDNParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ExplicitLBDNParams","text":"mutable struct ExplicitLBDNParams{T, N, M}\n\nExplicit LBDN parameter struct.\n\nThese parameters define the explicit form of a Lipschitz-bounded deep network used for model evaluation. Parameters are stored in NTuples, where each element of an NTuple is the parameter for a single layer of the network. Tuples are faster to work with than vectors of arrays.\n\nSee Wang et al. (2023) for more details on explicit parameterisations of LBDN.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#Recurrent-Equilibrium-Networks","page":"Model Parameterisations","title":"Recurrent Equilibrium Networks","text":"","category":"section"},{"location":"lib/model_params/","page":"Model Parameterisations","title":"Model Parameterisations","text":"AbstractRENParams\nContractingRENParams\nDirectRENParams\nExplicitRENParams\nGeneralRENParams\nLipschitzRENParams\nPassiveRENParams","category":"page"},{"location":"lib/model_params/#RobustNeuralNetworks.AbstractRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.AbstractRENParams","text":"abstract type AbstractRENParams{T} end\n\nDirect parameterisation for recurrent equilibrium networks.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ContractingRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ContractingRENParams","text":"ContractingRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a contracting REN.\n\nThe parameters can be used to construct an explicit REN model that has guaranteed, built-in contraction properties.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nnl::Function=Flux.relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\nContractingRENParams(nv, A, B, C, D; ...)\n\nAlternative constructor for ContractingRENParams that initialises the REN from a stable discrete-time linear system with state-space model\n\nbeginalign*\nx_t+1 = Ax_t + Bu_t \ny_t = Cx_t + Du_t\nendalign*\n\n[TODO:] This method may be removed in a later edition of the package.\n\n[TODO:] Make compatible with αbar ≠ 1.0.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.DirectRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.DirectRENParams","text":"DirectRENParams{T}(nu, nx, nv; <keyword arguments>) where T\n\nConstruct direct parameterisation for an (acyclic) recurrent equilibrium network.\n\nThis is typically used by higher-level constructors when defining a REN, which take the direct parameterisation and define rules for converting it to an explicit parameterisation. See for example GeneralRENParams.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\n\nKeyword arguments\n\ninit=:random: Initialisation method. Options are:\n:random: Random sampling for all parameters.\n:cholesky: Compute X with cholesky factorisation of H, sets E,F,P = I.\npolar_param::Bool=true: Use polar parameterisation to construct H matrix from X in REN parameterisation (recommended).\nD22_free::Bool=false: Specify whether to train D22 as a free parameter (true), or construct it separately from X3, Y3, Z3 (false). Typically use D22_free = true only for a contracting REN.\nD22_zero::Bool=false: Fix D22 = 0 to remove any feedthrough.\nbx_scale::T=0: Set scale of initial state bias vector bx.\nbv_scale::T=1: Set scalse of initial neuron input bias vector bv.\nϵ::T=1e-12: Regularising parameter for positive-definite matrices.\nrng::AbstractRNG=Random.GLOBAL_RNG: rng for model initialisation.\n\nSee Revay et al. (2021) for parameterisation details.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.ExplicitRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.ExplicitRENParams","text":"mutable struct ExplicitRENParams{T}\n\nExplicit REN parameter struct.\n\nThese parameters define a recurrent equilibrium network with model inputs and outputs u_t y_t, neuron inputs and outputs v_tw_t, and states x_t.\n\nbeginequation*\nbeginbmatrix\nx_t+1  v_t  y_t\nendbmatrix\n= \nbeginbmatrix\nA  B_1  B_2 \nC_1  D_11  D_12 \nC_2  D_21  D_22 \nendbmatrix\nbeginbmatrix\nx_t  w_t  u_t\nendbmatrix\n+ \nbeginbmatrix\nb_x  b_v  b_y\nendbmatrix\nendequation*\n\nSee Revay et al. (2021) for more details on explicit parameterisations of REN.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.GeneralRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.GeneralRENParams","text":"GeneralRENParams{T}(nu, nx, nv, ny, Q, S, R; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN satisfying general behavioural constraints.\n\nBehavioural constraints are encoded by the matrices Q,S,R in an incremental Integral Quadratic Constraint (IQC). See Equation 4 of Revay et al. (2021).\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nQ::Matrix{T}: IQC weight matrix on model outputs\nS::Matrix{T}: IQC coupling matrix on model outputs/inputs\nR::Matrix{T}: IQC weight matrix on model outputs\n\nKeyword arguments\n\nnl::Function=Flux.relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also ContractingRENParams, LipschitzRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.LipschitzRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.LipschitzRENParams","text":"LipschitzRENParams(nu, nx, nv, ny, γ; <keyword arguments>) where T\n\nConstruct direct parameterisation of a REN with a Lipschitz bound of γ.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\nγ::Number: Lipschitz upper bound.\n\nKeyword arguments\n\nnl::Function=Flux.relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, D22_zero, rng.\n\nSee also GeneralRENParams, ContractingRENParams, PassiveRENParams.\n\n\n\n\n\n","category":"type"},{"location":"lib/model_params/#RobustNeuralNetworks.PassiveRENParams","page":"Model Parameterisations","title":"RobustNeuralNetworks.PassiveRENParams","text":"PassiveRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T\n\nConstruct direct parameterisation of a passive REN.\n\nArguments\n\nnu::Int: Number of inputs.\nnx::Int: Number of states.\nnv::Int: Number of neurons.\nny::Int: Number of outputs.\n\nKeyword arguments\n\nν::T=0: Passivity parameter. Use ν>0 for incrementally strictly input passive model, and ν == 0 for incrementally passive model. \nnl::Function=Flux.relu: Sector-bounded static nonlinearity.\nαbar::T=1: Upper bound on the contraction rate with ᾱ ∈ (0,1].\n\nSee DirectRENParams for documentation of keyword arguments init, ϵ, bx_scale, bv_scale, polar_param, rng.\n\nSee also GeneralRENParams, ContractingRENParams, LipschitzRENParams.\n\n\n\n\n\n","category":"type"},{"location":"examples/nonlinear_ctrl/#Nonlinear-Control-Design-with-REN","page":"Nonlinear Control","title":"Nonlinear Control Design with REN","text":"","category":"section"},{"location":"examples/nonlinear_ctrl/","page":"Nonlinear Control","title":"Nonlinear Control","text":"[Example coming soon. See Section IX of Revay, Wang & Manchester (2021).]","category":"page"},{"location":"#RobustNeuralNetworks.jl-Documentation","page":"Home","title":"RobustNeuralNetworks.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Bringing robust machine learning to Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for RobustNeuralNetworks.jl! This package contains neural network models that are constructed to naturally satisfy robustness constraints, all in native Julia.","category":"page"},{"location":"#Why-Robust-Models?","page":"Home","title":"Why Robust Models?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modern machine learning relies heavily on rapidly training and evaluating neural networks in problems ranging from image classification to robotic control. Most existing neural network architectures have no robustness certificates, making them sensitive to poor data quality, adversarial attacks, and other input perturbations. The few neural network architectures proposed in recent years that offer solutions to this brittle behaviour rely on explicitly enforcing constraints during training to “smooth” the network response. These methods are computationally expensive, making them slow and difficult to scale up to complex real-world problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Recently, we proposed the Recurrent Equilibrium Network (REN) architecture as computationally efficient solutions to these problems. The REN architecture is flexible in that it includes all commonly used neural network models, such as fully-connected networks, convolutional neural networks, and recurrent neural networks. The weight matrices and bias vectors in a REN are directly parameterised to naturally satisfy behavioural constraints chosen by the user. For example, the user can build a REN with a given Lipschitz constant to ensure the output of the network is quantifiably less sensitive to unexpected input perturbations. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The direct parameterisation of RENs means that we can train RENs with standard, unconstrained optimization methods (such as gradient descent) while also guaranteeing their robustness. Achieving the “best of both worlds” in this way is the main advantage of our REN/LBDN model classes, and allows us to freely train them for common machine learning problems as well as more difficult applications where safety and robustness are critical.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[TODO: Add comments on LBDN when properly added to the package.]","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"introduction/getting_started.md\", \"introduction/layout.md\", \"introduction/developing.md\"]\nDepth = 1","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples/lbdn.md\", \"examples/rl.md\", \"examples/nonlinear_ctrl.md\", \"examples/pde_obsv.md\"]\nDepth = 1","category":"page"},{"location":"#Library","page":"Home","title":"Library","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"lib/models.md\", \"lib/model_params.md\", \"lib/functions.md\"]\nDepth = 1","category":"page"},{"location":"#Research-Papers","page":"Home","title":"Research Papers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RobustNeurlaNetworks.jl is built on the REN and LBDN model parameterisations described in the following two papers (respectively):","category":"page"},{"location":"","page":"Home","title":"Home","text":"M. Revay, R. Wang, and I. R. Manchester, \"Recurrent equilibrium networks: Flexible dynamic models with guaranteed stability and robustness,\" April 2021. doi: https://doi.org/10.48550/arXiv.2104.05942.","category":"page"},{"location":"","page":"Home","title":"Home","text":"R. Wang and I. R. Manchester, \"Direct parameterization of Lipschitz-bounded deep networks,\" January 2023. doi: https://doi.org/10.48550/arXiv.2301.11526.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The REN parameterisation was extended to continuous-time systems in:","category":"page"},{"location":"","page":"Home","title":"Home","text":"D. Martinelli, C. L. Galimberti, I. R. Manchester, L. Furieri, and G. Ferrari-Trecate, \"Unconstrained Parametrization of Dissipative and Contracting Neural Ordinary Differential Equations,\" April 2023. doi: https://doi.org/10.48550/arXiv.2304.02976.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See below for a collection of projects and papers using RobustNeuralNetworks.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"N. H. Barbara, R. Wang, and I. R. Manchester, \"Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems,\" April 2023. doi: https://doi.org/10.48550/arXiv.2304.06193.","category":"page"}]
}
