<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>(Convex) Nonlinear Control · RobustNeuralNetworks.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="RobustNeuralNetworks.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RobustNeuralNetworks.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/package_overview/">Package Overview</a></li><li><a class="tocitem" href="../../introduction/developing/">Contributing to the Package</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../lbdn_curvefit/">Fitting a Curve</a></li><li><a class="tocitem" href="../lbdn_mnist/">Image Classification</a></li><li><a class="tocitem" href="../rl/">Reinforcement Learning</a></li><li><a class="tocitem" href="../box_obsv/">Observer Design</a></li><li class="is-active"><a class="tocitem" href>(Convex) Nonlinear Control</a><ul class="internal"><li><a class="tocitem" href="#.-Background-theory"><span>1. Background theory</span></a></li><li><a class="tocitem" href="#.-Problem-setup"><span>2. Problem setup</span></a></li><li><a class="tocitem" href="#.-Generate-training-data"><span>3. Generate training data</span></a></li><li><a class="tocitem" href="#.-Define-a-stable-echo-state-network"><span>4. Define a stable echo state network</span></a></li><li><a class="tocitem" href="#.-Optimise-the-model"><span>5. Optimise the model</span></a></li><li><a class="tocitem" href="#.-Evaluate-the-model"><span>6. Evaluate the model</span></a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/models/">Model Wrappers</a></li><li><a class="tocitem" href="../../lib/model_params/">Model Parameterisations</a></li><li><a class="tocitem" href="../../lib/functions/">Functions</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>(Convex) Nonlinear Control</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>(Convex) Nonlinear Control</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/acfr/RobustNeuralNetworks.jl/blob/main/docs/src/examples/echo_ren.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="(Convex)-Nonlinear-Control-with-REN"><a class="docs-heading-anchor" href="#(Convex)-Nonlinear-Control-with-REN">(Convex) Nonlinear Control with REN</a><a id="(Convex)-Nonlinear-Control-with-REN-1"></a><a class="docs-heading-anchor-permalink" href="#(Convex)-Nonlinear-Control-with-REN" title="Permalink"></a></h1><p><em>This example was first presented in Section IX of <a href="https://ieeexplore.ieee.org/document/10179161">Revay, Wang &amp; Manchester (2021)</a>. Full example code can be found <a href="https://github.com/acfr/RobustNeuralNetworks.jl/blob/main/examples/src/echo_ren.jl">here</a>.</em></p><p>RENs and LBDNs can be used for a lot more than just learning-based problems. In this example, we&#39;ll see how RENs can be used to design nonlinear feedback controllers with stability guarantees for linear dynamical systems with constraints. Introducing constraints (eg: minimum/maximum control inputs) often means that nonlinear controllers perform better than linear policies. A common approach is to use <em>Model Predictive Control</em> (<a href="https://en.wikipedia.org/wiki/Model_predictive_control">MPC</a>). In our case, we&#39;ll use convex optimisation to design a nonlinear controller. The controller will be an <a href="https://en.wikipedia.org/wiki/Echo_state_network"><em>echo state network</em></a> based on a contracting REN. We&#39;ll use this alongside the <a href="https://www.sciencedirect.com/science/article/pii/S1367578820300249"><em>Youla-Kucera parameterisation</em></a> to guarantee stability of the final controller.</p><p>For a detailed explanation of the theory behind this example, please read Section IX of the original <a href="https://ieeexplore.ieee.org/document/10179161">paper</a>. For more on using RENs with the Youla parameterisation, see <a href="https://ieeexplore.ieee.org/abstract/document/9802667">Wang et al. (2022)</a> and <a href="https://doi.org/10.48550/arXiv.2304.06193">Barbara, Wang &amp; Manchester (2023)</a>.</p><h2 id=".-Background-theory"><a class="docs-heading-anchor" href="#.-Background-theory">1. Background theory</a><a id=".-Background-theory-1"></a><a class="docs-heading-anchor-permalink" href="#.-Background-theory" title="Permalink"></a></h2><h3 id="Stabilising-a-linear-system"><a class="docs-heading-anchor" href="#Stabilising-a-linear-system">Stabilising a linear system</a><a id="Stabilising-a-linear-system-1"></a><a class="docs-heading-anchor-permalink" href="#Stabilising-a-linear-system" title="Permalink"></a></h3><p>We&#39;ll start with some background on the structure of linear systems and output-feedback controllers. Consider a discrete-time linear system with state vector <span>$x_t$</span>, control signal <span>$u_t$</span>, external inputs <span>$d_t$</span>, measured output <span>$y_t,$</span> and some performance variable <span>$z_t$</span> to be kept small.</p><p class="math-container">\[\begin{aligned}
x_{t+1} &amp;= \mathbb{A}x_t + \mathbb{B_1} d_t + \mathbb{B_2} u_t \\
z_t &amp;= \mathbb{C_1} x_t + \mathbb{D_{11}} d_t + \mathbb{D_{12}} u_t \\
y_t &amp;= \mathbb{C_2} x_t + \mathbb{D_{21}} d_t
\end{aligned}\]</p><p>A typical choice of stabilising controller is an output-feedback structure with state estimate <span>$\hat{x}_t$</span> and observer/controller gain matrices <span>$L$</span> and <span>$K$</span>, respectively.</p><p class="math-container">\[\begin{aligned}
\hat{x}_{t+1} &amp;= \mathbb{A}\hat{x}_t + \mathbb{B_2} u_t + L \tilde{y}_t \\
\tilde{y}_t &amp;= y_t - \mathbb{C_2} \hat{x}_t \\
u_t &amp;= -K\hat{x}_t + \tilde{u}_t
\end{aligned}\]</p><p>We have also included an additional signal <span>$\tilde{u}_t$</span> to augment the control inputs <span>$u_t.$</span> With a little bit of algebra, the closed-loop dynamics of the system can be written in the following form, where <span>$\mathcal{T}_0, \mathcal{T}_1, \mathcal{T}_2$</span> are linear systems.</p><p class="math-container">\[\begin{bmatrix}
z \\ \tilde{y}
\end{bmatrix}
= 
\begin{bmatrix}
\mathcal{T}_0 &amp; \mathcal{T}_1 \\ \mathcal{T}_2 &amp; 0
\end{bmatrix}
\begin{bmatrix}
d \\ \tilde{u}
\end{bmatrix}\]</p><p>Notice that there is no coupling between <span>$\tilde{y}$</span> and <span>$\tilde{u}$</span>. </p><h3 id="Controller-augmentation"><a class="docs-heading-anchor" href="#Controller-augmentation">Controller augmentation</a><a id="Controller-augmentation-1"></a><a class="docs-heading-anchor-permalink" href="#Controller-augmentation" title="Permalink"></a></h3><p>The linear controller will stabilise our linear dynamical system in the absence of any constraints. But what if we want to shape the closed-loop response to meet some user-defined design criteria without losing stability? For example, what if we want to keep the control signal in some safe range <span>$u_\mathrm{min} &lt; u_t &lt; u_\mathrm{max}$</span> at all times?</p><p>It turns out that if we augment the original controller with <span>$\tilde{u} = \mathcal{Q}(\tilde{y})$</span> where <span>$\mathcal{Q}$</span> is a <a href="https://fbullo.github.io/ctds/"><em>contracting system</em></a> then the closed-loop system is guaranteed to remain stable. This is incredibly useful for optimal control design. For example, we could use a contracting REN as our parameter <span>$\mathcal{Q}$</span> and optimise it to meet some performance specifications (like control constrains), knowing that final closed-loop system is guaranteed to be stable. The closed-loop response can be written as follows.</p><p class="math-container">\[z = \mathcal{T}_0 d + \mathcal{T}_1 \mathcal{Q}(\mathcal{T}_2 d)\]</p><p>This is an old idea in linear control theory called the Youla-Kucera parameterisation. We extended it to nonlinear models (like RENs) and nonlinear dynamical systems in <a href="https://ieeexplore.ieee.org/abstract/document/9802667">Wang et al. (2022)</a> and <a href="https://doi.org/10.48550/arXiv.2304.06193">Barbara, Wang &amp; Manchester (2023)</a>, respectively.</p><h3 id="Echo-state-networks-with-REN"><a class="docs-heading-anchor" href="#Echo-state-networks-with-REN">Echo state networks with REN</a><a id="Echo-state-networks-with-REN-1"></a><a class="docs-heading-anchor-permalink" href="#Echo-state-networks-with-REN" title="Permalink"></a></h3><p>Now that we&#39;ve decided on a structure for our control framework, we need a way to create and optimise a contracting <span>$\mathcal{Q}$</span> to meet our design criteria. We could directly use a contracting REN for <span>$\mathcal{Q}$</span> and train it with reinforcement learning, thereby <a href="https://ieeexplore.ieee.org/abstract/document/9802667">learning over the space of all stabilising controllers</a>  for this linear system. While it&#39;s very useful to have this option, sometimes we&#39;ll want a more efficient solution. Enter convex optimisation with echo state networks.</p><p>Let&#39;s say <span>$\mathcal{Q}$</span> has learnable parameters <span>$\theta$</span>. Suppose our problem is to minimise some convex objective function <span>$J(z)$</span> subject to a set of convex constraints. I.e:</p><p class="math-container">\[\min_\theta J(z) \quad \text{s.t.} \quad c(z) \le 0\]</p><p>An <a href="https://en.wikipedia.org/wiki/Echo_state_network"><em>echo state network</em></a> is a dynamic model with randomly sampled but <em>fixed</em> dynamics and a learnable output map. We can create <em>contracting</em> echo state networks <span>$\mathcal{Q}$</span> with contracting RENs. When a REN model is called, it can be viewed as a system with the following form (see <a href="../../lib/model_params/#RobustNeuralNetworks.ExplicitRENParams"><code>ExplicitRENParams</code></a>).</p><p class="math-container">\[\begin{equation*}
\begin{bmatrix}
\bar{x}_{t+1} \\ v_t \\ \bar{y}_t
\end{bmatrix}
= 
\begin{bmatrix}
A &amp; B_1 &amp; B_2 \\
C_1 &amp; D_{11} &amp; D_{12} \\
C_2 &amp; D_{21} &amp; D_{22} \\
\end{bmatrix}
\begin{bmatrix}
\bar{x}_t \\ w_t \\ \bar{u}_t
\end{bmatrix}
+ 
\begin{bmatrix}
b_x \\ b_v \\ b_y
\end{bmatrix}
\end{equation*}
\quad \text{where} \quad w_t = \sigma(v_t)\]</p><p>Note that <span>$\sigma$</span> is the nonlinear activation function (eg: a ReLU). The inputs and outputs of the REN are <span>$\bar{u}_t$</span> and <span>$\bar{y}_t$</span>, respectively. We can therefore create a <em>contracting</em> echo state network by randomly initialising a contracting REN whose outputs are <span>$\bar{x}_t, w_t, \bar{u}_t$</span> and separately optimising the output layer </p><p class="math-container">\[\bar{y}_t = C_2 \bar{x}_t + D_{21} w_t + D_{22} \bar{u}_t + b_y,\]</p><p>where the learnable parameters are <span>$\theta = [C_2 \ D_{21} \ D_{22} \ b_y].$</span></p><p>The advantage of using an echo state network for <span>$\mathcal{Q}$</span> in the Youla parameterisation is that the problem is entirely convex in <span>$\theta.$</span> This means we can solve for the best choice of <span>$\theta$</span> using standard <strong>convex optimisation</strong> tools. To see why, think of the echo state network as <span>$\mathcal{Q}(\tilde{y}) = \sum_i \theta_i \mathcal{Q}_i(\tilde{y})$</span> where the <span>$\mathcal{Q}_i$</span> are defined by the contracting REN. The closed-loop dynamics are therefore affine in <span>$\theta$</span> since <span>$\mathcal{T}_1$</span> is linear.</p><p class="math-container">\[z = \mathcal{T}_0 d + \sum_i \theta_i \mathcal{T}_1 \mathcal{Q}_i(\mathcal{T}_2 d)\]</p><h2 id=".-Problem-setup"><a class="docs-heading-anchor" href="#.-Problem-setup">2. Problem setup</a><a id=".-Problem-setup-1"></a><a class="docs-heading-anchor-permalink" href="#.-Problem-setup" title="Permalink"></a></h2><p>Let&#39;s consider a simple discrete-time linear system whose closed-loop transfer functions are</p><p class="math-container">\[\mathcal{T}_0 = \mathcal{T}_1 = -\mathcal{T}_2 = \frac{0.3}{q^2 - 2\rho \cos(\phi)q + \rho^2}\]</p><p>where <span>$q$</span> is the shift operator and <span>$\rho = 0.8,$</span> <span>$\phi = 0.2\pi.$</span> <a href="https://juliacontrol.github.io/ControlSystems.jl/stable/"><code>ControlSystems.jl</code></a> offers a nice interface for working with discrete-time transfer functions.</p><pre><code class="language-julia hljs">using ControlSystems

# System parameters and poles: λ = ρ*exp(± im ϕ)
ρ = 0.8
ϕ = 0.2π
λ = ρ .* [cos(ϕ) + sin(ϕ)*im, cos(ϕ) - sin(ϕ)*im]

# Construct discrete-time system with gain 0.3, sampling time 1.0s
k = 0.3
Ts = 1.0
sys = zpk([], λ, k, Ts)

# Closed-loop system components
sim_sys(u::AbstractMatrix) = lsim(sys, u, 1:size(u,2))[1]
T0(u) = sim_sys(u)
T1(u) = sim_sys(u)
T2(u) = -sim_sys(u)</code></pre><div class="admonition is-success"><header class="admonition-header">Our goal</header><div class="admonition-body"><p>Our aim is to minimise the <span>$\ell^1$</span> norm of the performance objective <span>$z$</span> while constraining the control input to <span>$-5 \le u_t \le 5$</span> at all times in response to step inputs with an amplitude of <code>10.0</code>.</p></div></div><h2 id=".-Generate-training-data"><a class="docs-heading-anchor" href="#.-Generate-training-data">3. Generate training data</a><a id=".-Generate-training-data-1"></a><a class="docs-heading-anchor-permalink" href="#.-Generate-training-data" title="Permalink"></a></h2><p>We&#39;ll generate a long trajectory of sample inputs (&quot;disturbances&quot;) <span>$d$</span> consisting of a random step every 50 time samples. The step amplitude is at most <code>10.0</code>.</p><pre><code class="language-julia hljs">using Random
using LinearAlgebra

rng = MersenneTwister(1)

# Sample disturbances
function sample_disturbance(amplitude=10, samples=500, hold=50)
    d = 2 * amplitude * (rand(rng, 1, samples) .- 0.5)
    return kron(d, ones(1, hold))
end
d = sample_disturbance()</code></pre><p>Here&#39;s a plot of the inputs over the first 1000 time samples.</p><pre><code class="language-julia hljs">using CairoMakie

# Check out the disturbance
f = Figure(resolution = (600, 400))
ax = Axis(f[1,1], xlabel=&quot;Time steps&quot;, ylabel=&quot;Output&quot;)
lines!(ax, vec(d)[1:1000],  label=&quot;Disturbance&quot;)
axislegend(ax, position=:rt)
display(f)</code></pre><p><img src="../../assets/echo-ren/echo_ren_inputs.svg" alt/></p><h2 id=".-Define-a-stable-echo-state-network"><a class="docs-heading-anchor" href="#.-Define-a-stable-echo-state-network">4. Define a stable echo state network</a><a id=".-Define-a-stable-echo-state-network-1"></a><a class="docs-heading-anchor-permalink" href="#.-Define-a-stable-echo-state-network" title="Permalink"></a></h2><p>Now that we have training data, let&#39;s define a stable echo state network straight from a contracting REN, as described above. We&#39;ll start by creating a contracting REN whose inputs are <span>$\bar{u} = \tilde{y}$</span> and outputs are <span>$\bar{x}, w, \bar{u}$</span>.</p><pre><code class="language-julia hljs">using RobustNeuralNetworks

# Initialise a contracting REN
nu = 1
nx, nv = 50, 500
ny = nx + nv + nu
ren_ps = ContractingRENParams{Float64}(nu, nx, nv, ny; rng)
model  = REN(ren_ps)

# Make sure the outputs are yt = [xt; wt; ut]
model.explicit.C2  .= [I(nx); zeros(nv, nx); zeros(nu, nx)]
model.explicit.D21 .= [zeros(nx, nv); I(nv); zeros(nu, nv)]
model.explicit.D22 .= [zeros(nx, nu); zeros(nv, nu); I(nu)]
model.explicit.by  .= zeros(ny)</code></pre><p>When we simulate the REN components, we add a row of <code>ones</code> to the output to multiply the output bias vector <span>$b_y$</span> in <span>$\theta.$</span></p><pre><code class="language-julia hljs"># Echo-state components (add ones for bias vector)
function Qᵢ(u)
    x0 = init_states(model, size(u,2))
    _, y = model(x0, u)
    return [y; ones(1,size(y,2))]
end</code></pre><p>The last part of the echo state network is the optimisable output map, which we can set up with <a href="https://jump.dev/Convex.jl/stable/"><code>Convex.jl</code></a>.</p><pre><code class="language-julia hljs">using Convex

# Echo-state network params θ = [C2 D21 D22 by]
θ = Convex.Variable(1, nx+nv+nu+1)</code></pre><h2 id=".-Optimise-the-model"><a class="docs-heading-anchor" href="#.-Optimise-the-model">5. Optimise the model</a><a id=".-Optimise-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Optimise-the-model" title="Permalink"></a></h2><p>Now that we&#39;ve defined the model, we can simulate the closed-loop system and optimise it to meet our design requirements. We need a function that computes the performance signal <span>$z$</span> and control inputs <span>$u$</span> with the echo state network as our augmenting system <span>$\mathcal{Q}$</span>.</p><pre><code class="language-julia hljs"># Complete the closed-loop response and control inputs 
# z = T₀ + ∑ θᵢ*T₁(Qᵢ(T₂(d)))
# u = ∑ θᵢ*Qᵢ(T₂(d))
function sim_echo_state_network(d, θ)
    z0 = T0(d)
    ỹ  = T2(d)
    ũ  = Qᵢ(ỹ)
    z1 = reduce(vcat, T1(ũ&#39;) for ũ in eachrow(ũ))
    z  = z0 + θ * z1
    u  = θ * ũ
    return z, u, z0
end
z, u, _= sim_echo_state_network(d, θ)</code></pre><p>The variables <code>z</code> and <code>u</code> have been constructed through <code>Convex.jl</code>, so we can use them to define our objective function and constraints. That is, keep the <span>$\ell^1$</span> norm of <span>$z$</span> small and the control inputs between <span>$-5 \le u \le 5.$</span> We&#39;ve added a small regularisation term to the objective function to help with numerical conditioning.</p><pre><code class="language-julia hljs"># Cost function and constraints
J = norm(z, 1) + 1e-4*(sumsquares(u) + norm(θ, 2))
constraints = [u &lt; 5, u &gt; -5]</code></pre><p>With the problem all nicely defined, all we have to do is solve it and investigate the resulting control system. We used the Mosek solver with <a href="https://github.com/MOSEK/Mosek.jl"><code>Mosek.jl</code></a>. Note that Mosek requires a license. A free academic license can be obtained from <a href="https://www.mosek.com/products/academic-licenses/">this link</a>. You could try using any of the other solvers compatible with <code>Convex.jl</code>, but second-order interior point methods will be the most reliable.</p><pre><code class="language-julia hljs">using BSON
using Mosek, MosekTools

# Optimise the closed-loop response
problem = minimize(J, constraints)
Convex.solve!(problem, Mosek.Optimizer)

# Save the parameters
θ_solved = evaluate(θ)
bson(&quot;../results/echo_ren_params.bson&quot;, Dict(&quot;params&quot; =&gt; θ_solved))</code></pre><h2 id=".-Evaluate-the-model"><a class="docs-heading-anchor" href="#.-Evaluate-the-model">6. Evaluate the model</a><a id=".-Evaluate-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Evaluate-the-model" title="Permalink"></a></h2><p>We can now assess the closed-loop performance of our system under the optimised nonlinear controller. We&#39;ll first generate some test data: repeating square waves of increasing amplitude.</p><pre><code class="language-julia hljs"># Test on different inputs
a_test = range(0, length=7, stop=8)
d_test = reduce(hcat, a .* [ones(1, 50) zeros(1, 50)] for a in a_test)
z_test, u_test, z0_test = sim_echo_state_network(d_test, θ_solved)</code></pre><p>Now plot the closed-loop response and required control signal.</p><pre><code class="language-julia hljs"># Plot the results
f = Figure(resolution = (1000, 400))
ga = f[1,1] = GridLayout()

# Response
ax1 = Axis(ga[1,1], xlabel=&quot;Time steps&quot;, ylabel=&quot;Output&quot;)
lines!(ax1, vec(d_test),  label=&quot;Disturbance&quot;)
lines!(ax1, vec(z0_test), label=&quot;Open Loop&quot;)
lines!(ax1, vec(z_test),  label=&quot;Echo-REN&quot;)
axislegend(ax1, position=:lt)

# Control inputs
ax2 = Axis(ga[1,2], xlabel=&quot;Time steps&quot;, ylabel=&quot;Control signal&quot;)
lines!(ax2, vec(u_test), label=&quot;Echo-REN&quot;)
lines!(
    ax2, [1, length(u_test)], [-5, -5], 
    color=:black, linestyle=:dash, label=&quot;Constraints&quot;
)
lines!(ax2, [1, length(u_test)], [5, 5], color=:black, linestyle=:dash)
axislegend(ax2, position=:rt)

display(f)</code></pre><p><img src="../../assets/echo-ren/echo_ren_results.svg" alt/></p><p>In open loop (i.e: just the system <span>$\mathcal{T}_0$</span> without our echo state REN), the performance output increases linearly with the disturbance amplitude. When we add our optimised &quot;Echo-REN&quot;, it returns the performance output to zero as quickly as possible without exceeding the <span>$\pm 5$</span> limits on the control signal. The steady-state amplitude only starts to deviate from zero when the control signal reaches its limits.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../box_obsv/">« Observer Design</a><a class="docs-footer-nextpage" href="../../lib/models/">Model Wrappers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 21 July 2023 02:00">Friday 21 July 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
