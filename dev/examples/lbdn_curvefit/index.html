<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fitting a Curve · RobustNeuralNetworks.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="RobustNeuralNetworks.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RobustNeuralNetworks.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/package_overview/">Package Overview</a></li><li><a class="tocitem" href="../../introduction/developing/">Contributing to the Package</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Fitting a Curve</a><ul class="internal"><li><a class="tocitem" href="#.-Generate-training-data"><span>1. Generate training data</span></a></li><li><a class="tocitem" href="#.-Define-a-model"><span>2. Define a model</span></a></li><li><a class="tocitem" href="#.-Define-a-loss-function"><span>3. Define a loss function</span></a></li><li><a class="tocitem" href="#.-Train-the-model"><span>4. Train the model</span></a></li><li><a class="tocitem" href="#.-Examine-the-trained-model"><span>5. Examine the trained model</span></a></li></ul></li><li><a class="tocitem" href="../lbdn_mnist/">Image Classification</a></li><li><a class="tocitem" href="../rl/">Reinforcement Learning</a></li><li><a class="tocitem" href="../pde_obsv/">PDE Observer</a></li><li><a class="tocitem" href="../echo_ren/">(Convex) Nonlinear Control</a></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/models/">Model Wrappers</a></li><li><a class="tocitem" href="../../lib/model_params/">Model Parameterisations</a></li><li><a class="tocitem" href="../../lib/functions/">Functions</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Fitting a Curve</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fitting a Curve</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/acfr/RobustNeuralNetworks.jl/blob/main/docs/src/examples/lbdn_curvefit.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Fitting-a-Curve-with-LBDN"><a class="docs-heading-anchor" href="#Fitting-a-Curve-with-LBDN">Fitting a Curve with LBDN</a><a id="Fitting-a-Curve-with-LBDN-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-Curve-with-LBDN" title="Permalink"></a></h1><p>For our first example, let&#39;s fit a Lipschitz-bounded Deep Network (LBDN) to a curve in one dimension. Consider the multiple sine-wave function below.</p><p class="math-container">\[f(x) = \sin(x) + \frac{1}{N}\sin(Nx)\]</p><p>Our aim is to demonstrate how to train a model in <code>RobustNeuralNetworks.jl</code>, and how to ensure the model naturally satisfies some user-defined robustness certificate (the Lipschitz bound). We&#39;ll follow the steps below to fit an LBDN model to our function <span>$f(x)$</span>:</p><ol><li>Generate training data</li><li>Define a model with a Lipshitz bound (maximum slope) of <code>1.0</code></li><li>Define a loss function</li><li>Train the model to minimise the loss function</li><li>Examine the trained model</li></ol><h2 id=".-Generate-training-data"><a class="docs-heading-anchor" href="#.-Generate-training-data">1. Generate training data</a><a id=".-Generate-training-data-1"></a><a class="docs-heading-anchor-permalink" href="#.-Generate-training-data" title="Permalink"></a></h2><p>Let&#39;s generate training data for <span>$f(x)$</span> on the interval <span>$[0, 2\pi]$</span> and choose <span>$N = 5$</span> as an example. We <code>zip()</code> the data up into a sequence of tuples <code>(x,y)</code> to make training with <code>Flux.jl</code> easier in Step 4.</p><pre><code class="language-julia hljs"># Function to estimate
N = 5
f(x) = sin(x)+(1/N)*sin(N*x)

# Training data
dx = 0.1
xs = 0:dx:2π
ys = f.(xs)
data = zip(xs,ys)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">zip(0.0:0.1:6.2, [0.0, 0.19571852436766876, 0.36696352775664054, 0.4950192039821505, 0.5712778276737869, 0.5991199674249943, 0.5928664750070088, 0.5740610416997671, 0.5659955918379371, 0.587820886094464  …  -0.6363959136031172, -0.5814893018750865, -0.5656923192393724, -0.5770854802107477, -0.5954366706349968, -0.5973289562563506, -0.5620829464988267, -0.47702182301749824, -0.3407879521635516, -0.1638969318821094])</code></pre><h2 id=".-Define-a-model"><a class="docs-heading-anchor" href="#.-Define-a-model">2. Define a model</a><a id=".-Define-a-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Define-a-model" title="Permalink"></a></h2><p>Since we are only dealing with a simple one-dimensional curve, we can afford to use a small model. Let&#39;s choose an LBDN with four hidden layers, each with 15 neurons, and a Lipschitz bound of <code>γ = 1.0</code>. This means that the maximum slope the model can achieve between two points will be exactly <code>1.0</code> by construction.</p><pre><code class="language-julia hljs">using Random
using RobustNeuralNetworks

# Random seed for consistency
rng = MersenneTwister(42)

# Model specification
nu = 1                  # Number of inputs
ny = 1                  # Number of outputs
nh = fill(15,4)         # 4 hidden layers, each with 15 neurons
γ = 1                   # Lipschitz bound of 1

# Set up model: define parameters, then create model
model_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; rng=rng)
model = DiffLBDN(model_ps)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DiffLBDN{Float64}(NNlib.relu, 1, [15, 15, 15, 15], 1, 1.0, DenseLBDNParams{Float64}(NNlib.relu, 1, [15, 15, 15, 15], 1, 1.0, DirectLBDNParams{Float64, 5, 4}(([-0.141230970621109 0.17387960851192474 … -0.09250696003437042 -0.11793094128370285; -0.11287347972393036 -0.4040077030658722 … 0.03255797177553177 -0.02446627989411354; … ; 0.0933610275387764 0.14291802048683167 … -0.0010027296375483274 0.019438110291957855; -0.22509637475013733 0.027144780382514 … 0.16894812881946564 0.04044351354241371], [0.1543492078781128 0.3383992314338684 … 0.026512471958994865 0.0031768255867064; 0.22010403871536255 -0.08802293986082077 … -0.44399282336235046 -0.10694985836744308; … ; 0.1521054357290268 -0.009108913131058216 … 0.006619811058044434 -0.3894876539707184; 0.21607622504234314 -0.008398930542171001 … 0.05633923038840294 -0.38652437925338745], [-0.37974730134010315 0.08021149784326553 … 0.08441084623336792 0.09659316390752792; 0.261581689119339 -0.017746686935424805 … 0.23863326013088226 0.27688151597976685; … ; -0.2637748718261719 0.09837577491998672 … -0.002050981391221285 -0.44083622097969055; 0.14696216583251953 -0.35004937648773193 … 0.2088543027639389 0.10883954912424088], [0.12275252491235733 0.047191329300403595 … -0.04242870584130287 -0.12530212104320526; 0.00032806184026412666 0.2075732946395874 … -0.28339916467666626 0.11588791012763977; … ; -0.21241137385368347 -0.34259524941444397 … 0.013960636220872402 -0.10992234945297241; 0.018983401358127594 0.4975486099720001 … -0.13548624515533447 -0.4372578263282776], [-0.7977325320243835; 0.5656198263168335; … ; 0.40172749757766724; -0.29276904463768005]), ([4.052756666966102], [4.641194877024775], [4.641078414584623], [4.0489096334577805], [1.677250903923724]), ([0.37117499113082886, -0.030266664922237396, -0.6958891153335571, 0.32304951548576355, 0.5361956357955933, -0.3406331539154053, 0.42017170786857605, -0.08236061781644821, 0.33776840567588806, -0.10956566035747528, -0.014922786504030228, -0.7046615481376648, 0.4285116493701935, 0.5565788149833679, -0.29927965998649597], [0.04996958747506142, -0.2947773337364197, -0.10829579830169678, -0.3155266344547272, 0.04692481830716133, 0.13518333435058594, -0.27545127272605896, -0.1492801457643509, 0.2300928831100464, -0.4145483374595642, 0.03322426229715347, 0.370756596326828, -0.7738634347915649, 0.0643128752708435, -0.020377421751618385], [-0.02828282117843628, 0.21653670072555542, 0.05488758534193039, 0.9700963497161865, -0.07240238040685654, -0.2773574888706207, 0.61078941822052, -0.0942046195268631, 0.03325694799423218, -0.386398583650589, -0.23490825295448303, -0.555000364780426, -0.15811388194561005, 0.5182064771652222, -0.15010185539722443], [-0.3793148994445801, 0.4809475839138031, 0.19288913905620575, 0.07967095822095871, 0.3680903911590576, -0.03299703076481819, -0.7800991535186768, 0.8380967974662781, 0.2786998748779297, 0.047354575246572495, -0.23020735383033752, -0.13303856551647186, 0.34238025546073914, -0.14755308628082275, 0.22452138364315033]), ([0.15720373392105103, -0.42865240573883057, -0.47792553901672363, -0.364827424287796, 0.3797971308231354, 0.3773660659790039, -0.0601288266479969, 0.06851162761449814, -0.8773887157440186, -0.16237187385559082, -0.4206550419330597, 0.09212406724691391, -0.22307340800762177, 0.5444554090499878, -0.03660239279270172], [-0.2872127294540405, -0.318857342004776, -0.0425395704805851, -0.23145803809165955, 0.746471643447876, -0.2772376835346222, 0.6956958770751953, -0.2680701017379761, -0.5481454133987427, 0.07261897623538971, 0.6052132844924927, 0.4062584936618805, 0.06502638757228851, -0.8672463893890381, -0.689705491065979], [-0.3113490045070648, -0.29089951515197754, -0.20610769093036652, -0.3752562999725342, 0.1137150228023529, 0.21367906033992767, 0.48485636711120605, -0.11522132903337479, -0.08044777810573578, -0.1952490508556366, -0.6588795781135559, -0.021664567291736603, -0.3827346861362457, 0.6664094924926758, 0.10901253670454025], [0.5095623135566711, 0.13307899236679077, -0.31047990918159485, -0.20491959154605865, 0.14498381316661835, 0.536705493927002, 0.5124711394309998, -0.14533478021621704, 0.4158325493335724, 0.30673277378082275, 0.3805323839187622, -0.06268801540136337, -0.09289214015007019, -0.12029370665550232, -0.018660835921764374], [2.739854335784912]))))</code></pre><p>Note that we first constructed the model parameters <code>model_ps</code>, and <em>then</em> created a callable <code>model</code>. In <code>RobustNeuralNetworks.jl</code>, model parameterisations are separated from &quot;explicit&quot; definitions of a model used for evaluation on data. See the <a href="../../introduction/package_overview/#Direct-and-explicit-parameterisations">Direct &amp; explicit parameterisations</a> for more information.</p><div class="admonition is-info"><header class="admonition-header">A layer-wise approach</header><div class="admonition-body"><p>We have also provided single LBDN layers with <a href="../../lib/models/#RobustNeuralNetworks.SandwichFC"><code>SandwichFC</code></a> to mimic the layer-wise construction of models like with <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dense"><code>Flux.Dense</code></a>. This may be more convenient for users used to working with <code>Flux.jl</code>.</p><p>For example, we can construct an identical model to the LBDN <code>model</code> above with the following.</p><pre><code class="language-julia hljs">using Flux

chain_model = Flux.Chain(
    (x) -&gt; (√γ * x),
    SandwichFC(nu =&gt; nh[1], Flux.relu; T=Float64, rng=rng),
    SandwichFC(nh[1] =&gt; nh[2], Flux.relu; T=Float64, rng=rng),
    SandwichFC(nh[2] =&gt; nh[3], Flux.relu; T=Float64, rng=rng),
    SandwichFC(nh[3] =&gt; nh[4], Flux.relu; T=Float64, rng=rng),
    (x) -&gt; (√γ * x),
    SandwichFC(nh[4] =&gt; ny, Flux.relu; output_layer=true, T=Float64, rng=rng),
)</code></pre><p>See Section 3.1 of <a href="https://doi.org/10.48550/arXiv.2301.11526">Wang &amp; Manchester (2023)</a> for further details.</p></div></div><h2 id=".-Define-a-loss-function"><a class="docs-heading-anchor" href="#.-Define-a-loss-function">3. Define a loss function</a><a id=".-Define-a-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#.-Define-a-loss-function" title="Permalink"></a></h2><p>Let&#39;s stick to a simple loss function based on the mean-squared error (MSE) for this example. All <a href="../../lib/models/#RobustNeuralNetworks.AbstractLBDN"><code>AbstractLBDN</code></a> models take an <code>AbstractArray</code> as their input, which is why <code>x</code> and <code>y</code> are wrapped in vectors.</p><pre><code class="language-julia hljs"># Loss function
loss(model,x,y) = Flux.mse(model([x]),[y])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss (generic function with 1 method)</code></pre><h2 id=".-Train-the-model"><a class="docs-heading-anchor" href="#.-Train-the-model">4. Train the model</a><a id=".-Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Train-the-model" title="Permalink"></a></h2><p>Our objective is to minimise the loss function with a model that has a Lipschitz bound no greater than <code>1.0</code>. Let&#39;s set up a callback function to check the fit error and slope of our model at each training epoch.</p><pre><code class="language-julia hljs">using Flux

# Check fit error/slope during training
mse(model, xs, ys) = sum(loss.((model,), xs, ys)) / length(xs)
lip(model, xs, dx) = maximum(abs.(diff(model(xs&#39;), dims=2)))/dx

# Callback function to show results while training
function progress(model, iter, xs, ys, dx)
    fit_error = round(mse(model, xs, ys), digits=4)
    slope = round(lip(model, xs, dx), digits=4)
    @show iter fit_error slope
    println()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">progress (generic function with 1 method)</code></pre><p>We&#39;ll train the model for 200 training epochs a learning rate of <code>lr = 2e-4</code>. We&#39;ll also use the <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam"><code>Adam</code></a> optimiser from <code>Flux.jl</code> and the default <a href="https://fluxml.ai/Flux.jl/stable/training/reference/#Flux.Optimise.train!-NTuple{4,%20Any}"><code>Flux.train!</code></a> method.</p><pre><code class="language-julia hljs"># Define hyperparameters and optimiser
num_epochs = 200
lr = 2e-4
opt_state = Flux.setup(Adam(lr), model)

# Train the model
for i in 1:num_epochs
    Flux.train!(loss, model, data, opt_state)
    (i % 100 == 0) &amp;&amp; progress(model, i, xs, ys, dx)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">iter = 100
fit_error = 0.0423
slope = 0.7222

iter = 200
fit_error = 0.0164
slope = 0.8405</code></pre><p>Note that this training loop is for demonstration only. For a better fit, or on more complex problems, we strongly recommend:</p><ul><li>Increasing the number of training epochs</li><li>Defining your own <a href="https://fluxml.ai/Flux.jl/stable/training/training/">training loop</a> </li><li>Using <a href="https://github.com/FluxML/ParameterSchedulers.jl">ParameterSchedulers.jl</a> to vary the learning rate.</li></ul><h2 id=".-Examine-the-trained-model"><a class="docs-heading-anchor" href="#.-Examine-the-trained-model">5. Examine the trained model</a><a id=".-Examine-the-trained-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Examine-the-trained-model" title="Permalink"></a></h2><p>We can now plot the results to see what our model looks like.</p><pre><code class="language-julia hljs">using CairoMakie

# Create a figure
f1 = Figure(resolution = (600, 400))
ax = Axis(f1[1,1], xlabel=&quot;x&quot;, ylabel=&quot;y&quot;)

ŷ = map(x -&gt; model([x])[1], xs)
lines!(xs, ys, label = &quot;Data&quot;)
lines!(xs, ŷ, label = &quot;LBDN&quot;)
axislegend(ax)
save(&quot;lbdn_curve_fit.svg&quot;, f1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">CairoMakie.Screen{SVG}
</code></pre><p><img src="../lbdn_curve_fit.svg" alt/></p><p>The model roughly approximates the multiple sine-wave <span>$f(x)$</span>, but maintains a maximum Lipschitz constant (slope on the graph) below 1. </p><pre><code class="language-julia hljs"># Estimate Lipschitz lower-bound
lip(model, xs, dx) = maximum(abs.(diff(model(xs&#39;), dims=2)))/dx
println(&quot;Empirical lower Lipschitz bound: &quot;, round(lip(model, xs, dx); digits=2))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Empirical lower Lipschitz bound: 0.84</code></pre><p>The benefit of using an LBDN is that we have full control over the Lipschitz bound, and can still use standard unconstrained gradient descent tools lile <code>Flux.train!</code> to train our models. For examples in which setting the Lipschitz bound improves model performance and robustness, see <a href="../lbdn_mnist/#Image-Classification-with-LBDN">Image Classification with LBDN</a> and <a href="../rl/#Reinforcement-Learning-with-LBDN">Reinforcement Learning with LBDN</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../introduction/developing/">« Contributing to the Package</a><a class="docs-footer-nextpage" href="../lbdn_mnist/">Image Classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Friday 26 May 2023 04:55">Friday 26 May 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
