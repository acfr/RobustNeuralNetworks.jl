<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Fitting a Curve · RobustNeuralNetworks.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="RobustNeuralNetworks.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">RobustNeuralNetworks.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../introduction/getting_started/">Getting Started</a></li><li><a class="tocitem" href="../../introduction/package_overview/">Package Overview</a></li><li><a class="tocitem" href="../../introduction/developing/">Contributing to the Package</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Fitting a Curve</a><ul class="internal"><li><a class="tocitem" href="#.-Generate-training-data"><span>1. Generate training data</span></a></li><li><a class="tocitem" href="#.-Define-a-model"><span>2. Define a model</span></a></li><li><a class="tocitem" href="#.-Define-a-loss-function"><span>3. Define a loss function</span></a></li><li><a class="tocitem" href="#.-Train-the-model"><span>4. Train the model</span></a></li><li><a class="tocitem" href="#.-Examine-the-trained-model"><span>5. Examine the trained model</span></a></li></ul></li><li><a class="tocitem" href="../lbdn_mnist/">Image Classification</a></li><li><a class="tocitem" href="../rl/">Reinforcement Learning</a></li><li><a class="tocitem" href="../box_obsv/">Observer Design</a></li><li><a class="tocitem" href="../echo_ren/">(Convex) Nonlinear Control</a></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../lib/models/">Model Wrappers</a></li><li><a class="tocitem" href="../../lib/model_params/">Model Parameterisations</a></li><li><a class="tocitem" href="../../lib/functions/">Functions</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Fitting a Curve</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Fitting a Curve</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/acfr/RobustNeuralNetworks.jl/blob/main/docs/src/examples/lbdn_curvefit.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Fitting-a-Curve-with-LBDN"><a class="docs-heading-anchor" href="#Fitting-a-Curve-with-LBDN">Fitting a Curve with LBDN</a><a id="Fitting-a-Curve-with-LBDN-1"></a><a class="docs-heading-anchor-permalink" href="#Fitting-a-Curve-with-LBDN" title="Permalink"></a></h1><p>For our first example, let&#39;s fit a Lipschitz-bounded Deep Network (LBDN) to a curve in one dimension. Consider the step function function below.</p><p class="math-container">\[f(x) = 
\begin{cases}
1 \ \text{if} \ x &gt; 0 \\ 0  \ \text{if} \ x &lt; 0
\end{cases}\]</p><p>Our aim is to demonstrate how to train a model in <code>RobustNeuralNetworks.jl</code>, and how to ensure the model naturally satisfies some user-defined robustness certificate (the Lipschitz bound). We&#39;ll follow the steps below to fit an LBDN model to our function <span>$f(x)$</span>:</p><ol><li>Generate training data</li><li>Define a model with a Lipshitz bound (maximum slope) of <code>10.0</code></li><li>Define a loss function</li><li>Train the model to minimise the loss function</li><li>Examine the trained model</li></ol><h2 id=".-Generate-training-data"><a class="docs-heading-anchor" href="#.-Generate-training-data">1. Generate training data</a><a id=".-Generate-training-data-1"></a><a class="docs-heading-anchor-permalink" href="#.-Generate-training-data" title="Permalink"></a></h2><p>Let&#39;s generate training data for <span>$f(x)$</span> on the interval <span>$[-0.3, 0.3]$</span> as an example. We <code>zip()</code> the data up into a sequence of tuples <code>(x,y)</code> to make training with <code>Flux.jl</code> easier in Step 4.</p><pre><code class="language-julia hljs"># Function to estimate
f(x) = x &lt; 0 ? 0 : 1

# Training data
dx = 0.01
xs = -0.3:dx:0.3
ys = f.(xs)
data = zip(xs,ys)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">zip(-0.3:0.01:0.3, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</code></pre><h2 id=".-Define-a-model"><a class="docs-heading-anchor" href="#.-Define-a-model">2. Define a model</a><a id=".-Define-a-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Define-a-model" title="Permalink"></a></h2><p>Since we are only dealing with a simple one-dimensional curve, we can afford to use a small model. Let&#39;s choose an LBDN with four hidden layers, each with 16 neurons, and a Lipschitz bound of <code>γ = 10.0</code>. This means that the maximum slope the model can achieve between two points should be exactly <code>10.0</code> by construction.</p><pre><code class="language-julia hljs">using Random
using RobustNeuralNetworks

# Random seed for consistency
rng = MersenneTwister(42)

# Model specification
nu = 1                  # Number of inputs
ny = 1                  # Number of outputs
nh = fill(16,4)         # 4 hidden layers, each with 16 neurons
γ = 10                  # Lipschitz bound of 10

# Set up model: define parameters, then create model
model_ps = DenseLBDNParams{Float64}(nu, nh, ny, γ; rng)
model = DiffLBDN(model_ps)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DiffLBDN{Float64}(NNlib.relu, 1, [16, 16, 16, 16], 1, DenseLBDNParams{Float64}(NNlib.relu, 1, [16, 16, 16, 16], 1, DirectLBDNParams{Float64, 5, 4}(([-0.13688436150550842 -0.3915737271308899 … 0.018839871510863304 0.2584531009197235; -0.10939962416887283 0.10109587758779526 … 0.03919880464673042 -0.02107500284910202; … ; -0.21816867589950562 0.140190988779068 … 0.37911009788513184 0.18024104833602905; 0.16852818429470062 0.16767153143882751 … -0.02548663690686226 0.25702613592147827], [-0.1688804030418396 -0.07811985909938812 … 0.2257712483406067 0.05619138106703758; -0.3693792521953583 -0.4018996059894562 … -0.20043642818927765 -0.3080611228942871; … ; -0.24779440462589264 -0.1136888638138771 … -0.1585552841424942 0.16035549342632294; -0.04835795238614082 0.010114463977515697 … 0.19538505375385284 -0.13168814778327942], [-0.23528078198432922 -0.14559900760650635 … -0.1999872326850891 0.08348648995161057; -0.04775254428386688 0.04699297249317169 … -0.262779176235199 -0.12548503279685974; … ; -0.32253220677375793 0.049679722636938095 … -0.5403138399124146 0.1455782651901245; 0.06204642727971077 -0.07663608342409134 … -0.0583852156996727 -0.10783261805772781], [0.34514960646629333 -0.20820173621177673 … -0.14836210012435913 -0.3138638436794281; -0.08742053061723709 -0.03044208139181137 … 0.1568867564201355 0.2581433355808258; … ; -0.23312348127365112 0.05836038291454315 … 0.10704799741506577 -0.09697312861680984; -0.03870696946978569 0.15158973634243011 … -0.13241752982139587 0.15475988388061523], [0.22192633152008057; -0.3082961142063141; … ; -0.38415879011154175; -0.6190966367721558]), ([4.220611207108098], [4.808080373772013], [4.678697523506925], [4.567501950762178], [1.4328398299143486]), ([0.2949219048023224, 0.2776477634906769, 0.26687565445899963, -0.33890992403030396, -0.0601242296397686, -0.4912010729312897, 0.17087963223457336, 0.4991479814052582, -0.2965635359287262, 0.2151995152235031, 0.24747225642204285, 0.351551353931427, 0.5505682229995728, -0.14321142435073853, -0.40737977623939514, 0.6745933890342712], [-0.17201782763004303, 0.1513056457042694, 0.441098153591156, -0.15050721168518066, -0.12680283188819885, -0.2454695701599121, -0.31577154994010925, -0.40353816747665405, 0.21354864537715912, 0.2864471971988678, -0.05785873904824257, -0.36150500178337097, -0.14177212119102478, -0.1755463182926178, -0.07045135647058487, -0.16310331225395203], [0.2597928047180176, -0.42728325724601746, 0.15640889108181, 0.04983280599117279, -0.21352699398994446, -0.1168680265545845, 0.07855833321809769, -0.2857336103916168, -0.07068639248609543, 0.08566267043352127, -0.27968358993530273, 0.3596493601799011, 0.10564349591732025, -0.13930249214172363, -0.3795991837978363, 0.032387617975473404], [-0.2801879346370697, 0.4566338062286377, 0.3702215850353241, -0.3318230211734772, -0.41208863258361816, 0.141834557056427, 0.04177972674369812, -0.13789023458957672, 0.3563082814216614, 0.3375706076622009, -0.1371907889842987, 0.37675443291664124, 0.5317311882972717, -0.1089698076248169, 0.20241987705230713, 0.43033576011657715]), ([0.5561270117759705, 0.943281888961792, 0.5950038433074951, -0.4219045042991638, -0.12413075566291809, 0.05632342770695686, -0.09764140099287033, 0.6227636933326721, -0.2639862596988678, -0.12892699241638184, -0.10817783325910568, 0.07958763092756271, -0.31016403436660767, 0.050157107412815094, 0.07657332718372345, -0.025484846904873848], [0.26330801844596863, 0.37769031524658203, -0.07185274362564087, -0.3942621648311615, -0.11825945973396301, -0.41797399520874023, 0.0028410409577190876, -0.7552792429924011, -0.13185355067253113, 0.2671450674533844, -0.3319263160228729, -0.43978604674339294, 0.34058287739753723, -0.31803256273269653, 0.13363948464393616, 0.0005925644654780626], [-0.1264718472957611, -0.2577274441719055, -0.13553689420223236, 0.0880647823214531, 0.30640465021133423, -0.12403088808059692, 0.0273556187748909, -0.08639398962259293, 0.11978327482938766, -0.16468779742717743, -0.022894911468029022, -0.38888290524482727, 0.11429232358932495, 0.21736784279346466, 0.41620510816574097, 0.01626288890838623], [-0.41817373037338257, -0.13462907075881958, -0.22239384055137634, 0.12524200975894928, -0.2507738769054413, 0.05554039776325226, -0.08715879917144775, -0.17783884704113007, -0.6454927325248718, 0.1585848331451416, -0.08791015297174454, 0.12313690781593323, -0.047714684158563614, -0.20945775508880615, -0.2146414816379547, 0.17544031143188477], [0.25976964831352234]), [2.302585092994046], false)))</code></pre><p>Note that we first constructed the model parameters <code>model_ps</code>, and <em>then</em> created a callable <code>model</code>. In <code>RobustNeuralNetworks.jl</code>, model parameterisations are separated from &quot;explicit&quot; definitions of a model used for evaluation on data. See the <a href="../../introduction/package_overview/#Direct-and-explicit-parameterisations">Direct &amp; explicit parameterisations</a> for more information.</p><div class="admonition is-info"><header class="admonition-header">A layer-wise approach</header><div class="admonition-body"><p>We have also provided single LBDN layers with <a href="../../lib/models/#RobustNeuralNetworks.SandwichFC"><code>SandwichFC</code></a> to mimic the layer-wise construction of models like with <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dense"><code>Flux.Dense</code></a>. This may be more convenient for users used to working with <code>Flux.jl</code>.</p><p>For example, we can construct an identical model to the LBDN <code>model</code> above with the following.</p><pre><code class="language-julia hljs">using Flux

chain_model = Flux.Chain(
    (x) -&gt; (√γ * x),
    SandwichFC(nu =&gt; nh[1], Flux.relu; T=Float64, rng),
    SandwichFC(nh[1] =&gt; nh[2], Flux.relu; T=Float64, rng),
    SandwichFC(nh[2] =&gt; nh[3], Flux.relu; T=Float64, rng),
    SandwichFC(nh[3] =&gt; nh[4], Flux.relu; T=Float64, rng),
    (x) -&gt; (√γ * x),
    SandwichFC(nh[4] =&gt; ny; output_layer=true, T=Float64, rng),
)</code></pre><p>See Section 3.1 of <a href="https://doi.org/10.48550/arXiv.2301.11526">Wang &amp; Manchester (2023)</a> for further details.</p></div></div><h2 id=".-Define-a-loss-function"><a class="docs-heading-anchor" href="#.-Define-a-loss-function">3. Define a loss function</a><a id=".-Define-a-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#.-Define-a-loss-function" title="Permalink"></a></h2><p>Let&#39;s stick to a simple loss function based on the mean-squared error (MSE) for this example. All <a href="../../lib/models/#RobustNeuralNetworks.AbstractLBDN"><code>AbstractLBDN</code></a> models take an <code>AbstractArray</code> as their input, which is why <code>x</code> and <code>y</code> are wrapped in vectors.</p><pre><code class="language-julia hljs"># Loss function
loss(model,x,y) = Flux.mse(model([x]),[y])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss (generic function with 1 method)</code></pre><h2 id=".-Train-the-model"><a class="docs-heading-anchor" href="#.-Train-the-model">4. Train the model</a><a id=".-Train-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Train-the-model" title="Permalink"></a></h2><p>Our objective is to minimise the loss function with a model that has a Lipschitz bound no greater than <code>10.0</code>. Let&#39;s set up a callback function to check the fit error and slope of our model at each training epoch.</p><pre><code class="language-julia hljs">using Flux

# Check fit error/slope during training
mse(model, xs, ys) = sum(loss.((model,), xs, ys)) / length(xs)
lip(model, xs, dx) = maximum(abs.(diff(model(xs&#39;), dims=2)))/dx

# Callback function to show results while training
function progress(model, iter, xs, ys, dx)
    fit_error = round(mse(model, xs, ys), digits=4)
    slope = round(lip(model, xs, dx), digits=4)
    @show iter fit_error slope
    println()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">progress (generic function with 1 method)</code></pre><p>We&#39;ll train the model for 300 training epochs a learning rate of <code>lr = 2e-4</code>. We&#39;ll also use the <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam"><code>Adam</code></a> optimiser from <code>Flux.jl</code> and the default <a href="https://fluxml.ai/Flux.jl/stable/training/reference/#Flux.Optimise.train!-NTuple{4,%20Any}"><code>Flux.train!</code></a> method.</p><pre><code class="language-julia hljs"># Define hyperparameters and optimiser
num_epochs = 300
lr = 2e-4
opt_state = Flux.setup(Adam(lr), model)

# Train the model
for i in 1:num_epochs
    Flux.train!(loss, model, data, opt_state)
    (i % 100 == 0) &amp;&amp; progress(model, i, xs, ys, dx)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">iter = 100
fit_error = 0.0171
slope = 9.0355

iter = 200
fit_error = 0.0153
slope = 9.8895

iter = 300
fit_error = 0.0148
slope = 9.9432</code></pre><p>Note that this training loop is for demonstration only. For a better fit, or on more complex problems, we strongly recommend:</p><ul><li>Increasing the number of training epochs</li><li>Defining your own <a href="https://fluxml.ai/Flux.jl/stable/training/training/">training loop</a> </li><li>Using <a href="https://github.com/FluxML/ParameterSchedulers.jl">ParameterSchedulers.jl</a> to vary the learning rate.</li></ul><h2 id=".-Examine-the-trained-model"><a class="docs-heading-anchor" href="#.-Examine-the-trained-model">5. Examine the trained model</a><a id=".-Examine-the-trained-model-1"></a><a class="docs-heading-anchor-permalink" href="#.-Examine-the-trained-model" title="Permalink"></a></h2><p>The final estimated lower bound of our Lipschitz constantt is very close to the maximum allowable value of 10.0.</p><pre><code class="language-julia hljs">using Printf

# Estimate Lipschitz lower-bound
Empirical_Lipschitz = lip(model, xs, dx)
@printf &quot;Empirical lower Lipschitz bound: %.2f\n&quot; Empirical_Lipschitz</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Empirical lower Lipschitz bound: 9.94</code></pre><p>We can now plot the results to see what our model looks like.</p><pre><code class="language-julia hljs">using CairoMakie

# Create a figure
f1 = Figure(resolution = (600, 400))
ax = Axis(f1[1,1], xlabel=&quot;x&quot;, ylabel=&quot;y&quot;)

# Compute the best-possible fit with Lipschitz bound 10.0
get_best(x) = x&lt;-0.05 ? 0 : (x&lt;0.05 ? 10x + 0.5 : 1)
ybest = get_best.(xs)
ŷ = map(x -&gt; model([x])[1], xs)

# Plot
lines!(xs, ys, label = &quot;Data&quot;)
lines!(xs, ybest, label = &quot;Maximum slope = 10.0&quot;)
lines!(xs, ŷ, label = &quot;LBDN: slope = $(round(Empirical_Lipschitz; digits=2))&quot;)
axislegend(ax, position=:lt)
save(&quot;lbdn_curve_fit.svg&quot;, f1)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">CairoMakie.Screen{SVG}
</code></pre><p><img src="../lbdn_curve_fit.svg" alt/></p><p>The model roughly approximates the step function <span>$f(x)$</span>, but maintains a maximum Lipschitz constant (slope on the graph) below 10.0. It is reasonably close to the best-possible value, and can easily be improved with a slightly larger model and more training time.</p><p>The benefit of using an LBDN is that we have full control over the Lipschitz bound, and can still use standard unconstrained gradient descent tools lile <code>Flux.train!</code> to train our models. For examples in which setting the Lipschitz bound improves model performance and robustness, see <a href="../lbdn_mnist/#Image-Classification-with-LBDN">Image Classification with LBDN</a> and <a href="../rl/#Reinforcement-Learning-with-LBDN">Reinforcement Learning with LBDN</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../introduction/developing/">« Contributing to the Package</a><a class="docs-footer-nextpage" href="../lbdn_mnist/">Image Classification »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 13 July 2023 02:19">Thursday 13 July 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
