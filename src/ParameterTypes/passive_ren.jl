mutable struct PassiveRENParams{T} <: AbstractRENParams{T}
    nl                          # Sector-bounded nonlinearity
    nu::Int
    nx::Int
    nv::Int
    ny::Int
    direct::DirectParams{T}
    Œ±bar::T
    ŒΩ::T
    # TODO: Add a field for incrementally strictly output passive model (œÅ)
end

"""
    PassiveRENParams{T}(nu, nx, nv, ny; <keyword arguments>) where T

Construct direct parameterisation of a passive REN.

# Arguments
- `nu::Int`: Number of inputs.
- `nx::Int`: Number of states.
- `nv::Int`: Number of neurons.
- `ny::Int`: Number of outputs.
    
# Keyword arguments

- `ŒΩ::T=0`: Passivity parameter. Use ŒΩ>0 for incrementally strictly input passive model, and ŒΩ == 0 for incrementally passive model. 

- `nl=Flux.relu`: Static nonlinearity (eg: `Flux.relu` or `Flux.tanh`).

- `Œ±bar::T=1`: Upper bound on the contraction rate with `Œ±ÃÑ ‚àà (0,1]`.

See [`DirectParams`](@ref) documentation for arguments `init`, `œµ`, `bx_scale`, `bv_scale`, `polar_param`, `rng`.

See also [`GeneralRENParams`](@ref), [`ContractingRENParams`](@ref), [`LipschitzRENParams`](@ref).
"""
function PassiveRENParams{T}(
    nu::Int, nx::Int, nv::Int, ny::Int;
    ŒΩ::T = T(0),
    nl = Flux.relu, 
    Œ±bar::T = T(1),
    init = :random,
    polar_param::Bool = true,
    bx_scale::T = T(0), 
    bv_scale::T = T(1), 
    œµ::T = T(1e-12), 
    rng::AbstractRNG = Random.GLOBAL_RNG
) where T

    # Check input output pair dimensions
    if nu != ny
        error("Input and output must have the same dimension for passiveREN")
    end

    # Direct (implicit) params
    direct_ps = DirectParams{T}(
        nu, nx, nv, ny; 
        init=init, œµ=œµ, bx_scale=bx_scale, bv_scale=bv_scale, 
        polar_param=polar_param, D22_free=false, rng=rng
    )

    return PassiveRENParams{T}(nl, nu, nx, nv, ny, direct_ps, Œ±bar, ŒΩ)

end

function passive_trainable(L::DirectParams)
    ps = [L.œÅ, L.X, L.Y1, L.X3, L.Y3, L.Z3, L.B2, L.C2, L.D12, L.D21, L.bx, L.bv, L.by]
    !(L.polar_param) && popfirst!(ps)
    return filter(p -> length(p) !=0, ps)
end

Flux.trainable(m::PassiveRENParams) = passive_trainable(m.direct)

function Flux.gpu(m::PassiveRENParams{T}) where T
    # TODO: Test and complete this
    direct_ps = Flux.gpu(m.direct)
    return PassiveRENParams{T}(
        m.nl, m.nu, m.nx, m.nv, m.ny, direct_ps, m.Œ±bar, m.ŒΩ
    )
end

function Flux.cpu(m::PassiveRENParams{T}) where T
    # TODO: Test and complete this
    direct_ps = Flux.cpu(m.direct)
    return PassiveRENParams{T}(
        m.nl, m.nu, m.nx, m.nv, m.ny, direct_ps, m.Œ±bar, m.ŒΩ
    )
end

function direct_to_explicit(ps::PassiveRENParams{T}, return_h=false) where T

    # System sizes
    nu = ps.nu
    nx = ps.nx
    ny = ps.ny
    ŒΩ = ps.ŒΩ
        
    # Implicit parameters
    œµ = ps.direct.œµ
    œÅ = ps.direct.œÅ
    X = ps.direct.X

    X3 = ps.direct.X3
    Y3 = ps.direct.Y3

    # Implicit system and output matrices
    B2_imp = ps.direct.B2
    D12_imp = ps.direct.D12

    C2 = ps.direct.C2
    D21 = ps.direct.D21

    # Constructing D22 for incrementally passive and incrementally strictly input passive. 
    # See Eqns 31-33 of TAC paper 
    # Currently converts to Hermitian to avoid numerical conditioning issues
    M = X3'*X3 + Y3 - Y3' + œµ*I

    D22 = ŒΩ*Matrix(I, ny,nu) + M
    D21_imp = D21 - D12_imp'

    ùëÖ = -2ŒΩ * Matrix(I, nu, nu) + D22 + D22'

    Œì2 = [C2'; D21_imp'; B2_imp] * (ùëÖ \ [C2 D21_imp B2_imp'])

    if ps.direct.polar_param 
        # See Eqns 29 of TAC paper 
        H = exp(œÅ[1])*X'*X / norm(X)^2 + Œì2
    else
        H = X'*X + œµ*I + Œì2
    end

    # Get explicit parameterisation
    !return_h && (return hmatrix_to_explicit(ps, H, D22))
    return H

end
